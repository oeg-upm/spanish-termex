{
    "id": "I-72",
    "original_text": "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers. This requires the service consumers and providers to negotiate their service needs and offers. Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price. In contrast, this work develops an approach through which the parties can negotiate the content of a service. This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time. Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service. Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers. To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques. We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics. Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1. INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9]. However, negotiation on price presupposes that other properties of the service have already been agreed upon. Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3]. When this is the case, the producer and the consumer need to negotiate the content of the requested service [15]. However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2]. However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer. For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer. Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process. For this purpose, evaluation of the service components with different weights can be useful. Some studies take these weights as a priori and uses the fixed weights [4]. On the other hand, mostly the producer does not know the consumers preferences before the negotiation. Hence, it is more appropriate for the producer to learn these preferences for each consumer. Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time. We represent service requests as a vector of service features. We use an ontology in order to capture the relations between services and to construct the features for a given service. By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation. The particular service we have used is a wine selling service. The wine seller learns the wine preferences of the customer to sell better targeted wines. The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer. Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction. Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services. In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer. For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine. What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine? If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers. However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences. The rest of this paper is organized as follows: Section 2 explains our proposed architecture. Section 3 explains the learning algorithms that were studied to learn consumer preferences. Section 4 studies the different service offering mechanisms. Section 5 contains the similarity metrics used in the experiments. The details of the developed system is analyzed in Section 6. Section 7 provides our experimental setup, test cases, and results. Finally, Section 8 discusses and compares our work with other related work. 2. ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation. Figure 1 depicts our architecture. The consumer agent represents the customer and hence has access to the preferences of the customer. The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences. Similarly, the producer agent has access to the producers inventory and knows which wines are available or not. A shared ontology provides the necessary vocabulary and hence enables a common language for agents. This ontology describes the content of the service. Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated. Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used. However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system. The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on. With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine. The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer. The data repository includes information on the products the producer owns, the number of the products and ratings of those products. Ratings indicate the popularity of the products among customers. Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent. The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request. The request is composed of significant features of the service. In the wine example, these features include color, winery and so on. This is the particular wine that the customer is interested in purchasing. If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends. Otherwise, the producer offers an alternative wine from the inventory. When the consumer receives a counter offer from the producer, it will evaluate it. If it is acceptable, then the negotiation will end. Otherwise, the customer will generate a new request or stick to the previous request. This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer. One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer. When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services. Both the consumers current request and the producers own available services are accessible by the producer. However, the consumers preferences in most cases will not be available. Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer. This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3. PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature. The requests of the consumers represent individual wine products whereas their preferences are constraints over service features. For example, a consumer may have preference for red wine. This means that the consumer is willing to accept any wine offered by the producers as long as the color is red. Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red). At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer. The preferences denote the relative importance of the features of the services demanded by the consumer agents. For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ?}, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ?}, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ?}, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose. On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red. To tackle this problem, we propose to use incremental learning algorithms [6]. This is necessary since no training data is available before the interactions start. We particularly investigate two approaches. The first one is inductive learning. This technique is applied to learn the preferences as concepts. We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10]. CEA is known to perform poorly if the information to be learned is disjunctive. Interestingly, most of the time consumer preferences are disjunctive. Say, we are considering an agent that is buying wine. The consumer may prefer red wine or rose wine but not white wine. To use CEA with such preferences, a solid modification is necessary. The second approach is decision trees. Decision trees can learn from examples easily and classify new instances as positive or negative. A well-known incremental decision tree is ID5R [18]. However, ID5R is known to suffer from high computational complexity. For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples. The algorithm maintains two sets to model the concept to be learned. The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain. As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept. The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned. At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty. During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example. At each interaction between the producer and the consumer, both G and S are modified. The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive. When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance. As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones. Incrementally, G specializes and S generalizes until G and S are equal to each other. When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts. On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes. There are several studies on learning disjunctive concepts via Version Space. Some of these approaches use multiple version space. For instance, Hong et al. maintain several version spaces by split and merge operation [7]. To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation. Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature. EXAMPLE 1. Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes. The original CEA will generalize this as (Light, Delicate, ?), meaning the color can take any value. However, in fact, we only know that the color can be red or white. In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}. Only when all the values exist in the list, they will be replaced by ?. In other words, we let the algorithm generalize more slowly than before. We modify the CEA algorithm to deal with this change. The modified algorithm, DCEA, is given as Algorithm 1. Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space. The initialization phase is the same as the original algorithm (lines 1, 2). If any positive sample comes, we add the sample to the special set as before (line 4). However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other. Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it. After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7). When a negative sample comes, we do not change S as before. We only modify the most general hypotheses not to cover this negative sample (lines 11-15). Different from the original CEA, we try to specialize the G minimally. The algorithm removes the hypothesis covering the negative sample (line 13). Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis. For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis. Thus, all possible hypotheses that do not cover the negative sample are generated (line 14). Note that, exclusive list contains the values that the attribute cannot take. For example, consider the color attribute. If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2. Table 1 illustrates the first three interactions and the workings of DCEA. The most general set and the most specific set show the contents of G and S after the sample comes in. After the first positive sample, S is generalized to also cover the instance. The second sample is negative. Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized. In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set. The third sample is positive and generalizes S even more. Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White). This stems from the possibility of using this rule in the generation of other hypotheses. For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}. By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs. Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts. The ID3 algorithm is used in the learning process with the purpose of classification of offers. There are two classes: positive and negative. Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer. Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones. The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held. The test attribute in a non-leaf node is one of the attributes making up the service description. For instance, body, flavor, color and so on are potential test attributes for wine service. When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node. The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning. To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples. After each coming request, the decision tree is rebuilt. Without doubt, there is a drawback of reconstruction such as additional process load. However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4. SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm. The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different. When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample. The learning components, the most specific set S and the most general set G are actively used in offering service. The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent. In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer. The most specific set, S is used in order to find best offer, which is similar to the consumers preferences. Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list. When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object. This list constitutes the providers inventory of services. Upon receiving a request, if the producer can offer an exactly matching service, then it does so. For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically. When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation. To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S). We compute the similarities in various ways as will be explained in Section 5. After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity. The producer agent can break the tie in a number of ways. Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences. First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list. Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5. SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are. There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12]. The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request. We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17]. In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes. Our current assumption is that α and β is equal to each other. SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values. Tverskys similarity metric is designed to compare two feature vectors. In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector. S consists of hypotheses of feature vectors. Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities. EXAMPLE 3. Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}. Take service s as (Light, Strong, Rose). Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1). Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2. However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request. As a result, we expect the effect of the first hypothesis to be greater than that of the second. Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover. Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service. We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover. We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4. For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9. The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute. For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}. When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts. Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts. The links among the nodes can be considered as distances. Then, the length of the path between the nodes indicates how closely similar the concepts are. An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8]. The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them. Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20]. The semantic similarity is represented with Equation (4) [20]. Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes. N1 is the number of edges between c1 and c0. N2 is the number of edges between c2 and c0. N0 is the number of IS-A links of c0 from the root of the taxonomy. SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions. We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that. Generalization of The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept. The more general concepts are, the less similar they are. For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red. Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling. For instance, Red and Rose are children of ReddishColor. In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent. To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings. Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red. As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept. At starting node related to the first concept, the similarity value is constant and equal to one. This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept. The shorter the path between the concepts, the higher the similarity between nodes. Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way. Starting from c1, the tree is traversed to reach c2. At each hop, the similarity decreases since the concepts are getting farther away from each other. However, based on our intuitions, not all hops decrease the similarity equally. Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling. Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 . According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one. Algorithm 2 shows the distance calculation. According to the algorithm, firstly the similarity is initialized with the value of one (line 1). If the concepts are equal to each other then, similarity will be one (lines 2-4). Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7). If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts. For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation. As a result, we decrease the similarity at each level with the rate of m (line9). Otherwise, there has to be a sibling relation. This means that we have to consider the effect of n when measuring similarity. Recall that we have counted N1+N2 edges between the concepts. Since there is a sibling relation, two of these edges constitute the sibling relation. Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11). Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2. In this example, m is taken as 2/3 and n is taken as 4/7. Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology. In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities. Then the result is equal to the average semantic similarity of the entire feature vector. 6. DEVELOPED SYSTEM We have implemented our architecture in Java. To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests. The producer agent is fully automated and the learning and service offering operations work as explained before. In this section, we explain the implementation details of the developed system. We use OWL [11] as our ontology language and JENA as our ontology reasoner. The shared ontology is the modified version of the Wine Ontology [19]. It includes the description of wine as a concept and different types of wine. All participants of the negotiation use this ontology for understanding each other. According to the ontology, seven properties make up the wine concept. The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology. Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents. Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on. Intuitively, any wine type described in the ontology also represents a wine concept. This allows us to consider instances of Chardonnay wine as instances of Wine class. In addition to wine description, the hierarchical information of some features can be inferred from the ontology. For instance, we can represent the information Europe Continent covers Western Country. Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on. This hierarchical information is used in estimation of semantic similarity. In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z. For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled. When that is the case, we give the reasonable similarity values for these features. For example, the body can be light, medium, or strong. In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong. WineStock Ontology is the producers inventory and describes a product class as WineProduct. This class is necessary for the producer to record the wines that it sells. Ontology involves the individuals of this class. The individuals represent available services that the producer owns. We have prepared two separate WineStock ontologies for testing. In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7. PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only). We apply a variety of scenarios on this dataset in order to see the performance differences. Each test scenario contains a list of preferences for the user and number of matches from the product list. Table 3 shows these preferences and availability of those products in the inventory for first five scenarios. Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation. Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3. Here, first we use Tverskys similarity measure. With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer. Since the performance also depends on the initial request, we repeat our experiments with different initial requests. Consequently, for each case, we run the algorithms five times with several variations of the initial requests. In each experiment, we count the number of iterations that were needed to reach an agreement. We take the average of these numbers in order to evaluate these systems fairly. As is customary, we test each algorithm with the same initial requests. Table 4 compares the approaches using different learning algorithm. When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1). As the number of compatible services drops, RO performs poorly as expected. The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests. CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5. ID3 and DCEA achieve the best results. Their performance is comparable and they can handle all cases including Scenario 5. Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA. In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape. Moreover, the winery of the wine should not be expensive. There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape. In addition, the location of wine should be in Europe. The sweetness degree is wished to be dry or off dry. The flavor should be delicate or moderate where the body should be medium or light. Furthermore, the winery of the wine should be an expensive winery. There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region. The category of winery should be Moderate Winery. There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region. There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery. There are two available products. The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services. Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus. Tverskys metric gives the worst results since it does not consider the semantic similarity. Lins performance are better than Tversky but worse than others. Wu Palmers metric and RP similarity measure nearly give the same performance and better than others. When the results are examined, considering semantic closeness increases the performance. 8. DISCUSSION We review the recent literature in comparison to our work. Tama et al. [16] propose a new approach based on ontology for negotiation. According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies. Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details. While The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated. Further, we have built a system with which negotiation preferences can be learned. Sadri et al. study negotiation in the context of resource allocation [14]. Agents have limited resources and need to require missing resources from other agents. A mechanism which is based on dialogue sequences among agents is proposed as a solution. The mechanism relies on observe-think-action agent cycle. These dialogues include offering resources, resource exchanges and offering alternative resource. Each agent in the system plans its actions to reach a goal state. Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other. Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1]. For achieving this, they use case-based reasoning. Their approach is probabilistic since the behavior of the partners can change at each iteration. In our approach, we are interested in negotiation the content of the service. After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price. Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5]. In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price. Time interval affects these agents differently. Compared to Fatima et al. our focus is different. While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation. Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4]. They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers. Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences. Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer. Unlike them, we also take into account the previous requests of the consumer. In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori. On the contrary, we learn these preferences over time. In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations. Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge. These are interesting directions that we will pursue in our future work. 9. REFERENCES [1] J. Brzostowski and R. Kowalczyk. On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation. In Proceedings of the 4th Intl. Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman. A comment on issue-by-issue negotiations. Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham. Managing e-market negotiation in context with a multiagent system. In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings. Using similarity criteria to make issue trade-offs in automated negotiations. Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings. Optimal agents for multi-issue negotiation. In Proceeding of the 2nd Intl. Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier. A note on the utility of incremental learning. AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng. Splitting and merging version spaces to learn disjunctive concepts. IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin. An information-theoretic definition of similarity. In Proc. 15th International Conf. on Machine Learning, pages 296-304. Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas. Agents that buy and sell. Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell. Machine Learning. McGraw Hill, NY, 1997. [11] OWL. OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu. Foundations of Soft Case-Based Reasoning. John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan. Induction of decision trees. Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni. Dialogues for negotiation: Agent varieties and dialogue sequences. In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421. Springer-Verlag, 2002. [15] M. P. Singh. Value-oriented electronic commerce. IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge. Ontologies for supporting negotiation in e-commerce. Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky. Features of similarity. Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff. Incremental induction of decision trees. Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer. Verb semantics and lexical selection. In 32nd. Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)",
    "original_translation": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor. La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor. Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios. Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las preferencias del consumidor y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta. Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor. Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles. Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor. Este desafío se puede estudiar en tres etapas: • Aprendizaje de preferencias: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5) APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica. Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio. Por ejemplo, un consumidor puede tener preferencia por el vino tinto. Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo. Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo). Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor. Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores. Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa. Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo. Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6]. Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones. Investigamos particularmente dos enfoques. El primero es el aprendizaje inductivo. Esta técnica se aplica para aprender las preferencias como conceptos. Desarrollamos el Algoritmo de Eliminación de Candidatos (CEA) para el Espacio de Versiones [10]. Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva. Curiosamente, la mayoría de las veces las preferencias del consumidor son disyuntivas. Estamos considerando un agente que está comprando vino. El consumidor puede preferir vino tinto o vino rosado pero no vino blanco. Para utilizar CEA con tales preferencias, es necesaria una modificación sólida. El segundo enfoque son los árboles de decisión. Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas. Un árbol de decisión incremental bien conocido es ID5R [18]. Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional. Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados. El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender. El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener. Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto. El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo. Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío. Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo. En cada interacción entre el productor y el consumidor, tanto G como S son modificados. Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas. Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento. Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa. Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí. Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 CEA Disyuntivo Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos. Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos. Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones. Algunos de estos enfoques utilizan múltiples espacios de versión. Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7]. Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir hipótesis disyuntivas además de las conjunciones y la negación. Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica. EJEMPLO 1. Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco). El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor. Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco. En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}. Solo cuando todos los valores existan en la lista, serán reemplazados por ?. En otras palabras, permitimos que el algoritmo generalice más lentamente que antes. Modificamos el algoritmo CEA para hacer frente a este cambio. El algoritmo modificado, DCEA, se presenta como Algoritmo 1. Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de múltiples espacios de versiones. La fase de inicialización es la misma que el algoritmo original (líneas 1, 2). Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4). Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí. Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran. Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7). Cuando llega una muestra negativa, no cambiamos S como antes. Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15). A diferencia del CEA original, intentamos especializar el G mínimamente. El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13). Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada. Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez. Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14). Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar. Por ejemplo, considera el atributo del color. Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2. La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA. El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra. Después de la primera muestra positiva, S se generaliza para cubrir también la instancia. La segunda muestra es negativa. Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada. En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general. La tercera muestra es positiva y generaliza S aún más. Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco). Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis. Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}. Por el Algoritmo 1, no perdemos ninguna información. 3.3 ID3 ID3 [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor. Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos. El algoritmo ID3 se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas. Hay dos clases: positiva y negativa. Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor. Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas. El árbol de decisión tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba. El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio. Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos. Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja. El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender. Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos. Después de cada solicitud entrante, el árbol de decisiones se reconstruye. Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso. Sin embargo, en la práctica hemos evaluado que el ID3 es rápido y el costo de reconstrucción es insignificante. 4. OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA. El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes. Cuando el productor recibe una solicitud del consumidor, el conjunto de aprendizaje del productor se entrena con esta solicitud como una muestra positiva. Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios. El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor. En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor. El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores. Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios. Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios. Esta lista constituye el inventario de servicios de los proveedores. Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace. Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor. Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación. Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S). Calculamos las similitudes de varias maneras, como se explicará en la Sección 5. Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud. El agente productor puede romper el empate de varias maneras. Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de ID3 Si el productor aprende las preferencias de los consumidores con ID3, se aplica un mecanismo similar con dos diferencias. Primero, dado que ID3 no mantiene G, se eliminan de la lista de servicios aquellos no aceptados que se clasifican como negativos. Segundo, las similitudes de los posibles servicios no se miden con respecto a S, sino en cambio a todas las solicitudes previamente realizadas. 4.3 Mecanismos Alternativos de Oferta de Servicios Además de estos tres mecanismos de oferta de servicios (Oferta de Servicio con CEA, Oferta de Servicio con DCEA y Oferta de Servicio con ID3), incluimos otros dos mecanismos. 1304 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) • Oferta de Servicio Aleatoria (RO): El productor genera una contraoferta aleatoriamente de la lista de servicios disponibles, sin considerar las preferencias de los consumidores. • Oferta de Servicio considerando solo la solicitud actual (SCR): El productor selecciona una contraoferta de acuerdo con la similitud de la solicitud actual del consumidor pero no considera solicitudes anteriores. 5. ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una métrica de similitud que toma dos entradas y devuelve qué tan similares son. Existen varios métricos de similitud utilizados en sistemas de razonamiento basado en casos, como la suma ponderada de la distancia euclidiana, la distancia de Hamming, entre otros [12]. La métrica de similitud afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor. Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de similitud semántica llamada Similitud RP. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente. En la Ecuación (1), común representa la cantidad de atributos coincidentes, mientras que diferente representa la cantidad de atributos diferentes. Nuestra suposición actual es que α y β son iguales entre sí. SMpq = α(común) α(común) + β(diferente) (1) Aquí, al comparar dos características, asignamos cero para la disimilitud y uno para la similitud al omitir la cercanía semántica entre los valores de las características. La métrica de similitud de Tversky está diseñada para comparar dos vectores de características. En nuestro sistema, mientras que la lista de servicios que puede ofrecer el productor son cada uno un vector de características, el conjunto más específico S no es un vector de características. S consiste en hipótesis de vectores de características. Por lo tanto, estimamos la similitud de cada hipótesis dentro del conjunto más específico S y luego calculamos el promedio de las similitudes. EJEMPLO 3. Suponga que S contiene las siguientes dos hipótesis: { {Luz, Moderado, (Rojo, Blanco)} , {Completo, Fuerte, Rosa}}. Toma el servicio s como (Ligero, Resistente, Rosa). Entonces, la similitud del primero es igual a 1/3 y la del segundo es igual a 2/3 de acuerdo con la Ecuación (1). Normalmente, tomamos el promedio de ello y obtenemos (1/3 + 2/3)/2, que es igual a 1/2. Sin embargo, la primera hipótesis implica el efecto de dos solicitudes y la segunda hipótesis implica solo una solicitud. Por lo tanto, esperamos que el efecto de la primera hipótesis sea mayor que el de la segunda. Por lo tanto, calculamos la similitud promedio teniendo en cuenta la cantidad de muestras que las hipótesis cubren. Que ch denote el número de muestras que cubre la hipótesis h y (SM(h,servicio)) denote la similitud de la hipótesis h con el servicio dado. Calculamos la similitud de cada hipótesis con el servicio dado y las ponderamos con el número de muestras que cubren. Encontramos la similitud dividiendo la suma ponderada de las similitudes de todas las hipótesis en S con el servicio por el número de todas las muestras que están cubiertas en S. AV G−SM(servicio, S) = |S| |h| (ch ∗ SM(h, servicio)) |S| |h| ch (2) Figura 2: Taxonomía de muestra para estimación de similitud EJEMPLO 4. Para el ejemplo anterior, la similitud de (Luz, Fuerte, Rosa) con el conjunto específico es (2 ∗ 1/3 + 2/3)/3, igual a 4/9. El número posible de muestras que abarca una hipótesis se puede estimar multiplicando las cardinalidades de cada atributo. Por ejemplo, la cardinalidad del primer atributo es dos y la de los demás es igual a uno para la hipótesis dada, como {Luz, Moderado, (Rojo, Blanco)}. Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La métrica de similitud de Lins Un taxonomía puede ser utilizada al estimar la similitud semántica entre dos conceptos. Estimar la similitud semántica en una taxonomía de tipo Es-Un se puede hacer calculando la distancia entre los nodos relacionados con los conceptos comparados. Los enlaces entre los nodos pueden considerarse como distancias. Entonces, la longitud del camino entre los nodos indica qué tan similares son los conceptos. Una estimación alternativa para utilizar el contenido de información en la estimación de la similitud semántica en lugar del método de conteo de aristas, fue propuesta por Lin [8]. La ecuación (3) [8] muestra la similitud de Lin donde c1 y c2 son los conceptos comparados y c0 es el concepto más específico que subsume a ambos. Además, P(C) representa la probabilidad de que un objeto seleccionado arbitrariamente pertenezca al concepto C. La similitud(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Métrica de similitud de Wu y Palmers Diferente de Lin, Wu y Palmer utilizan la distancia entre los nodos en la taxonomía ES-UN [20]. La similitud semántica se representa con la Ecuación (4) [20]. Aquí, se estima la similitud entre c1 y c2 y c0 es el concepto más específico que subsume estas clases. N1 es el número de aristas entre c1 y c0. N2 es el número de aristas entre c2 y c0. N0 es el número de enlaces IS-A de c0 desde la raíz de la taxonomía. Proponemos estimar la distancia relativa en una taxonomía entre dos conceptos utilizando las siguientes intuiciones. Utilizamos la Figura 2 para ilustrar estas intuiciones. • Padre versus abuelo: El padre de un nodo es más similar al nodo que los abuelos de ese. Generalización del Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1305 es un concepto que razonablemente resulta en alejarse más de ese concepto. Cuanto más generales son los conceptos, menos similares son. Por ejemplo, AnyWineColor es el padre de ReddishColor y ReddishColor es el padre de Red. Entonces, esperamos que la similitud entre ReddishColor y Red sea mayor que la similitud entre AnyWineColor y Red. • Padre versus hermano: Un nodo tendría una similitud mayor con su padre que con su hermano. Por ejemplo, Rojo y Rosa son hijos de ColorRojo. En este caso, esperamos que la similitud entre Rojo y ColorRojizo sea mayor que la de Rojo y Rosa. • Hermano versus abuelo: Un nodo es más similar a su hermano que a su abuelo. Para ilustrar, AnyWineColor es el abuelo de Red, y Red y Rose son hermanos. Por lo tanto, posiblemente anticipamos que Rojo y Rosa son más similares que CualquierColorDeVino y Rojo. Como una taxonomía está representada en un árbol, ese árbol puede ser recorrido desde el primer concepto que se está comparando hasta el segundo concepto. En el nodo inicial relacionado con el primer concepto, el valor de similitud es constante y igual a uno. Este valor se reduce por una constante en cada nodo visitado a lo largo del camino que llegará al nodo que incluye el segundo concepto. Cuanto más corto sea el camino entre los conceptos, mayor será la similitud entre los nodos. Algoritmo 2 Estimar-Similitud-RP(c1,c2) Requerido: Las constantes deben ser m > n > m2 donde m, n ∈ R[0, 1] 1: Similitud ← 1 2: si c1 es igual a c2 entonces 3: Devolver Similitud 4: fin si 5: padreComun ← encontrarPadreComun(c1, c2) {padreComun es el concepto más específico que cubre tanto c1 como c2} 6: N1 ← encontrarDistancia(padreComun, c1) 7: N2 ← encontrarDistancia(padreComun, c2) {N1 y N2 son el número de enlaces entre el concepto y el concepto padre} 8: si (padreComun == c1) o (padreComun == c2) entonces 9: Similitud ← Similitud ∗ m(N1+N2) 10: sino 11: Similitud ← Similitud ∗ n ∗ m(N1+N2−2) 12: fin si 13: Devolver Similitud La distancia relativa entre los nodos c1 y c2 se estima de la siguiente manera. Comenzando desde c1, se recorre el árbol para llegar a c2. En cada salto, la similitud disminuye ya que los conceptos se están alejando cada vez más entre sí. Sin embargo, según nuestras intuiciones, no todos los saltos disminuyen la similitud de igual manera. Que m represente el factor para saltar de un hijo a un padre y que n represente el factor para saltar de un hermano a otro hermano. Dado que saltar de un nodo a su abuelo cuenta como dos saltos de padre, el factor de descuento al moverse de un nodo a su abuelo es m2. De acuerdo con las intuiciones anteriores, nuestras constantes deben estar en la forma m > n > m2 donde el valor de m y n debe estar entre cero y uno. El algoritmo 2 muestra el cálculo de la distancia. Según el algoritmo, en primer lugar la similitud se inicializa con el valor de uno (línea 1). Si los conceptos son iguales entre sí, entonces la similitud será uno (líneas 2-4). De lo contrario, calculamos el ancestro común de los dos nodos y la distancia de cada concepto al ancestro común sin considerar al hermano (líneas 5-7). Si uno de los conceptos es igual al padre común, entonces no hay relación de hermanos entre los conceptos. Para cada nivel, multiplicamos la similitud por m y no consideramos el factor de hermanos en la estimación de la similitud. Como resultado, disminuimos la similitud en cada nivel con la tasa de m (línea 9). De lo contrario, tiene que existir una relación de hermanos. Esto significa que debemos considerar el efecto de n al medir la similitud. Recuerde que hemos contado N1+N2 aristas entre los conceptos. Dado que existe una relación de hermanos, dos de estos bordes constituyen la relación de hermanos. Por lo tanto, al calcular el efecto de la relación parental, utilizamos N1+N2 −2 aristas (línea 11). Algunas estimaciones de similitud relacionadas con la taxonomía en la Figura 2 se presentan en la Tabla 2. En este ejemplo, se toma m como 2/3 y n como 4/7. Tabla 2: Estimación de similitud de muestra sobre la taxonomía de muestra. Similitud(ColorRojo, Rosa) = 1 ∗ (2/3) = 0.6666667 Similitud(Rojo, Rosa) = 1 ∗ (4/7) = 0.5714286 Similitud(CualquierColorVino, Rosa) = 1 ∗ (2/3)2 = 0.44444445 Similitud(Blanco, Rosa) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 Para todas las métricas de similitud semántica en nuestra arquitectura, la taxonomía de características se mantiene en la ontología compartida. Para evaluar la similitud del vector de características, primero estimamos la similitud para cada característica individualmente y luego calculamos la suma promedio de estas similitudes. Entonces, el resultado es igual a la similitud semántica promedio de todo el vector de características. 6. SISTEMA DESARROLLADO Hemos implementado nuestra arquitectura en Java. Para facilitar las pruebas del sistema, el agente del consumidor tiene una interfaz de usuario que nos permite ingresar varias solicitudes. El agente productor está completamente automatizado y las operaciones de aprendizaje y oferta de servicios funcionan como se explicó anteriormente. En esta sección, explicamos los detalles de implementación del sistema desarrollado. Utilizamos OWL [11] como nuestro lenguaje de ontología y JENA como nuestro razonador de ontología. La ontología compartida es la versión modificada de la Ontología del Vino [19]. Incluye la descripción del vino como concepto y diferentes tipos de vino. Todos los participantes de la negociación utilizan esta ontología para entenderse mutuamente. Según la ontología, siete propiedades conforman el concepto de vino. El agente consumidor y el agente productor obtienen los valores posibles para estas propiedades consultando la ontología. Por lo tanto, todos los valores posibles para los componentes del concepto del vino, como el color, cuerpo, azúcar, etc., pueden ser alcanzados por ambos agentes. También se describen en esta ontología una variedad de tipos de vino como Borgoña, Chardonnay, Chenin Blanc, entre otros. Intuitivamente, cualquier tipo de vino descrito en la ontología también representa un concepto de vino. Esto nos permite considerar las instancias de vino Chardonnay como instancias de la clase Vino. Además de la descripción del vino, la información jerárquica de algunas características se puede inferir de la ontología. Por ejemplo, podemos representar la información de que el continente europeo abarca países occidentales. El país occidental abarca la región francesa, que incluye algunos territorios como el Loira, Burdeos, entre otros. Esta información jerárquica se utiliza en la estimación de similitud semántica. En esta parte, se pueden hacer algunos razonamientos como si un concepto X abarca Y y Y abarca Z, entonces el concepto X abarca Z. Por ejemplo, el Continente Europeo abarca Burdeos. 1306 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Para algunas características como cuerpo, sabor y azúcar, no hay información jerárquica, pero sus valores están nivelados semánticamente. Cuando eso sucede, proporcionamos los valores de similitud razonables para estas características. Por ejemplo, el cuerpo puede ser ligero, medio o fuerte. En este caso, asumimos que la luz es 0.66 similar a media pero solo 0.33 a fuerte. La ontología de WineStock es el inventario de los productores y describe una clase de producto como WineProduct. Esta clase es necesaria para que el productor registre los vinos que vende. La ontología implica a los individuos de esta clase. Los individuos representan los servicios disponibles que posee el productor. Hemos preparado dos ontologías de WineStock separadas para realizar pruebas. En la primera ontología, hay 19 productos de vino disponibles y en la segunda ontología, hay 50 productos. EVALUACIÓN DEL RENDIMIENTO Evaluamos el rendimiento de los sistemas propuestos en relación con la técnica de aprendizaje que utilizaron, DCEA e ID3, comparándolos con CEA, RO (para oferta aleatoria) y SCR (oferta basada solo en la solicitud actual). Aplicamos una variedad de escenarios en este conjunto de datos para ver las diferencias de rendimiento. Cada escenario de prueba contiene una lista de preferencias para el usuario y el número de coincidencias de la lista de productos. La Tabla 3 muestra estas preferencias y la disponibilidad de esos productos en el inventario para los primeros cinco escenarios. Ten en cuenta que estas preferencias son internas al consumidor y el productor intenta aprenderlas durante la negociación. Tabla 3: Disponibilidad de vinos en diferentes escenarios de prueba ID Preferencia del consumidor Disponibilidad (de 19) 1 Vino seco 15 2 Vino tinto y seco 8 3 Vino tinto, seco y moderado 4 4 Vino tinto y fuerte 2 5 Vino tinto o rosado, y fuerte 3 7.1 Comparación de Algoritmos de Aprendizaje En la comparación de algoritmos de aprendizaje, utilizamos los cinco escenarios de la Tabla 3. Aquí, primero usamos la medida de similitud de Tversky. Con estos casos de prueba, estamos interesados en encontrar el número de iteraciones que se requieren para que el productor genere una oferta aceptable para el consumidor. Dado que el rendimiento también depende de la solicitud inicial, repetimos nuestros experimentos con diferentes solicitudes iniciales. Por consiguiente, para cada caso, ejecutamos los algoritmos cinco veces con varias variaciones de las solicitudes iniciales. En cada experimento, contamos el número de iteraciones necesarias para llegar a un acuerdo. Tomamos el promedio de estos números para evaluar estos sistemas de manera justa. Como es costumbre, probamos cada algoritmo con las mismas solicitudes iniciales. La Tabla 4 compara los enfoques utilizando diferentes algoritmos de aprendizaje. Cuando las partes grandes del inventario son compatibles con las preferencias de los clientes, como en el primer caso de prueba, el rendimiento de todas las técnicas es casi el mismo (por ejemplo, Escenario 1). A medida que el número de servicios compatibles disminuye, RO funciona mal como se esperaba. El segundo peor método es SCR ya que solo considera la solicitud más reciente de los clientes y no aprende de las solicitudes anteriores. CEA da los mejores resultados cuando puede generar una respuesta pero no puede manejar los casos que contienen preferencias disyuntivas, como el que se presenta en el Escenario 5. ID3 y DCEA logran los mejores resultados. Su rendimiento es comparable y pueden manejar todos los casos, incluido el Escenario 5. Tabla 4: Comparación de algoritmos de aprendizaje en términos del número promedio de interacciones. Ejecutar DCEA SCR RO CEA ID3 Escenario 1: 1.2 1.4 1.2 1.2 1.2 Escenario 2: 1.4 1.4 2.6 1.4 1.4 Escenario 3: 1.4 1.8 4.4 1.4 1.4 Escenario 4: 2.2 2.8 9.6 1.8 2 Escenario 5: 2 2.6 7.6 1.75+ Sin oferta 1.8 Promedio de todos los casos: 1.64 2 5.08 1.51+Sin oferta 1.56 7.2 Comparación de Métricas de Similitud Para comparar las métricas de similitud que se explicaron en la Sección 5, fijamos el algoritmo de aprendizaje en DCEA. Además de los escenarios mostrados en la Tabla 3, agregamos los siguientes cinco nuevos escenarios considerando la información jerárquica. • El cliente desea comprar vino cuya bodega esté ubicada en California y cuya uva sea de tipo blanco. Además, la bodega del vino no debería ser costosa. Solo hay cuatro productos que cumplen con estas condiciones. • El cliente quiere comprar vino de color rojo o rosado y de tipo de uva tinta. Además, la ubicación del vino debe ser en Europa. Se desea que el grado de dulzura sea seco o semiseco. El sabor debe ser delicado o moderado, mientras que el cuerpo debe ser medio o ligero. Además, la bodega del vino debería ser una bodega cara. Hay dos productos que cumplen con todos estos requisitos. El cliente quiere comprar vino rosado moderado, que se encuentra alrededor de la región francesa. La categoría de bodega debería ser Bodega Moderada. Solo hay un producto que cumple con estos requisitos. • El cliente quiere comprar vino tinto caro, que se encuentra alrededor de la Región de California o vino blanco barato, que se encuentra alrededor de la Región de Texas. Hay cinco productos disponibles. • El cliente quiere comprar un vino blanco delicado cuyo productor esté en la categoría de Bodega Costosa. Hay dos productos disponibles. Los primeros siete escenarios se prueban con el primer conjunto de datos que contiene un total de 19 servicios y los últimos tres escenarios se prueban con el segundo conjunto de datos que contiene 50 servicios. La Tabla 5 muestra la evaluación del rendimiento en términos del número de interacciones necesarias para llegar a un consenso. La métrica de Tversky da los peores resultados ya que no considera la similitud semántica. El rendimiento de Lins es mejor que el de Tversky pero peor que el de otros. La métrica de Wu-Palmer y la medida de similitud de RP casi ofrecen el mismo rendimiento y son mejores que otras. Cuando se examinan los resultados, considerar la cercanía semántica aumenta el rendimiento. 8. DISCUSIÓN Revisamos la literatura reciente en comparación con nuestro trabajo. Tama et al. [16] proponen un nuevo enfoque basado en ontología para la negociación. Según su enfoque, los protocolos de negociación utilizados en el comercio electrónico pueden ser modelados como ontologías. Por lo tanto, los agentes pueden llevar a cabo un protocolo de negociación utilizando esta ontología compartida sin necesidad de estar codificados con los detalles del protocolo de negociación. Mientras tanto, la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1307 Tabla 5: Comparación de métricas de similitud en términos de número de interacciones. Ejecutar Tversky Lin Wu Palmer RP Escenario 1: 1.2 1.2 1 1 Escenario 2: 1.4 1.4 1.6 1.6 Escenario 3: 1.4 1.8 2 2 Escenario 4: 2.2 1 1.2 1.2 Escenario 5: 2 1.6 1.6 1.6 Escenario 6: 5 3.8 2.4 2.6 Escenario 7: 3.2 1.2 1 1 Escenario 8: 5.6 2 2 2.2 Escenario 9: 2.6 2.2 2.2 2.6 Escenario 10: 4.4 2 2 1.8 Promedio de todos los casos: 2.9 1.82 1.7 1.76 Tama et al. modelan el protocolo de negociación utilizando ontologías, en cambio, nosotros hemos modelado el servicio a ser negociado. Además, hemos construido un sistema con el cual se pueden aprender las preferencias de negociación. El estudio de Sadri et al. analiza la negociación en el contexto de la asignación de recursos [14]. Los agentes tienen recursos limitados y necesitan solicitar recursos faltantes a otros agentes. Se propone un mecanismo basado en secuencias de diálogo entre agentes como solución. El mecanismo se basa en el ciclo de agente de observar-pensar-actuar. Estos diálogos incluyen ofrecer recursos, intercambios de recursos y ofrecer recursos alternativos. Cada agente en el sistema planea sus acciones para alcanzar un estado objetivo. A diferencia de nuestro enfoque, el estudio de Sadri et al. no se preocupa por las preferencias de aprendizaje mutuas. Brzostowski y Kowalczyk proponen un enfoque para seleccionar un socio de negociación adecuado investigando negociaciones previas de múltiples atributos [1]. Para lograr esto, utilizan el razonamiento basado en casos. Su enfoque es probabilístico ya que el comportamiento de los socios puede cambiar en cada iteración. En nuestro enfoque, estamos interesados en negociar el contenido del servicio. Después de que el consumidor y el productor acuerden el servicio, se pueden utilizar mecanismos de negociación orientados al precio para acordar el precio. Fatima et al. estudian los factores que afectan la negociación, como las preferencias, el plazo, el precio, entre otros, ya que el agente que desarrolla una estrategia contra su oponente debe considerar todos ellos [5]. En su enfoque, el objetivo del agente vendedor es vender el servicio al precio más alto posible, mientras que el objetivo del agente comprador es comprar el bien al precio más bajo posible. El intervalo de tiempo afecta a estos agentes de manera diferente. En comparación con Fatima et al., nuestro enfoque es diferente. Mientras ellos estudian el efecto del tiempo en la negociación, nuestro enfoque está en aprender las preferencias para una negociación exitosa. Faratin et al. proponen un mecanismo de negociación multi-tema, donde las variables de servicio para la negociación, como el precio, la calidad del servicio, entre otros, se consideran intercambios entre sí (es decir, un precio más alto por una entrega más temprana) [4]. Generan un modelo heurístico para compensaciones que incluye la estimación de similitud difusa y una exploración de escalada de colina para ofertas posiblemente aceptables. Aunque abordamos un problema similar, aprendemos las preferencias del cliente con la ayuda del aprendizaje inductivo y generamos contraofertas de acuerdo con estas preferencias aprendidas. Faratin et al. solo utilizan la última oferta realizada por el consumidor al calcular la similitud para elegir la contraoferta. A diferencia de ellos, también tenemos en cuenta las solicitudes previas del consumidor. En sus experimentos, Faratin et al. asumen que los pesos de las variables de servicio están fijos a priori. Por el contrario, aprendemos estas preferencias con el tiempo. En nuestro trabajo futuro, planeamos integrar el razonamiento ontológico en el algoritmo de aprendizaje para que la información jerárquica pueda ser aprendida a partir de la jerarquía de subsumpción de relaciones. Además, al utilizar las relaciones entre las características, el productor puede descubrir nuevos conocimientos a partir de los conocimientos existentes. Estas son direcciones interesantes que seguiremos en nuestro trabajo futuro. 9. REFERENCIAS [1] J. Brzostowski y R. Kowalczyk. En el razonamiento basado en casos posibilístico para la selección de socios para la negociación de agentes de múltiples atributos. En Actas del 4to Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 273-278, 2005. [2] L. Busch e I. Horstman. Un comentario sobre negociaciones tema por tema. Juegos y Comportamiento Económico, 19:144-148, 1997. [3] J. K. Debenham. Gestión de la negociación en el mercado electrónico en el contexto de un sistema multiagente. En Actas de la 21ª Conferencia Internacional sobre Sistemas Basados en el Conocimiento e Inteligencia Artificial Aplicada, ES2002:, 2002. [4] P. Faratin, C. Sierra y N. R. Jennings. Utilizando criterios de similitud para hacer compensaciones de problemas en negociaciones automatizadas. Inteligencia Artificial, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge y N. Jennings. Agentes óptimos para negociaciones de múltiples temas. En Actas del 2do Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 129-136, 2003. [6] C. Giraud-Carrier. Una nota sobre la utilidad del aprendizaje incremental. Comunicaciones de IA, 13(4):215-223, 2000. [7] T.-P. Hong y S.-S. Tseng. Dividiendo y fusionando espacios de versiones para aprender conceptos disyuntivos. IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.\n\nTraducción al español:\nIEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin. Una definición de similitud basada en teoría de la información. En Actas de la 15ª Conferencia Internacional sobre Aprendizaje Automático, páginas 296-304. Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, y A. G. Moukas. Agentes que compran y venden. Comunicaciones de la ACM, 42(3):81-91, 1999. [10] T. M. Mitchell. Aprendizaje automático. McGraw Hill, NY, 1997. [11] Búho. OWL: Guía del lenguaje de ontologías web, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal y S. C. K. Shiu. Fundamentos del Razonamiento Basado en Casos Blandos. John Wiley & Sons, Nueva Jersey, 2004. [13] J. R. Quinlan. Inducción de árboles de decisión. Aprendizaje automático, 1(1):81-106, 1986. [14] F. Sadri, F. Toni y P. Torroni. Diálogos para negociación: Variedades de agentes y secuencias de diálogo. En ATAL 2001, Artículos Revisados, volumen 2333 de LNAI, páginas 405-421. Springer-Verlag, 2002. [15] M. P. Singh. \n\nSpringer-Verlag, 2002. [15] M. P. Singh. Comercio electrónico orientado al valor. IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, y M. Wooldridge. Ontologías para apoyar la negociación en el comercio electrónico. Aplicaciones de la Inteligencia Artificial en Ingeniería, 18:223-236, 2005. [17] A. Tversky. Características de similitud. Revisión Psicológica, 84(4):327-352, 1977. [18] P. E. Utgoff. Inducción incremental de árboles de decisión. Aprendizaje automático, 4:161-186, 1989. [19] Vino, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu y M. Palmer. Semántica de verbos y selección léxica. En el 32. Reunión anual de la Asociación de Lingüística Computacional, páginas 133-138, 1994. 1308 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07)",
    "original_sentences": [
        "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
        "This requires the service consumers and providers to negotiate their service needs and offers.",
        "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
        "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
        "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
        "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
        "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
        "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
        "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
        "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
        "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
        "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
        "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
        "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
        "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
        "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
        "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
        "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
        "For this purpose, evaluation of the service components with different weights can be useful.",
        "Some studies take these weights as a priori and uses the fixed weights [4].",
        "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
        "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
        "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
        "We represent service requests as a vector of service features.",
        "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
        "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
        "The particular service we have used is a wine selling service.",
        "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
        "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
        "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
        "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
        "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
        "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
        "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
        "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
        "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
        "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
        "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
        "Section 4 studies the different service offering mechanisms.",
        "Section 5 contains the similarity metrics used in the experiments.",
        "The details of the developed system is analyzed in Section 6.",
        "Section 7 provides our experimental setup, test cases, and results.",
        "Finally, Section 8 discusses and compares our work with other related work. 2.",
        "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
        "Figure 1 depicts our architecture.",
        "The consumer agent represents the customer and hence has access to the preferences of the customer.",
        "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
        "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
        "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
        "This ontology describes the content of the service.",
        "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
        "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
        "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
        "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
        "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
        "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
        "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
        "Ratings indicate the popularity of the products among customers.",
        "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
        "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
        "The request is composed of significant features of the service.",
        "In the wine example, these features include color, winery and so on.",
        "This is the particular wine that the customer is interested in purchasing.",
        "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
        "Otherwise, the producer offers an alternative wine from the inventory.",
        "When the consumer receives a counter offer from the producer, it will evaluate it.",
        "If it is acceptable, then the negotiation will end.",
        "Otherwise, the customer will generate a new request or stick to the previous request.",
        "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
        "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
        "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
        "Both the consumers current request and the producers own available services are accessible by the producer.",
        "However, the consumers preferences in most cases will not be available.",
        "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
        "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
        "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
        "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
        "For example, a consumer may have preference for red wine.",
        "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
        "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
        "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
        "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
        "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
        "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
        "To tackle this problem, we propose to use incremental learning algorithms [6].",
        "This is necessary since no training data is available before the interactions start.",
        "We particularly investigate two approaches.",
        "The first one is inductive learning.",
        "This technique is applied to learn the preferences as concepts.",
        "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
        "CEA is known to perform poorly if the information to be learned is disjunctive.",
        "Interestingly, most of the time consumer preferences are disjunctive.",
        "Say, we are considering an agent that is buying wine.",
        "The consumer may prefer red wine or rose wine but not white wine.",
        "To use CEA with such preferences, a solid modification is necessary.",
        "The second approach is decision trees.",
        "Decision trees can learn from examples easily and classify new instances as positive or negative.",
        "A well-known incremental decision tree is ID5R [18].",
        "However, ID5R is known to suffer from high computational complexity.",
        "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
        "The algorithm maintains two sets to model the concept to be learned.",
        "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
        "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
        "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
        "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
        "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
        "At each interaction between the producer and the consumer, both G and S are modified.",
        "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
        "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
        "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
        "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
        "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
        "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
        "There are several studies on learning disjunctive concepts via Version Space.",
        "Some of these approaches use multiple version space.",
        "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
        "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
        "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
        "EXAMPLE 1.",
        "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
        "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
        "However, in fact, we only know that the color can be red or white.",
        "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
        "Only when all the values exist in the list, they will be replaced by ?.",
        "In other words, we let the algorithm generalize more slowly than before.",
        "We modify the CEA algorithm to deal with this change.",
        "The modified algorithm, DCEA, is given as Algorithm 1.",
        "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
        "The initialization phase is the same as the original algorithm (lines 1, 2).",
        "If any positive sample comes, we add the sample to the special set as before (line 4).",
        "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
        "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
        "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
        "When a negative sample comes, we do not change S as before.",
        "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
        "Different from the original CEA, we try to specialize the G minimally.",
        "The algorithm removes the hypothesis covering the negative sample (line 13).",
        "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
        "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
        "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
        "Note that, exclusive list contains the values that the attribute cannot take.",
        "For example, consider the color attribute.",
        "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
        "Table 1 illustrates the first three interactions and the workings of DCEA.",
        "The most general set and the most specific set show the contents of G and S after the sample comes in.",
        "After the first positive sample, S is generalized to also cover the instance.",
        "The second sample is negative.",
        "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
        "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
        "The third sample is positive and generalizes S even more.",
        "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
        "This stems from the possibility of using this rule in the generation of other hypotheses.",
        "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
        "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
        "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
        "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
        "There are two classes: positive and negative.",
        "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
        "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
        "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
        "The test attribute in a non-leaf node is one of the attributes making up the service description.",
        "For instance, body, flavor, color and so on are potential test attributes for wine service.",
        "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
        "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
        "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
        "After each coming request, the decision tree is rebuilt.",
        "Without doubt, there is a drawback of reconstruction such as additional process load.",
        "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
        "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
        "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
        "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
        "The learning components, the most specific set S and the most general set G are actively used in offering service.",
        "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
        "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
        "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
        "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
        "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
        "This list constitutes the providers inventory of services.",
        "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
        "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
        "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
        "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
        "We compute the similarities in various ways as will be explained in Section 5.",
        "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
        "The producer agent can break the tie in a number of ways.",
        "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
        "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
        "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
        "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
        "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
        "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
        "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
        "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
        "Our current assumption is that α and β is equal to each other.",
        "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
        "Tverskys similarity metric is designed to compare two feature vectors.",
        "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
        "S consists of hypotheses of feature vectors.",
        "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
        "EXAMPLE 3.",
        "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
        "Take service s as (Light, Strong, Rose).",
        "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
        "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
        "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
        "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
        "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
        "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
        "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
        "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
        "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
        "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
        "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
        "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
        "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
        "The links among the nodes can be considered as distances.",
        "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
        "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
        "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
        "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
        "The semantic similarity is represented with Equation (4) [20].",
        "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
        "N1 is the number of edges between c1 and c0.",
        "N2 is the number of edges between c2 and c0.",
        "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
        "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
        "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
        "Generalization of The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
        "The more general concepts are, the less similar they are.",
        "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
        "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
        "For instance, Red and Rose are children of ReddishColor.",
        "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
        "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
        "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
        "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
        "At starting node related to the first concept, the similarity value is constant and equal to one.",
        "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
        "The shorter the path between the concepts, the higher the similarity between nodes.",
        "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
        "Starting from c1, the tree is traversed to reach c2.",
        "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
        "However, based on our intuitions, not all hops decrease the similarity equally.",
        "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
        "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
        "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
        "Algorithm 2 shows the distance calculation.",
        "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
        "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
        "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
        "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
        "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
        "As a result, we decrease the similarity at each level with the rate of m (line9).",
        "Otherwise, there has to be a sibling relation.",
        "This means that we have to consider the effect of n when measuring similarity.",
        "Recall that we have counted N1+N2 edges between the concepts.",
        "Since there is a sibling relation, two of these edges constitute the sibling relation.",
        "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
        "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
        "In this example, m is taken as 2/3 and n is taken as 4/7.",
        "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
        "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
        "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
        "DEVELOPED SYSTEM We have implemented our architecture in Java.",
        "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
        "The producer agent is fully automated and the learning and service offering operations work as explained before.",
        "In this section, we explain the implementation details of the developed system.",
        "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
        "The shared ontology is the modified version of the Wine Ontology [19].",
        "It includes the description of wine as a concept and different types of wine.",
        "All participants of the negotiation use this ontology for understanding each other.",
        "According to the ontology, seven properties make up the wine concept.",
        "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
        "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
        "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
        "Intuitively, any wine type described in the ontology also represents a wine concept.",
        "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
        "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
        "For instance, we can represent the information Europe Continent covers Western Country.",
        "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
        "This hierarchical information is used in estimation of semantic similarity.",
        "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
        "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
        "When that is the case, we give the reasonable similarity values for these features.",
        "For example, the body can be light, medium, or strong.",
        "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
        "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
        "This class is necessary for the producer to record the wines that it sells.",
        "Ontology involves the individuals of this class.",
        "The individuals represent available services that the producer owns.",
        "We have prepared two separate WineStock ontologies for testing.",
        "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
        "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
        "We apply a variety of scenarios on this dataset in order to see the performance differences.",
        "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
        "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
        "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
        "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
        "Here, first we use Tverskys similarity measure.",
        "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
        "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
        "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
        "In each experiment, we count the number of iterations that were needed to reach an agreement.",
        "We take the average of these numbers in order to evaluate these systems fairly.",
        "As is customary, we test each algorithm with the same initial requests.",
        "Table 4 compares the approaches using different learning algorithm.",
        "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
        "As the number of compatible services drops, RO performs poorly as expected.",
        "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
        "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
        "ID3 and DCEA achieve the best results.",
        "Their performance is comparable and they can handle all cases including Scenario 5.",
        "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
        "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
        "Moreover, the winery of the wine should not be expensive.",
        "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
        "In addition, the location of wine should be in Europe.",
        "The sweetness degree is wished to be dry or off dry.",
        "The flavor should be delicate or moderate where the body should be medium or light.",
        "Furthermore, the winery of the wine should be an expensive winery.",
        "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
        "The category of winery should be Moderate Winery.",
        "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
        "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
        "There are two available products.",
        "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
        "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
        "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
        "Lins performance are better than Tversky but worse than others.",
        "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
        "When the results are examined, considering semantic closeness increases the performance. 8.",
        "DISCUSSION We review the recent literature in comparison to our work.",
        "Tama et al. [16] propose a new approach based on ontology for negotiation.",
        "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
        "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
        "While The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
        "Further, we have built a system with which negotiation preferences can be learned.",
        "Sadri et al. study negotiation in the context of resource allocation [14].",
        "Agents have limited resources and need to require missing resources from other agents.",
        "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
        "The mechanism relies on observe-think-action agent cycle.",
        "These dialogues include offering resources, resource exchanges and offering alternative resource.",
        "Each agent in the system plans its actions to reach a goal state.",
        "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
        "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
        "For achieving this, they use case-based reasoning.",
        "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
        "In our approach, we are interested in negotiation the content of the service.",
        "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
        "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
        "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
        "Time interval affects these agents differently.",
        "Compared to Fatima et al. our focus is different.",
        "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
        "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
        "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
        "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
        "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
        "Unlike them, we also take into account the previous requests of the consumer.",
        "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
        "On the contrary, we learn these preferences over time.",
        "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
        "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
        "These are interesting directions that we will pursue in our future work. 9.",
        "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
        "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
        "In Proceedings of the 4th Intl.",
        "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
        "A comment on issue-by-issue negotiations.",
        "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
        "Managing e-market negotiation in context with a multiagent system.",
        "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
        "Using similarity criteria to make issue trade-offs in automated negotiations.",
        "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
        "Optimal agents for multi-issue negotiation.",
        "In Proceeding of the 2nd Intl.",
        "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
        "A note on the utility of incremental learning.",
        "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
        "Splitting and merging version spaces to learn disjunctive concepts.",
        "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
        "An information-theoretic definition of similarity.",
        "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
        "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
        "Agents that buy and sell.",
        "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
        "Machine Learning.",
        "McGraw Hill, NY, 1997. [11] OWL.",
        "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
        "Foundations of Soft Case-Based Reasoning.",
        "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
        "Induction of decision trees.",
        "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
        "Dialogues for negotiation: Agent varieties and dialogue sequences.",
        "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
        "Springer-Verlag, 2002. [15] M. P. Singh.",
        "Value-oriented electronic commerce.",
        "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
        "Ontologies for supporting negotiation in e-commerce.",
        "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
        "Features of similarity.",
        "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
        "Incremental induction of decision trees.",
        "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
        "Verb semantics and lexical selection.",
        "In 32nd.",
        "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
    ],
    "translated_text_sentences": [
        "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores.",
        "Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio.",
        "Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio.",
        "Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio.",
        "Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo.",
        "En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio.",
        "A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas.",
        "Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes.",
        "Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud.",
        "Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1.",
        "INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9].",
        "Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas.",
        "Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3].",
        "Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15].",
        "Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2].",
        "Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro.",
        "Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor.",
        "Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación.",
        "Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil.",
        "Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4].",
        "Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación.",
        "Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor.",
        "Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo.",
        "Representamos las solicitudes de servicio como un vector de características del servicio.",
        "Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado.",
        "Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación.",
        "El servicio en particular que hemos utilizado es un servicio de venta de vinos.",
        "El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos.",
        "El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor.",
        "Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción.",
        "Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios.",
        "En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor.",
        "Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco.",
        "¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado?",
        "Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas.",
        "Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas.",
        "El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta.",
        "La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor.",
        "La sección 4 estudia los diferentes mecanismos de oferta de servicios.",
        "La sección 5 contiene las métricas de similitud utilizadas en los experimentos.",
        "Los detalles del sistema desarrollado se analizan en la Sección 6.",
        "La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados.",
        "Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2.",
        "Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido.",
        "La Figura 1 representa nuestra arquitectura.",
        "El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente.",
        "El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias.",
        "De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no.",
        "Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes.",
        "Esta ontología describe el contenido del servicio.",
        "Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando.",
        "Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada.",
        "Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema.",
        "La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros.",
        "Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino.",
        "El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor.",
        "El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos.",
        "Las calificaciones indican la popularidad de los productos entre los clientes.",
        "Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor.",
        "La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular.",
        "La solicitud está compuesta por características significativas del servicio.",
        "En el ejemplo del vino, estas características incluyen el color, la bodega y demás.",
        "Este es el vino en particular que el cliente está interesado en comprar.",
        "Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina.",
        "De lo contrario, el productor ofrece un vino alternativo del inventario.",
        "Cuando el consumidor recibe una contraoferta del productor, la evaluará.",
        "Si es aceptable, entonces la negociación terminará.",
        "De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior.",
        "Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor.",
        "Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios.",
        "Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las preferencias del consumidor y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta.",
        "Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor.",
        "Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles.",
        "Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor.",
        "Este desafío se puede estudiar en tres etapas: • Aprendizaje de preferencias: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5)",
        "APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica.",
        "Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio.",
        "Por ejemplo, un consumidor puede tener preferencia por el vino tinto.",
        "Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo.",
        "Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo).",
        "Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor.",
        "Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores.",
        "Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa.",
        "Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo.",
        "Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6].",
        "Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones.",
        "Investigamos particularmente dos enfoques.",
        "El primero es el aprendizaje inductivo.",
        "Esta técnica se aplica para aprender las preferencias como conceptos.",
        "Desarrollamos el Algoritmo de Eliminación de Candidatos (CEA) para el Espacio de Versiones [10].",
        "Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva.",
        "Curiosamente, la mayoría de las veces las preferencias del consumidor son disyuntivas.",
        "Estamos considerando un agente que está comprando vino.",
        "El consumidor puede preferir vino tinto o vino rosado pero no vino blanco.",
        "Para utilizar CEA con tales preferencias, es necesaria una modificación sólida.",
        "El segundo enfoque son los árboles de decisión.",
        "Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas.",
        "Un árbol de decisión incremental bien conocido es ID5R [18].",
        "Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional.",
        "Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados.",
        "El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender.",
        "El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener.",
        "Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto.",
        "El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo.",
        "Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío.",
        "Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo.",
        "En cada interacción entre el productor y el consumidor, tanto G como S son modificados.",
        "Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas.",
        "Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento.",
        "Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa.",
        "Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí.",
        "Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 CEA Disyuntivo Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos.",
        "Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos.",
        "Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones.",
        "Algunos de estos enfoques utilizan múltiples espacios de versión.",
        "Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7].",
        "Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir hipótesis disyuntivas además de las conjunciones y la negación.",
        "Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica.",
        "EJEMPLO 1.",
        "Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco).",
        "El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor.",
        "Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco.",
        "En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}.",
        "Solo cuando todos los valores existan en la lista, serán reemplazados por ?.",
        "En otras palabras, permitimos que el algoritmo generalice más lentamente que antes.",
        "Modificamos el algoritmo CEA para hacer frente a este cambio.",
        "El algoritmo modificado, DCEA, se presenta como Algoritmo 1.",
        "Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de múltiples espacios de versiones.",
        "La fase de inicialización es la misma que el algoritmo original (líneas 1, 2).",
        "Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4).",
        "Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí.",
        "Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran.",
        "Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7).",
        "Cuando llega una muestra negativa, no cambiamos S como antes.",
        "Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15).",
        "A diferencia del CEA original, intentamos especializar el G mínimamente.",
        "El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13).",
        "Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada.",
        "Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez.",
        "Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14).",
        "Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar.",
        "Por ejemplo, considera el atributo del color.",
        "Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo.",
        "El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2.",
        "La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA.",
        "El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra.",
        "Después de la primera muestra positiva, S se generaliza para cubrir también la instancia.",
        "La segunda muestra es negativa.",
        "Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada.",
        "En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general.",
        "La tercera muestra es positiva y generaliza S aún más.",
        "Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco).",
        "Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis.",
        "Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}.",
        "Por el Algoritmo 1, no perdemos ninguna información. 3.3 ID3 ID3 [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor.",
        "Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos.",
        "El algoritmo ID3 se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas.",
        "Hay dos clases: positiva y negativa.",
        "Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor.",
        "Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas.",
        "El árbol de decisión tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba.",
        "El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio.",
        "Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos.",
        "Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja.",
        "El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender.",
        "Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos.",
        "Después de cada solicitud entrante, el árbol de decisiones se reconstruye.",
        "Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso.",
        "Sin embargo, en la práctica hemos evaluado que el ID3 es rápido y el costo de reconstrucción es insignificante. 4.",
        "OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA.",
        "El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes.",
        "Cuando el productor recibe una solicitud del consumidor, el conjunto de aprendizaje del productor se entrena con esta solicitud como una muestra positiva.",
        "Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios.",
        "El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor.",
        "En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor.",
        "El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores.",
        "Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios.",
        "Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios.",
        "Esta lista constituye el inventario de servicios de los proveedores.",
        "Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace.",
        "Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor.",
        "Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación.",
        "Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S).",
        "Calculamos las similitudes de varias maneras, como se explicará en la Sección 5.",
        "Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud.",
        "El agente productor puede romper el empate de varias maneras.",
        "Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de ID3 Si el productor aprende las preferencias de los consumidores con ID3, se aplica un mecanismo similar con dos diferencias.",
        "Primero, dado que ID3 no mantiene G, se eliminan de la lista de servicios aquellos no aceptados que se clasifican como negativos.",
        "Segundo, las similitudes de los posibles servicios no se miden con respecto a S, sino en cambio a todas las solicitudes previamente realizadas. 4.3 Mecanismos Alternativos de Oferta de Servicios Además de estos tres mecanismos de oferta de servicios (Oferta de Servicio con CEA, Oferta de Servicio con DCEA y Oferta de Servicio con ID3), incluimos otros dos mecanismos. 1304 El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) • Oferta de Servicio Aleatoria (RO): El productor genera una contraoferta aleatoriamente de la lista de servicios disponibles, sin considerar las preferencias de los consumidores. • Oferta de Servicio considerando solo la solicitud actual (SCR): El productor selecciona una contraoferta de acuerdo con la similitud de la solicitud actual del consumidor pero no considera solicitudes anteriores. 5.",
        "ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una métrica de similitud que toma dos entradas y devuelve qué tan similares son.",
        "Existen varios métricos de similitud utilizados en sistemas de razonamiento basado en casos, como la suma ponderada de la distancia euclidiana, la distancia de Hamming, entre otros [12].",
        "La métrica de similitud afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor.",
        "Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de similitud semántica llamada Similitud RP. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente.",
        "En la Ecuación (1), común representa la cantidad de atributos coincidentes, mientras que diferente representa la cantidad de atributos diferentes.",
        "Nuestra suposición actual es que α y β son iguales entre sí.",
        "SMpq = α(común) α(común) + β(diferente) (1) Aquí, al comparar dos características, asignamos cero para la disimilitud y uno para la similitud al omitir la cercanía semántica entre los valores de las características.",
        "La métrica de similitud de Tversky está diseñada para comparar dos vectores de características.",
        "En nuestro sistema, mientras que la lista de servicios que puede ofrecer el productor son cada uno un vector de características, el conjunto más específico S no es un vector de características.",
        "S consiste en hipótesis de vectores de características.",
        "Por lo tanto, estimamos la similitud de cada hipótesis dentro del conjunto más específico S y luego calculamos el promedio de las similitudes.",
        "EJEMPLO 3.",
        "Suponga que S contiene las siguientes dos hipótesis: { {Luz, Moderado, (Rojo, Blanco)} , {Completo, Fuerte, Rosa}}.",
        "Toma el servicio s como (Ligero, Resistente, Rosa).",
        "Entonces, la similitud del primero es igual a 1/3 y la del segundo es igual a 2/3 de acuerdo con la Ecuación (1).",
        "Normalmente, tomamos el promedio de ello y obtenemos (1/3 + 2/3)/2, que es igual a 1/2.",
        "Sin embargo, la primera hipótesis implica el efecto de dos solicitudes y la segunda hipótesis implica solo una solicitud.",
        "Por lo tanto, esperamos que el efecto de la primera hipótesis sea mayor que el de la segunda.",
        "Por lo tanto, calculamos la similitud promedio teniendo en cuenta la cantidad de muestras que las hipótesis cubren.",
        "Que ch denote el número de muestras que cubre la hipótesis h y (SM(h,servicio)) denote la similitud de la hipótesis h con el servicio dado.",
        "Calculamos la similitud de cada hipótesis con el servicio dado y las ponderamos con el número de muestras que cubren.",
        "Encontramos la similitud dividiendo la suma ponderada de las similitudes de todas las hipótesis en S con el servicio por el número de todas las muestras que están cubiertas en S. AV G−SM(servicio, S) = |S| |h| (ch ∗ SM(h, servicio)) |S| |h| ch (2) Figura 2: Taxonomía de muestra para estimación de similitud EJEMPLO 4.",
        "Para el ejemplo anterior, la similitud de (Luz, Fuerte, Rosa) con el conjunto específico es (2 ∗ 1/3 + 2/3)/3, igual a 4/9.",
        "El número posible de muestras que abarca una hipótesis se puede estimar multiplicando las cardinalidades de cada atributo.",
        "Por ejemplo, la cardinalidad del primer atributo es dos y la de los demás es igual a uno para la hipótesis dada, como {Luz, Moderado, (Rojo, Blanco)}.",
        "Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La métrica de similitud de Lins Un taxonomía puede ser utilizada al estimar la similitud semántica entre dos conceptos.",
        "Estimar la similitud semántica en una taxonomía de tipo Es-Un se puede hacer calculando la distancia entre los nodos relacionados con los conceptos comparados.",
        "Los enlaces entre los nodos pueden considerarse como distancias.",
        "Entonces, la longitud del camino entre los nodos indica qué tan similares son los conceptos.",
        "Una estimación alternativa para utilizar el contenido de información en la estimación de la similitud semántica en lugar del método de conteo de aristas, fue propuesta por Lin [8].",
        "La ecuación (3) [8] muestra la similitud de Lin donde c1 y c2 son los conceptos comparados y c0 es el concepto más específico que subsume a ambos.",
        "Además, P(C) representa la probabilidad de que un objeto seleccionado arbitrariamente pertenezca al concepto C. La similitud(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Métrica de similitud de Wu y Palmers Diferente de Lin, Wu y Palmer utilizan la distancia entre los nodos en la taxonomía ES-UN [20].",
        "La similitud semántica se representa con la Ecuación (4) [20].",
        "Aquí, se estima la similitud entre c1 y c2 y c0 es el concepto más específico que subsume estas clases.",
        "N1 es el número de aristas entre c1 y c0.",
        "N2 es el número de aristas entre c2 y c0.",
        "N0 es el número de enlaces IS-A de c0 desde la raíz de la taxonomía.",
        "Proponemos estimar la distancia relativa en una taxonomía entre dos conceptos utilizando las siguientes intuiciones.",
        "Utilizamos la Figura 2 para ilustrar estas intuiciones. • Padre versus abuelo: El padre de un nodo es más similar al nodo que los abuelos de ese.",
        "Generalización del Sexto Internacional.",
        "La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1305 es un concepto que razonablemente resulta en alejarse más de ese concepto.",
        "Cuanto más generales son los conceptos, menos similares son.",
        "Por ejemplo, AnyWineColor es el padre de ReddishColor y ReddishColor es el padre de Red.",
        "Entonces, esperamos que la similitud entre ReddishColor y Red sea mayor que la similitud entre AnyWineColor y Red. • Padre versus hermano: Un nodo tendría una similitud mayor con su padre que con su hermano.",
        "Por ejemplo, Rojo y Rosa son hijos de ColorRojo.",
        "En este caso, esperamos que la similitud entre Rojo y ColorRojizo sea mayor que la de Rojo y Rosa. • Hermano versus abuelo: Un nodo es más similar a su hermano que a su abuelo.",
        "Para ilustrar, AnyWineColor es el abuelo de Red, y Red y Rose son hermanos.",
        "Por lo tanto, posiblemente anticipamos que Rojo y Rosa son más similares que CualquierColorDeVino y Rojo.",
        "Como una taxonomía está representada en un árbol, ese árbol puede ser recorrido desde el primer concepto que se está comparando hasta el segundo concepto.",
        "En el nodo inicial relacionado con el primer concepto, el valor de similitud es constante y igual a uno.",
        "Este valor se reduce por una constante en cada nodo visitado a lo largo del camino que llegará al nodo que incluye el segundo concepto.",
        "Cuanto más corto sea el camino entre los conceptos, mayor será la similitud entre los nodos.",
        "Algoritmo 2 Estimar-Similitud-RP(c1,c2) Requerido: Las constantes deben ser m > n > m2 donde m, n ∈ R[0, 1] 1: Similitud ← 1 2: si c1 es igual a c2 entonces 3: Devolver Similitud 4: fin si 5: padreComun ← encontrarPadreComun(c1, c2) {padreComun es el concepto más específico que cubre tanto c1 como c2} 6: N1 ← encontrarDistancia(padreComun, c1) 7: N2 ← encontrarDistancia(padreComun, c2) {N1 y N2 son el número de enlaces entre el concepto y el concepto padre} 8: si (padreComun == c1) o (padreComun == c2) entonces 9: Similitud ← Similitud ∗ m(N1+N2) 10: sino 11: Similitud ← Similitud ∗ n ∗ m(N1+N2−2) 12: fin si 13: Devolver Similitud La distancia relativa entre los nodos c1 y c2 se estima de la siguiente manera.",
        "Comenzando desde c1, se recorre el árbol para llegar a c2.",
        "En cada salto, la similitud disminuye ya que los conceptos se están alejando cada vez más entre sí.",
        "Sin embargo, según nuestras intuiciones, no todos los saltos disminuyen la similitud de igual manera.",
        "Que m represente el factor para saltar de un hijo a un padre y que n represente el factor para saltar de un hermano a otro hermano.",
        "Dado que saltar de un nodo a su abuelo cuenta como dos saltos de padre, el factor de descuento al moverse de un nodo a su abuelo es m2.",
        "De acuerdo con las intuiciones anteriores, nuestras constantes deben estar en la forma m > n > m2 donde el valor de m y n debe estar entre cero y uno.",
        "El algoritmo 2 muestra el cálculo de la distancia.",
        "Según el algoritmo, en primer lugar la similitud se inicializa con el valor de uno (línea 1).",
        "Si los conceptos son iguales entre sí, entonces la similitud será uno (líneas 2-4).",
        "De lo contrario, calculamos el ancestro común de los dos nodos y la distancia de cada concepto al ancestro común sin considerar al hermano (líneas 5-7).",
        "Si uno de los conceptos es igual al padre común, entonces no hay relación de hermanos entre los conceptos.",
        "Para cada nivel, multiplicamos la similitud por m y no consideramos el factor de hermanos en la estimación de la similitud.",
        "Como resultado, disminuimos la similitud en cada nivel con la tasa de m (línea 9).",
        "De lo contrario, tiene que existir una relación de hermanos.",
        "Esto significa que debemos considerar el efecto de n al medir la similitud.",
        "Recuerde que hemos contado N1+N2 aristas entre los conceptos.",
        "Dado que existe una relación de hermanos, dos de estos bordes constituyen la relación de hermanos.",
        "Por lo tanto, al calcular el efecto de la relación parental, utilizamos N1+N2 −2 aristas (línea 11).",
        "Algunas estimaciones de similitud relacionadas con la taxonomía en la Figura 2 se presentan en la Tabla 2.",
        "En este ejemplo, se toma m como 2/3 y n como 4/7.",
        "Tabla 2: Estimación de similitud de muestra sobre la taxonomía de muestra. Similitud(ColorRojo, Rosa) = 1 ∗ (2/3) = 0.6666667 Similitud(Rojo, Rosa) = 1 ∗ (4/7) = 0.5714286 Similitud(CualquierColorVino, Rosa) = 1 ∗ (2/3)2 = 0.44444445 Similitud(Blanco, Rosa) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 Para todas las métricas de similitud semántica en nuestra arquitectura, la taxonomía de características se mantiene en la ontología compartida.",
        "Para evaluar la similitud del vector de características, primero estimamos la similitud para cada característica individualmente y luego calculamos la suma promedio de estas similitudes.",
        "Entonces, el resultado es igual a la similitud semántica promedio de todo el vector de características. 6.",
        "SISTEMA DESARROLLADO Hemos implementado nuestra arquitectura en Java.",
        "Para facilitar las pruebas del sistema, el agente del consumidor tiene una interfaz de usuario que nos permite ingresar varias solicitudes.",
        "El agente productor está completamente automatizado y las operaciones de aprendizaje y oferta de servicios funcionan como se explicó anteriormente.",
        "En esta sección, explicamos los detalles de implementación del sistema desarrollado.",
        "Utilizamos OWL [11] como nuestro lenguaje de ontología y JENA como nuestro razonador de ontología.",
        "La ontología compartida es la versión modificada de la Ontología del Vino [19].",
        "Incluye la descripción del vino como concepto y diferentes tipos de vino.",
        "Todos los participantes de la negociación utilizan esta ontología para entenderse mutuamente.",
        "Según la ontología, siete propiedades conforman el concepto de vino.",
        "El agente consumidor y el agente productor obtienen los valores posibles para estas propiedades consultando la ontología.",
        "Por lo tanto, todos los valores posibles para los componentes del concepto del vino, como el color, cuerpo, azúcar, etc., pueden ser alcanzados por ambos agentes.",
        "También se describen en esta ontología una variedad de tipos de vino como Borgoña, Chardonnay, Chenin Blanc, entre otros.",
        "Intuitivamente, cualquier tipo de vino descrito en la ontología también representa un concepto de vino.",
        "Esto nos permite considerar las instancias de vino Chardonnay como instancias de la clase Vino.",
        "Además de la descripción del vino, la información jerárquica de algunas características se puede inferir de la ontología.",
        "Por ejemplo, podemos representar la información de que el continente europeo abarca países occidentales.",
        "El país occidental abarca la región francesa, que incluye algunos territorios como el Loira, Burdeos, entre otros.",
        "Esta información jerárquica se utiliza en la estimación de similitud semántica.",
        "En esta parte, se pueden hacer algunos razonamientos como si un concepto X abarca Y y Y abarca Z, entonces el concepto X abarca Z.",
        "Por ejemplo, el Continente Europeo abarca Burdeos. 1306 El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Para algunas características como cuerpo, sabor y azúcar, no hay información jerárquica, pero sus valores están nivelados semánticamente.",
        "Cuando eso sucede, proporcionamos los valores de similitud razonables para estas características.",
        "Por ejemplo, el cuerpo puede ser ligero, medio o fuerte.",
        "En este caso, asumimos que la luz es 0.66 similar a media pero solo 0.33 a fuerte.",
        "La ontología de WineStock es el inventario de los productores y describe una clase de producto como WineProduct.",
        "Esta clase es necesaria para que el productor registre los vinos que vende.",
        "La ontología implica a los individuos de esta clase.",
        "Los individuos representan los servicios disponibles que posee el productor.",
        "Hemos preparado dos ontologías de WineStock separadas para realizar pruebas.",
        "En la primera ontología, hay 19 productos de vino disponibles y en la segunda ontología, hay 50 productos.",
        "EVALUACIÓN DEL RENDIMIENTO Evaluamos el rendimiento de los sistemas propuestos en relación con la técnica de aprendizaje que utilizaron, DCEA e ID3, comparándolos con CEA, RO (para oferta aleatoria) y SCR (oferta basada solo en la solicitud actual).",
        "Aplicamos una variedad de escenarios en este conjunto de datos para ver las diferencias de rendimiento.",
        "Cada escenario de prueba contiene una lista de preferencias para el usuario y el número de coincidencias de la lista de productos.",
        "La Tabla 3 muestra estas preferencias y la disponibilidad de esos productos en el inventario para los primeros cinco escenarios.",
        "Ten en cuenta que estas preferencias son internas al consumidor y el productor intenta aprenderlas durante la negociación.",
        "Tabla 3: Disponibilidad de vinos en diferentes escenarios de prueba ID Preferencia del consumidor Disponibilidad (de 19) 1 Vino seco 15 2 Vino tinto y seco 8 3 Vino tinto, seco y moderado 4 4 Vino tinto y fuerte 2 5 Vino tinto o rosado, y fuerte 3 7.1 Comparación de Algoritmos de Aprendizaje En la comparación de algoritmos de aprendizaje, utilizamos los cinco escenarios de la Tabla 3.",
        "Aquí, primero usamos la medida de similitud de Tversky.",
        "Con estos casos de prueba, estamos interesados en encontrar el número de iteraciones que se requieren para que el productor genere una oferta aceptable para el consumidor.",
        "Dado que el rendimiento también depende de la solicitud inicial, repetimos nuestros experimentos con diferentes solicitudes iniciales.",
        "Por consiguiente, para cada caso, ejecutamos los algoritmos cinco veces con varias variaciones de las solicitudes iniciales.",
        "En cada experimento, contamos el número de iteraciones necesarias para llegar a un acuerdo.",
        "Tomamos el promedio de estos números para evaluar estos sistemas de manera justa.",
        "Como es costumbre, probamos cada algoritmo con las mismas solicitudes iniciales.",
        "La Tabla 4 compara los enfoques utilizando diferentes algoritmos de aprendizaje.",
        "Cuando las partes grandes del inventario son compatibles con las preferencias de los clientes, como en el primer caso de prueba, el rendimiento de todas las técnicas es casi el mismo (por ejemplo, Escenario 1).",
        "A medida que el número de servicios compatibles disminuye, RO funciona mal como se esperaba.",
        "El segundo peor método es SCR ya que solo considera la solicitud más reciente de los clientes y no aprende de las solicitudes anteriores.",
        "CEA da los mejores resultados cuando puede generar una respuesta pero no puede manejar los casos que contienen preferencias disyuntivas, como el que se presenta en el Escenario 5.",
        "ID3 y DCEA logran los mejores resultados.",
        "Su rendimiento es comparable y pueden manejar todos los casos, incluido el Escenario 5.",
        "Tabla 4: Comparación de algoritmos de aprendizaje en términos del número promedio de interacciones. Ejecutar DCEA SCR RO CEA ID3 Escenario 1: 1.2 1.4 1.2 1.2 1.2 Escenario 2: 1.4 1.4 2.6 1.4 1.4 Escenario 3: 1.4 1.8 4.4 1.4 1.4 Escenario 4: 2.2 2.8 9.6 1.8 2 Escenario 5: 2 2.6 7.6 1.75+ Sin oferta 1.8 Promedio de todos los casos: 1.64 2 5.08 1.51+Sin oferta 1.56 7.2 Comparación de Métricas de Similitud Para comparar las métricas de similitud que se explicaron en la Sección 5, fijamos el algoritmo de aprendizaje en DCEA.",
        "Además de los escenarios mostrados en la Tabla 3, agregamos los siguientes cinco nuevos escenarios considerando la información jerárquica. • El cliente desea comprar vino cuya bodega esté ubicada en California y cuya uva sea de tipo blanco.",
        "Además, la bodega del vino no debería ser costosa.",
        "Solo hay cuatro productos que cumplen con estas condiciones. • El cliente quiere comprar vino de color rojo o rosado y de tipo de uva tinta.",
        "Además, la ubicación del vino debe ser en Europa.",
        "Se desea que el grado de dulzura sea seco o semiseco.",
        "El sabor debe ser delicado o moderado, mientras que el cuerpo debe ser medio o ligero.",
        "Además, la bodega del vino debería ser una bodega cara.",
        "Hay dos productos que cumplen con todos estos requisitos. El cliente quiere comprar vino rosado moderado, que se encuentra alrededor de la región francesa.",
        "La categoría de bodega debería ser Bodega Moderada.",
        "Solo hay un producto que cumple con estos requisitos. • El cliente quiere comprar vino tinto caro, que se encuentra alrededor de la Región de California o vino blanco barato, que se encuentra alrededor de la Región de Texas.",
        "Hay cinco productos disponibles. • El cliente quiere comprar un vino blanco delicado cuyo productor esté en la categoría de Bodega Costosa.",
        "Hay dos productos disponibles.",
        "Los primeros siete escenarios se prueban con el primer conjunto de datos que contiene un total de 19 servicios y los últimos tres escenarios se prueban con el segundo conjunto de datos que contiene 50 servicios.",
        "La Tabla 5 muestra la evaluación del rendimiento en términos del número de interacciones necesarias para llegar a un consenso.",
        "La métrica de Tversky da los peores resultados ya que no considera la similitud semántica.",
        "El rendimiento de Lins es mejor que el de Tversky pero peor que el de otros.",
        "La métrica de Wu-Palmer y la medida de similitud de RP casi ofrecen el mismo rendimiento y son mejores que otras.",
        "Cuando se examinan los resultados, considerar la cercanía semántica aumenta el rendimiento. 8.",
        "DISCUSIÓN Revisamos la literatura reciente en comparación con nuestro trabajo.",
        "Tama et al. [16] proponen un nuevo enfoque basado en ontología para la negociación.",
        "Según su enfoque, los protocolos de negociación utilizados en el comercio electrónico pueden ser modelados como ontologías.",
        "Por lo tanto, los agentes pueden llevar a cabo un protocolo de negociación utilizando esta ontología compartida sin necesidad de estar codificados con los detalles del protocolo de negociación.",
        "Mientras tanto, la Sexta Conferencia Internacional.",
        "La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1307 Tabla 5: Comparación de métricas de similitud en términos de número de interacciones. Ejecutar Tversky Lin Wu Palmer RP Escenario 1: 1.2 1.2 1 1 Escenario 2: 1.4 1.4 1.6 1.6 Escenario 3: 1.4 1.8 2 2 Escenario 4: 2.2 1 1.2 1.2 Escenario 5: 2 1.6 1.6 1.6 Escenario 6: 5 3.8 2.4 2.6 Escenario 7: 3.2 1.2 1 1 Escenario 8: 5.6 2 2 2.2 Escenario 9: 2.6 2.2 2.2 2.6 Escenario 10: 4.4 2 2 1.8 Promedio de todos los casos: 2.9 1.82 1.7 1.76 Tama et al. modelan el protocolo de negociación utilizando ontologías, en cambio, nosotros hemos modelado el servicio a ser negociado.",
        "Además, hemos construido un sistema con el cual se pueden aprender las preferencias de negociación.",
        "El estudio de Sadri et al. analiza la negociación en el contexto de la asignación de recursos [14].",
        "Los agentes tienen recursos limitados y necesitan solicitar recursos faltantes a otros agentes.",
        "Se propone un mecanismo basado en secuencias de diálogo entre agentes como solución.",
        "El mecanismo se basa en el ciclo de agente de observar-pensar-actuar.",
        "Estos diálogos incluyen ofrecer recursos, intercambios de recursos y ofrecer recursos alternativos.",
        "Cada agente en el sistema planea sus acciones para alcanzar un estado objetivo.",
        "A diferencia de nuestro enfoque, el estudio de Sadri et al. no se preocupa por las preferencias de aprendizaje mutuas.",
        "Brzostowski y Kowalczyk proponen un enfoque para seleccionar un socio de negociación adecuado investigando negociaciones previas de múltiples atributos [1].",
        "Para lograr esto, utilizan el razonamiento basado en casos.",
        "Su enfoque es probabilístico ya que el comportamiento de los socios puede cambiar en cada iteración.",
        "En nuestro enfoque, estamos interesados en negociar el contenido del servicio.",
        "Después de que el consumidor y el productor acuerden el servicio, se pueden utilizar mecanismos de negociación orientados al precio para acordar el precio.",
        "Fatima et al. estudian los factores que afectan la negociación, como las preferencias, el plazo, el precio, entre otros, ya que el agente que desarrolla una estrategia contra su oponente debe considerar todos ellos [5].",
        "En su enfoque, el objetivo del agente vendedor es vender el servicio al precio más alto posible, mientras que el objetivo del agente comprador es comprar el bien al precio más bajo posible.",
        "El intervalo de tiempo afecta a estos agentes de manera diferente.",
        "En comparación con Fatima et al., nuestro enfoque es diferente.",
        "Mientras ellos estudian el efecto del tiempo en la negociación, nuestro enfoque está en aprender las preferencias para una negociación exitosa.",
        "Faratin et al. proponen un mecanismo de negociación multi-tema, donde las variables de servicio para la negociación, como el precio, la calidad del servicio, entre otros, se consideran intercambios entre sí (es decir, un precio más alto por una entrega más temprana) [4].",
        "Generan un modelo heurístico para compensaciones que incluye la estimación de similitud difusa y una exploración de escalada de colina para ofertas posiblemente aceptables.",
        "Aunque abordamos un problema similar, aprendemos las preferencias del cliente con la ayuda del aprendizaje inductivo y generamos contraofertas de acuerdo con estas preferencias aprendidas.",
        "Faratin et al. solo utilizan la última oferta realizada por el consumidor al calcular la similitud para elegir la contraoferta.",
        "A diferencia de ellos, también tenemos en cuenta las solicitudes previas del consumidor.",
        "En sus experimentos, Faratin et al. asumen que los pesos de las variables de servicio están fijos a priori.",
        "Por el contrario, aprendemos estas preferencias con el tiempo.",
        "En nuestro trabajo futuro, planeamos integrar el razonamiento ontológico en el algoritmo de aprendizaje para que la información jerárquica pueda ser aprendida a partir de la jerarquía de subsumpción de relaciones.",
        "Además, al utilizar las relaciones entre las características, el productor puede descubrir nuevos conocimientos a partir de los conocimientos existentes.",
        "Estas son direcciones interesantes que seguiremos en nuestro trabajo futuro. 9.",
        "REFERENCIAS [1] J. Brzostowski y R. Kowalczyk.",
        "En el razonamiento basado en casos posibilístico para la selección de socios para la negociación de agentes de múltiples atributos.",
        "En Actas del 4to Congreso Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 273-278, 2005. [2] L. Busch e I. Horstman.",
        "Un comentario sobre negociaciones tema por tema.",
        "Juegos y Comportamiento Económico, 19:144-148, 1997. [3] J. K. Debenham.",
        "Gestión de la negociación en el mercado electrónico en el contexto de un sistema multiagente.",
        "En Actas de la 21ª Conferencia Internacional sobre Sistemas Basados en el Conocimiento e Inteligencia Artificial Aplicada, ES2002:, 2002. [4] P. Faratin, C. Sierra y N. R. Jennings.",
        "Utilizando criterios de similitud para hacer compensaciones de problemas en negociaciones automatizadas.",
        "Inteligencia Artificial, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge y N. Jennings.",
        "Agentes óptimos para negociaciones de múltiples temas.",
        "En Actas del 2do Congreso Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 129-136, 2003. [6] C. Giraud-Carrier.",
        "Una nota sobre la utilidad del aprendizaje incremental.",
        "Comunicaciones de IA, 13(4):215-223, 2000. [7] T.-P. Hong y S.-S. Tseng.",
        "Dividiendo y fusionando espacios de versiones para aprender conceptos disyuntivos.",
        "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.\n\nTraducción al español:\nIEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
        "Una definición de similitud basada en teoría de la información.",
        "En Actas de la 15ª Conferencia Internacional sobre Aprendizaje Automático, páginas 296-304.",
        "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, y A. G. Moukas.",
        "Agentes que compran y venden.",
        "Comunicaciones de la ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
        "Aprendizaje automático.",
        "McGraw Hill, NY, 1997. [11] Búho.",
        "OWL: Guía del lenguaje de ontologías web, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal y S. C. K. Shiu.",
        "Fundamentos del Razonamiento Basado en Casos Blandos.",
        "John Wiley & Sons, Nueva Jersey, 2004. [13] J. R. Quinlan.",
        "Inducción de árboles de decisión.",
        "Aprendizaje automático, 1(1):81-106, 1986. [14] F. Sadri, F. Toni y P. Torroni.",
        "Diálogos para negociación: Variedades de agentes y secuencias de diálogo.",
        "En ATAL 2001, Artículos Revisados, volumen 2333 de LNAI, páginas 405-421.",
        "Springer-Verlag, 2002. [15] M. P. Singh. \n\nSpringer-Verlag, 2002. [15] M. P. Singh.",
        "Comercio electrónico orientado al valor.",
        "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, y M. Wooldridge.",
        "Ontologías para apoyar la negociación en el comercio electrónico.",
        "Aplicaciones de la Inteligencia Artificial en Ingeniería, 18:223-236, 2005. [17] A. Tversky.",
        "Características de similitud.",
        "Revisión Psicológica, 84(4):327-352, 1977. [18] P. E. Utgoff.",
        "Inducción incremental de árboles de decisión.",
        "Aprendizaje automático, 4:161-186, 1989. [19] Vino, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu y M. Palmer.",
        "Semántica de verbos y selección léxica.",
        "En el 32.",
        "Reunión anual de la Asociación de Lingüística Computacional, páginas 133-138, 1994. 1308 La Sexta Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07)"
    ],
    "error_count": 7,
    "keys": {
        "service": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the <br>service</br> consumers and providers to negotiate their <br>service</br> needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on <br>service</br> content and focus on finding a consensus on <br>service</br> price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a <br>service</br>.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a <br>service</br>.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat <br>service</br> price as the primary construct for negotiation by assuming that the <br>service</br> content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the <br>service</br> have already been agreed upon.",
                "Nevertheless, many times the <br>service</br> provider may not be offering the exact requested <br>service</br> due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested <br>service</br> [15].",
                "However, most existing negotiation approaches assume that all features of a <br>service</br> are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a <br>service</br> may be important for one consumer whereas the quality of the <br>service</br> may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the <br>service</br> components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the <br>service</br> providers learn the relevant features of a <br>service</br> for a particular customer over time.",
                "We represent <br>service</br> requests as a vector of <br>service</br> features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given <br>service</br>.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular <br>service</br> we have used is a wine selling <br>service</br>.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "<br>service</br> Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different <br>service</br> offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the <br>service</br>.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the <br>service</br> that is being negotiated.",
                "Since a <br>service</br> can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the <br>service</br> that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular <br>service</br> request.",
                "The request is composed of significant features of the <br>service</br>.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some <br>service</br> is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the <br>service</br> producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • <br>service</br> Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over <br>service</br> features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a <br>service</br> since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the <br>service</br> description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the <br>service</br> description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine <br>service</br>.",
                "When we want to find whether the given <br>service</br> description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "<br>service</br> OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 <br>service</br> Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The <br>service</br> offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering <br>service</br>.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the <br>service</br> set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every <br>service</br> in the <br>service</br> list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the <br>service</br> list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching <br>service</br>, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the <br>service</br> as requested, it tries to find the <br>service</br> that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one <br>service</br> with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each <br>service</br> and the producer prefers the higher rated <br>service</br> to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the <br>service</br> list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative <br>service</br> Offering Mechanisms In addition to these three <br>service</br> offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random <br>service</br> Offering (RO): The producer generates a counter offer randomly from the available <br>service</br> list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which <br>service</br> is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take <br>service</br> s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,<br>service</br>)) denote the similarity of hypothesis h with the given <br>service</br>.",
                "We compute the similarity of each hypothesis with the given <br>service</br> and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the <br>service</br> by the number of all samples that are covered in S. AV G−SM(<br>service</br>S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and <br>service</br> offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the <br>service</br> to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the <br>service</br>.",
                "After the consumer and producer agree on the <br>service</br>, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the <br>service</br> for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the <br>service</br> variables for the negotiation such as price, quality of the <br>service</br>, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for <br>service</br> variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "This requires the <br>service</br> consumers and providers to negotiate their <br>service</br> needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on <br>service</br> content and focus on finding a consensus on <br>service</br> price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a <br>service</br>.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a <br>service</br>.",
                "INTRODUCTION Current approaches to e-commerce treat <br>service</br> price as the primary construct for negotiation by assuming that the <br>service</br> content is fixed [9]."
            ],
            "translated_annotated_samples": [
                "Esto requiere que los consumidores y proveedores de <br>servicio</br>s negocien sus necesidades y ofertas de <br>servicio</br>.",
                "Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el <br>contenido del servicio</br> y se centran en encontrar un consenso sobre el precio del servicio.",
                "Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un <br>servicio</br>.",
                "En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un <br>servicio</br>.",
                "INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del <br>servicio</br> como el principal elemento para la negociación al asumir que el contenido del <br>servicio</br> está fijo [9]."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de <br>servicio</br>s negocien sus necesidades y ofertas de <br>servicio</br>. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el <br>contenido del servicio</br> y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un <br>servicio</br>. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un <br>servicio</br>. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del <br>servicio</br> como el principal elemento para la negociación al asumir que el contenido del <br>servicio</br> está fijo [9]. ",
            "candidates": [],
            "error": [
                [
                    "servicio",
                    "servicio",
                    "contenido del servicio",
                    "servicio",
                    "servicio",
                    "servicio",
                    "servicio"
                ]
            ]
        },
        "price": {
            "translated_key": "precio",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service <br>price</br>.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service <br>price</br> as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on <br>price</br> presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the <br>price</br> [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, <br>price</br>-oriented negotiation mechanisms can be used to agree on the <br>price</br>.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, <br>price</br> and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible <br>price</br> whereas the goal of the buyer agent is to buy the good with the lowest possible <br>price</br>.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as <br>price</br>, quality of the service, and so on are considered traded-offs against each other (i.e., higher <br>price</br> for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service <br>price</br>.",
                "INTRODUCTION Current approaches to e-commerce treat service <br>price</br> as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on <br>price</br> presupposes that other properties of the service have already been agreed upon.",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the <br>price</br> [5, 2].",
                "After the consumer and producer agree on the service, <br>price</br>-oriented negotiation mechanisms can be used to agree on the <br>price</br>."
            ],
            "translated_annotated_samples": [
                "Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el <br>precio</br> del servicio.",
                "INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el <br>precio</br> del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9].",
                "Sin embargo, la negociación sobre el <br>precio</br> presupone que otras propiedades del servicio ya han sido acordadas.",
                "Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el <br>precio</br> [5, 2].",
                "Después de que el consumidor y el productor acuerden el servicio, se pueden utilizar mecanismos de negociación orientados al <br>precio</br> para acordar el <br>precio</br>."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el <br>precio</br> del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el <br>precio</br> del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el <br>precio</br> presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el <br>precio</br> [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor. La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor. Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios. Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las preferencias del consumidor y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta. Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor. Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles. Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor. Este desafío se puede estudiar en tres etapas: • Aprendizaje de preferencias: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5) APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica. Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio. Por ejemplo, un consumidor puede tener preferencia por el vino tinto. Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo. Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo). Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor. Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores. Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa. Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo. Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6]. Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones. Investigamos particularmente dos enfoques. El primero es el aprendizaje inductivo. Esta técnica se aplica para aprender las preferencias como conceptos. Desarrollamos el Algoritmo de Eliminación de Candidatos (CEA) para el Espacio de Versiones [10]. Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva. Curiosamente, la mayoría de las veces las preferencias del consumidor son disyuntivas. Estamos considerando un agente que está comprando vino. El consumidor puede preferir vino tinto o vino rosado pero no vino blanco. Para utilizar CEA con tales preferencias, es necesaria una modificación sólida. El segundo enfoque son los árboles de decisión. Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas. Un árbol de decisión incremental bien conocido es ID5R [18]. Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional. Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados. El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender. El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener. Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto. El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo. Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío. Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo. En cada interacción entre el productor y el consumidor, tanto G como S son modificados. Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas. Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento. Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa. Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí. Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 CEA Disyuntivo Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos. Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos. Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones. Algunos de estos enfoques utilizan múltiples espacios de versión. Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7]. Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir hipótesis disyuntivas además de las conjunciones y la negación. Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica. EJEMPLO 1. Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco). El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor. Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco. En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}. Solo cuando todos los valores existan en la lista, serán reemplazados por ?. En otras palabras, permitimos que el algoritmo generalice más lentamente que antes. Modificamos el algoritmo CEA para hacer frente a este cambio. El algoritmo modificado, DCEA, se presenta como Algoritmo 1. Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de múltiples espacios de versiones. La fase de inicialización es la misma que el algoritmo original (líneas 1, 2). Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4). Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí. Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran. Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7). Cuando llega una muestra negativa, no cambiamos S como antes. Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15). A diferencia del CEA original, intentamos especializar el G mínimamente. El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13). Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada. Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez. Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14). Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar. Por ejemplo, considera el atributo del color. Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2. La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA. El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra. Después de la primera muestra positiva, S se generaliza para cubrir también la instancia. La segunda muestra es negativa. Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada. En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general. La tercera muestra es positiva y generaliza S aún más. Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco). Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis. Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}. Por el Algoritmo 1, no perdemos ninguna información. 3.3 ID3 ID3 [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor. Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos. El algoritmo ID3 se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas. Hay dos clases: positiva y negativa. Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor. Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas. El árbol de decisión tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba. El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio. Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos. Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja. El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender. Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos. Después de cada solicitud entrante, el árbol de decisiones se reconstruye. Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso. Sin embargo, en la práctica hemos evaluado que el ID3 es rápido y el costo de reconstrucción es insignificante. 4. OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA. El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes. Cuando el productor recibe una solicitud del consumidor, el conjunto de aprendizaje del productor se entrena con esta solicitud como una muestra positiva. Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios. El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor. En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor. El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores. Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios. Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios. Esta lista constituye el inventario de servicios de los proveedores. Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace. Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor. Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación. Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S). Calculamos las similitudes de varias maneras, como se explicará en la Sección 5. Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud. El agente productor puede romper el empate de varias maneras. Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de ID3 Si el productor aprende las preferencias de los consumidores con ID3, se aplica un mecanismo similar con dos diferencias. Primero, dado que ID3 no mantiene G, se eliminan de la lista de servicios aquellos no aceptados que se clasifican como negativos. Segundo, las similitudes de los posibles servicios no se miden con respecto a S, sino en cambio a todas las solicitudes previamente realizadas. 4.3 Mecanismos Alternativos de Oferta de Servicios Además de estos tres mecanismos de oferta de servicios (Oferta de Servicio con CEA, Oferta de Servicio con DCEA y Oferta de Servicio con ID3), incluimos otros dos mecanismos. 1304 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) • Oferta de Servicio Aleatoria (RO): El productor genera una contraoferta aleatoriamente de la lista de servicios disponibles, sin considerar las preferencias de los consumidores. • Oferta de Servicio considerando solo la solicitud actual (SCR): El productor selecciona una contraoferta de acuerdo con la similitud de la solicitud actual del consumidor pero no considera solicitudes anteriores. 5. ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una métrica de similitud que toma dos entradas y devuelve qué tan similares son. Existen varios métricos de similitud utilizados en sistemas de razonamiento basado en casos, como la suma ponderada de la distancia euclidiana, la distancia de Hamming, entre otros [12]. La métrica de similitud afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor. Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de similitud semántica llamada Similitud RP. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente. En la Ecuación (1), común representa la cantidad de atributos coincidentes, mientras que diferente representa la cantidad de atributos diferentes. Nuestra suposición actual es que α y β son iguales entre sí. SMpq = α(común) α(común) + β(diferente) (1) Aquí, al comparar dos características, asignamos cero para la disimilitud y uno para la similitud al omitir la cercanía semántica entre los valores de las características. La métrica de similitud de Tversky está diseñada para comparar dos vectores de características. En nuestro sistema, mientras que la lista de servicios que puede ofrecer el productor son cada uno un vector de características, el conjunto más específico S no es un vector de características. S consiste en hipótesis de vectores de características. Por lo tanto, estimamos la similitud de cada hipótesis dentro del conjunto más específico S y luego calculamos el promedio de las similitudes. EJEMPLO 3. Suponga que S contiene las siguientes dos hipótesis: { {Luz, Moderado, (Rojo, Blanco)} , {Completo, Fuerte, Rosa}}. Toma el servicio s como (Ligero, Resistente, Rosa). Entonces, la similitud del primero es igual a 1/3 y la del segundo es igual a 2/3 de acuerdo con la Ecuación (1). Normalmente, tomamos el promedio de ello y obtenemos (1/3 + 2/3)/2, que es igual a 1/2. Sin embargo, la primera hipótesis implica el efecto de dos solicitudes y la segunda hipótesis implica solo una solicitud. Por lo tanto, esperamos que el efecto de la primera hipótesis sea mayor que el de la segunda. Por lo tanto, calculamos la similitud promedio teniendo en cuenta la cantidad de muestras que las hipótesis cubren. Que ch denote el número de muestras que cubre la hipótesis h y (SM(h,servicio)) denote la similitud de la hipótesis h con el servicio dado. Calculamos la similitud de cada hipótesis con el servicio dado y las ponderamos con el número de muestras que cubren. Encontramos la similitud dividiendo la suma ponderada de las similitudes de todas las hipótesis en S con el servicio por el número de todas las muestras que están cubiertas en S. AV G−SM(servicio, S) = |S| |h| (ch ∗ SM(h, servicio)) |S| |h| ch (2) Figura 2: Taxonomía de muestra para estimación de similitud EJEMPLO 4. Para el ejemplo anterior, la similitud de (Luz, Fuerte, Rosa) con el conjunto específico es (2 ∗ 1/3 + 2/3)/3, igual a 4/9. El número posible de muestras que abarca una hipótesis se puede estimar multiplicando las cardinalidades de cada atributo. Por ejemplo, la cardinalidad del primer atributo es dos y la de los demás es igual a uno para la hipótesis dada, como {Luz, Moderado, (Rojo, Blanco)}. Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La métrica de similitud de Lins Un taxonomía puede ser utilizada al estimar la similitud semántica entre dos conceptos. Estimar la similitud semántica en una taxonomía de tipo Es-Un se puede hacer calculando la distancia entre los nodos relacionados con los conceptos comparados. Los enlaces entre los nodos pueden considerarse como distancias. Entonces, la longitud del camino entre los nodos indica qué tan similares son los conceptos. Una estimación alternativa para utilizar el contenido de información en la estimación de la similitud semántica en lugar del método de conteo de aristas, fue propuesta por Lin [8]. La ecuación (3) [8] muestra la similitud de Lin donde c1 y c2 son los conceptos comparados y c0 es el concepto más específico que subsume a ambos. Además, P(C) representa la probabilidad de que un objeto seleccionado arbitrariamente pertenezca al concepto C. La similitud(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Métrica de similitud de Wu y Palmers Diferente de Lin, Wu y Palmer utilizan la distancia entre los nodos en la taxonomía ES-UN [20]. La similitud semántica se representa con la Ecuación (4) [20]. Aquí, se estima la similitud entre c1 y c2 y c0 es el concepto más específico que subsume estas clases. N1 es el número de aristas entre c1 y c0. N2 es el número de aristas entre c2 y c0. N0 es el número de enlaces IS-A de c0 desde la raíz de la taxonomía. Proponemos estimar la distancia relativa en una taxonomía entre dos conceptos utilizando las siguientes intuiciones. Utilizamos la Figura 2 para ilustrar estas intuiciones. • Padre versus abuelo: El padre de un nodo es más similar al nodo que los abuelos de ese. Generalización del Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1305 es un concepto que razonablemente resulta en alejarse más de ese concepto. Cuanto más generales son los conceptos, menos similares son. Por ejemplo, AnyWineColor es el padre de ReddishColor y ReddishColor es el padre de Red. Entonces, esperamos que la similitud entre ReddishColor y Red sea mayor que la similitud entre AnyWineColor y Red. • Padre versus hermano: Un nodo tendría una similitud mayor con su padre que con su hermano. Por ejemplo, Rojo y Rosa son hijos de ColorRojo. En este caso, esperamos que la similitud entre Rojo y ColorRojizo sea mayor que la de Rojo y Rosa. • Hermano versus abuelo: Un nodo es más similar a su hermano que a su abuelo. Para ilustrar, AnyWineColor es el abuelo de Red, y Red y Rose son hermanos. Por lo tanto, posiblemente anticipamos que Rojo y Rosa son más similares que CualquierColorDeVino y Rojo. Como una taxonomía está representada en un árbol, ese árbol puede ser recorrido desde el primer concepto que se está comparando hasta el segundo concepto. En el nodo inicial relacionado con el primer concepto, el valor de similitud es constante y igual a uno. Este valor se reduce por una constante en cada nodo visitado a lo largo del camino que llegará al nodo que incluye el segundo concepto. Cuanto más corto sea el camino entre los conceptos, mayor será la similitud entre los nodos. Algoritmo 2 Estimar-Similitud-RP(c1,c2) Requerido: Las constantes deben ser m > n > m2 donde m, n ∈ R[0, 1] 1: Similitud ← 1 2: si c1 es igual a c2 entonces 3: Devolver Similitud 4: fin si 5: padreComun ← encontrarPadreComun(c1, c2) {padreComun es el concepto más específico que cubre tanto c1 como c2} 6: N1 ← encontrarDistancia(padreComun, c1) 7: N2 ← encontrarDistancia(padreComun, c2) {N1 y N2 son el número de enlaces entre el concepto y el concepto padre} 8: si (padreComun == c1) o (padreComun == c2) entonces 9: Similitud ← Similitud ∗ m(N1+N2) 10: sino 11: Similitud ← Similitud ∗ n ∗ m(N1+N2−2) 12: fin si 13: Devolver Similitud La distancia relativa entre los nodos c1 y c2 se estima de la siguiente manera. Comenzando desde c1, se recorre el árbol para llegar a c2. En cada salto, la similitud disminuye ya que los conceptos se están alejando cada vez más entre sí. Sin embargo, según nuestras intuiciones, no todos los saltos disminuyen la similitud de igual manera. Que m represente el factor para saltar de un hijo a un padre y que n represente el factor para saltar de un hermano a otro hermano. Dado que saltar de un nodo a su abuelo cuenta como dos saltos de padre, el factor de descuento al moverse de un nodo a su abuelo es m2. De acuerdo con las intuiciones anteriores, nuestras constantes deben estar en la forma m > n > m2 donde el valor de m y n debe estar entre cero y uno. El algoritmo 2 muestra el cálculo de la distancia. Según el algoritmo, en primer lugar la similitud se inicializa con el valor de uno (línea 1). Si los conceptos son iguales entre sí, entonces la similitud será uno (líneas 2-4). De lo contrario, calculamos el ancestro común de los dos nodos y la distancia de cada concepto al ancestro común sin considerar al hermano (líneas 5-7). Si uno de los conceptos es igual al padre común, entonces no hay relación de hermanos entre los conceptos. Para cada nivel, multiplicamos la similitud por m y no consideramos el factor de hermanos en la estimación de la similitud. Como resultado, disminuimos la similitud en cada nivel con la tasa de m (línea 9). De lo contrario, tiene que existir una relación de hermanos. Esto significa que debemos considerar el efecto de n al medir la similitud. Recuerde que hemos contado N1+N2 aristas entre los conceptos. Dado que existe una relación de hermanos, dos de estos bordes constituyen la relación de hermanos. Por lo tanto, al calcular el efecto de la relación parental, utilizamos N1+N2 −2 aristas (línea 11). Algunas estimaciones de similitud relacionadas con la taxonomía en la Figura 2 se presentan en la Tabla 2. En este ejemplo, se toma m como 2/3 y n como 4/7. Tabla 2: Estimación de similitud de muestra sobre la taxonomía de muestra. Similitud(ColorRojo, Rosa) = 1 ∗ (2/3) = 0.6666667 Similitud(Rojo, Rosa) = 1 ∗ (4/7) = 0.5714286 Similitud(CualquierColorVino, Rosa) = 1 ∗ (2/3)2 = 0.44444445 Similitud(Blanco, Rosa) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 Para todas las métricas de similitud semántica en nuestra arquitectura, la taxonomía de características se mantiene en la ontología compartida. Para evaluar la similitud del vector de características, primero estimamos la similitud para cada característica individualmente y luego calculamos la suma promedio de estas similitudes. Entonces, el resultado es igual a la similitud semántica promedio de todo el vector de características. 6. SISTEMA DESARROLLADO Hemos implementado nuestra arquitectura en Java. Para facilitar las pruebas del sistema, el agente del consumidor tiene una interfaz de usuario que nos permite ingresar varias solicitudes. El agente productor está completamente automatizado y las operaciones de aprendizaje y oferta de servicios funcionan como se explicó anteriormente. En esta sección, explicamos los detalles de implementación del sistema desarrollado. Utilizamos OWL [11] como nuestro lenguaje de ontología y JENA como nuestro razonador de ontología. La ontología compartida es la versión modificada de la Ontología del Vino [19]. Incluye la descripción del vino como concepto y diferentes tipos de vino. Todos los participantes de la negociación utilizan esta ontología para entenderse mutuamente. Según la ontología, siete propiedades conforman el concepto de vino. El agente consumidor y el agente productor obtienen los valores posibles para estas propiedades consultando la ontología. Por lo tanto, todos los valores posibles para los componentes del concepto del vino, como el color, cuerpo, azúcar, etc., pueden ser alcanzados por ambos agentes. También se describen en esta ontología una variedad de tipos de vino como Borgoña, Chardonnay, Chenin Blanc, entre otros. Intuitivamente, cualquier tipo de vino descrito en la ontología también representa un concepto de vino. Esto nos permite considerar las instancias de vino Chardonnay como instancias de la clase Vino. Además de la descripción del vino, la información jerárquica de algunas características se puede inferir de la ontología. Por ejemplo, podemos representar la información de que el continente europeo abarca países occidentales. El país occidental abarca la región francesa, que incluye algunos territorios como el Loira, Burdeos, entre otros. Esta información jerárquica se utiliza en la estimación de similitud semántica. En esta parte, se pueden hacer algunos razonamientos como si un concepto X abarca Y y Y abarca Z, entonces el concepto X abarca Z. Por ejemplo, el Continente Europeo abarca Burdeos. 1306 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Para algunas características como cuerpo, sabor y azúcar, no hay información jerárquica, pero sus valores están nivelados semánticamente. Cuando eso sucede, proporcionamos los valores de similitud razonables para estas características. Por ejemplo, el cuerpo puede ser ligero, medio o fuerte. En este caso, asumimos que la luz es 0.66 similar a media pero solo 0.33 a fuerte. La ontología de WineStock es el inventario de los productores y describe una clase de producto como WineProduct. Esta clase es necesaria para que el productor registre los vinos que vende. La ontología implica a los individuos de esta clase. Los individuos representan los servicios disponibles que posee el productor. Hemos preparado dos ontologías de WineStock separadas para realizar pruebas. En la primera ontología, hay 19 productos de vino disponibles y en la segunda ontología, hay 50 productos. EVALUACIÓN DEL RENDIMIENTO Evaluamos el rendimiento de los sistemas propuestos en relación con la técnica de aprendizaje que utilizaron, DCEA e ID3, comparándolos con CEA, RO (para oferta aleatoria) y SCR (oferta basada solo en la solicitud actual). Aplicamos una variedad de escenarios en este conjunto de datos para ver las diferencias de rendimiento. Cada escenario de prueba contiene una lista de preferencias para el usuario y el número de coincidencias de la lista de productos. La Tabla 3 muestra estas preferencias y la disponibilidad de esos productos en el inventario para los primeros cinco escenarios. Ten en cuenta que estas preferencias son internas al consumidor y el productor intenta aprenderlas durante la negociación. Tabla 3: Disponibilidad de vinos en diferentes escenarios de prueba ID Preferencia del consumidor Disponibilidad (de 19) 1 Vino seco 15 2 Vino tinto y seco 8 3 Vino tinto, seco y moderado 4 4 Vino tinto y fuerte 2 5 Vino tinto o rosado, y fuerte 3 7.1 Comparación de Algoritmos de Aprendizaje En la comparación de algoritmos de aprendizaje, utilizamos los cinco escenarios de la Tabla 3. Aquí, primero usamos la medida de similitud de Tversky. Con estos casos de prueba, estamos interesados en encontrar el número de iteraciones que se requieren para que el productor genere una oferta aceptable para el consumidor. Dado que el rendimiento también depende de la solicitud inicial, repetimos nuestros experimentos con diferentes solicitudes iniciales. Por consiguiente, para cada caso, ejecutamos los algoritmos cinco veces con varias variaciones de las solicitudes iniciales. En cada experimento, contamos el número de iteraciones necesarias para llegar a un acuerdo. Tomamos el promedio de estos números para evaluar estos sistemas de manera justa. Como es costumbre, probamos cada algoritmo con las mismas solicitudes iniciales. La Tabla 4 compara los enfoques utilizando diferentes algoritmos de aprendizaje. Cuando las partes grandes del inventario son compatibles con las preferencias de los clientes, como en el primer caso de prueba, el rendimiento de todas las técnicas es casi el mismo (por ejemplo, Escenario 1). A medida que el número de servicios compatibles disminuye, RO funciona mal como se esperaba. El segundo peor método es SCR ya que solo considera la solicitud más reciente de los clientes y no aprende de las solicitudes anteriores. CEA da los mejores resultados cuando puede generar una respuesta pero no puede manejar los casos que contienen preferencias disyuntivas, como el que se presenta en el Escenario 5. ID3 y DCEA logran los mejores resultados. Su rendimiento es comparable y pueden manejar todos los casos, incluido el Escenario 5. Tabla 4: Comparación de algoritmos de aprendizaje en términos del número promedio de interacciones. Ejecutar DCEA SCR RO CEA ID3 Escenario 1: 1.2 1.4 1.2 1.2 1.2 Escenario 2: 1.4 1.4 2.6 1.4 1.4 Escenario 3: 1.4 1.8 4.4 1.4 1.4 Escenario 4: 2.2 2.8 9.6 1.8 2 Escenario 5: 2 2.6 7.6 1.75+ Sin oferta 1.8 Promedio de todos los casos: 1.64 2 5.08 1.51+Sin oferta 1.56 7.2 Comparación de Métricas de Similitud Para comparar las métricas de similitud que se explicaron en la Sección 5, fijamos el algoritmo de aprendizaje en DCEA. Además de los escenarios mostrados en la Tabla 3, agregamos los siguientes cinco nuevos escenarios considerando la información jerárquica. • El cliente desea comprar vino cuya bodega esté ubicada en California y cuya uva sea de tipo blanco. Además, la bodega del vino no debería ser costosa. Solo hay cuatro productos que cumplen con estas condiciones. • El cliente quiere comprar vino de color rojo o rosado y de tipo de uva tinta. Además, la ubicación del vino debe ser en Europa. Se desea que el grado de dulzura sea seco o semiseco. El sabor debe ser delicado o moderado, mientras que el cuerpo debe ser medio o ligero. Además, la bodega del vino debería ser una bodega cara. Hay dos productos que cumplen con todos estos requisitos. El cliente quiere comprar vino rosado moderado, que se encuentra alrededor de la región francesa. La categoría de bodega debería ser Bodega Moderada. Solo hay un producto que cumple con estos requisitos. • El cliente quiere comprar vino tinto caro, que se encuentra alrededor de la Región de California o vino blanco barato, que se encuentra alrededor de la Región de Texas. Hay cinco productos disponibles. • El cliente quiere comprar un vino blanco delicado cuyo productor esté en la categoría de Bodega Costosa. Hay dos productos disponibles. Los primeros siete escenarios se prueban con el primer conjunto de datos que contiene un total de 19 servicios y los últimos tres escenarios se prueban con el segundo conjunto de datos que contiene 50 servicios. La Tabla 5 muestra la evaluación del rendimiento en términos del número de interacciones necesarias para llegar a un consenso. La métrica de Tversky da los peores resultados ya que no considera la similitud semántica. El rendimiento de Lins es mejor que el de Tversky pero peor que el de otros. La métrica de Wu-Palmer y la medida de similitud de RP casi ofrecen el mismo rendimiento y son mejores que otras. Cuando se examinan los resultados, considerar la cercanía semántica aumenta el rendimiento. 8. DISCUSIÓN Revisamos la literatura reciente en comparación con nuestro trabajo. Tama et al. [16] proponen un nuevo enfoque basado en ontología para la negociación. Según su enfoque, los protocolos de negociación utilizados en el comercio electrónico pueden ser modelados como ontologías. Por lo tanto, los agentes pueden llevar a cabo un protocolo de negociación utilizando esta ontología compartida sin necesidad de estar codificados con los detalles del protocolo de negociación. Mientras tanto, la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1307 Tabla 5: Comparación de métricas de similitud en términos de número de interacciones. Ejecutar Tversky Lin Wu Palmer RP Escenario 1: 1.2 1.2 1 1 Escenario 2: 1.4 1.4 1.6 1.6 Escenario 3: 1.4 1.8 2 2 Escenario 4: 2.2 1 1.2 1.2 Escenario 5: 2 1.6 1.6 1.6 Escenario 6: 5 3.8 2.4 2.6 Escenario 7: 3.2 1.2 1 1 Escenario 8: 5.6 2 2 2.2 Escenario 9: 2.6 2.2 2.2 2.6 Escenario 10: 4.4 2 2 1.8 Promedio de todos los casos: 2.9 1.82 1.7 1.76 Tama et al. modelan el protocolo de negociación utilizando ontologías, en cambio, nosotros hemos modelado el servicio a ser negociado. Además, hemos construido un sistema con el cual se pueden aprender las preferencias de negociación. El estudio de Sadri et al. analiza la negociación en el contexto de la asignación de recursos [14]. Los agentes tienen recursos limitados y necesitan solicitar recursos faltantes a otros agentes. Se propone un mecanismo basado en secuencias de diálogo entre agentes como solución. El mecanismo se basa en el ciclo de agente de observar-pensar-actuar. Estos diálogos incluyen ofrecer recursos, intercambios de recursos y ofrecer recursos alternativos. Cada agente en el sistema planea sus acciones para alcanzar un estado objetivo. A diferencia de nuestro enfoque, el estudio de Sadri et al. no se preocupa por las preferencias de aprendizaje mutuas. Brzostowski y Kowalczyk proponen un enfoque para seleccionar un socio de negociación adecuado investigando negociaciones previas de múltiples atributos [1]. Para lograr esto, utilizan el razonamiento basado en casos. Su enfoque es probabilístico ya que el comportamiento de los socios puede cambiar en cada iteración. En nuestro enfoque, estamos interesados en negociar el contenido del servicio. Después de que el consumidor y el productor acuerden el servicio, se pueden utilizar mecanismos de negociación orientados al <br>precio</br> para acordar el <br>precio</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "consumer preference": {
            "translated_key": "preferencias del consumidor",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn <br>consumer preference</br>s.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, <br>consumer preference</br>s and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time <br>consumer preference</br>s are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Section 3 explains the learning algorithms that were studied to learn <br>consumer preference</br>s.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, <br>consumer preference</br>s and the producers available services.",
                "Interestingly, most of the time <br>consumer preference</br>s are disjunctive."
            ],
            "translated_annotated_samples": [
                "La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las <br>preferencias del consumidor</br>.",
                "Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las <br>preferencias del consumidor</br> y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta.",
                "Curiosamente, la mayoría de las veces las <br>preferencias del consumidor</br> son disyuntivas."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las <br>preferencias del consumidor</br>. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor. La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor. Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios. Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las <br>preferencias del consumidor</br> y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta. Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor. Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles. Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor. Este desafío se puede estudiar en tres etapas: • Aprendizaje de preferencias: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5) APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica. Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio. Por ejemplo, un consumidor puede tener preferencia por el vino tinto. Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo. Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo). Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor. Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores. Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa. Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo. Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6]. Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones. Investigamos particularmente dos enfoques. El primero es el aprendizaje inductivo. Esta técnica se aplica para aprender las preferencias como conceptos. Desarrollamos el Algoritmo de Eliminación de Candidatos (CEA) para el Espacio de Versiones [10]. Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva. Curiosamente, la mayoría de las veces las <br>preferencias del consumidor</br> son disyuntivas. Estamos considerando un agente que está comprando vino. El consumidor puede preferir vino tinto o vino rosado pero no vino blanco. Para utilizar CEA con tales preferencias, es necesaria una modificación sólida. El segundo enfoque son los árboles de decisión. Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas. Un árbol de decisión incremental bien conocido es ID5R [18]. Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional. Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados. El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender. El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener. Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto. El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo. Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío. Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo. En cada interacción entre el productor y el consumidor, tanto G como S son modificados. Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas. Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento. Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa. Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí. Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 CEA Disyuntivo Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos. Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos. Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones. Algunos de estos enfoques utilizan múltiples espacios de versión. Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7]. Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir hipótesis disyuntivas además de las conjunciones y la negación. Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica. EJEMPLO 1. Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco). El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor. Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco. En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}. Solo cuando todos los valores existan en la lista, serán reemplazados por ?. En otras palabras, permitimos que el algoritmo generalice más lentamente que antes. Modificamos el algoritmo CEA para hacer frente a este cambio. El algoritmo modificado, DCEA, se presenta como Algoritmo 1. Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de múltiples espacios de versiones. La fase de inicialización es la misma que el algoritmo original (líneas 1, 2). Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4). Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí. Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran. Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7). Cuando llega una muestra negativa, no cambiamos S como antes. Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15). A diferencia del CEA original, intentamos especializar el G mínimamente. El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13). Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada. Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez. Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14). Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar. Por ejemplo, considera el atributo del color. Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2. La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA. El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra. Después de la primera muestra positiva, S se generaliza para cubrir también la instancia. La segunda muestra es negativa. Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada. En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general. La tercera muestra es positiva y generaliza S aún más. Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco). Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis. Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}. Por el Algoritmo 1, no perdemos ninguna información. 3.3 ID3 ID3 [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor. Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos. El algoritmo ID3 se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas. Hay dos clases: positiva y negativa. Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor. Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas. El árbol de decisión tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba. El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio. Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos. Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja. El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender. Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos. Después de cada solicitud entrante, el árbol de decisiones se reconstruye. Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso. Sin embargo, en la práctica hemos evaluado que el ID3 es rápido y el costo de reconstrucción es insignificante. 4. OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA. El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes. Cuando el productor recibe una solicitud del consumidor, el conjunto de aprendizaje del productor se entrena con esta solicitud como una muestra positiva. Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios. El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor. En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor. El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores. Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios. Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios. Esta lista constituye el inventario de servicios de los proveedores. Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace. Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor. Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación. Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S). Calculamos las similitudes de varias maneras, como se explicará en la Sección 5. Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud. El agente productor puede romper el empate de varias maneras. Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de ID3 Si el productor aprende las preferencias de los consumidores con ID3, se aplica un mecanismo similar con dos diferencias. Primero, dado que ID3 no mantiene G, se eliminan de la lista de servicios aquellos no aceptados que se clasifican como negativos. Segundo, las similitudes de los posibles servicios no se miden con respecto a S, sino en cambio a todas las solicitudes previamente realizadas. 4.3 Mecanismos Alternativos de Oferta de Servicios Además de estos tres mecanismos de oferta de servicios (Oferta de Servicio con CEA, Oferta de Servicio con DCEA y Oferta de Servicio con ID3), incluimos otros dos mecanismos. 1304 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) • Oferta de Servicio Aleatoria (RO): El productor genera una contraoferta aleatoriamente de la lista de servicios disponibles, sin considerar las preferencias de los consumidores. • Oferta de Servicio considerando solo la solicitud actual (SCR): El productor selecciona una contraoferta de acuerdo con la similitud de la solicitud actual del consumidor pero no considera solicitudes anteriores. 5. ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una métrica de similitud que toma dos entradas y devuelve qué tan similares son. Existen varios métricos de similitud utilizados en sistemas de razonamiento basado en casos, como la suma ponderada de la distancia euclidiana, la distancia de Hamming, entre otros [12]. La métrica de similitud afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor. Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de similitud semántica llamada Similitud RP. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente. En la Ecuación (1), común representa la cantidad de atributos coincidentes, mientras que diferente representa la cantidad de atributos diferentes. Nuestra suposición actual es que α y β son iguales entre sí. SMpq = α(común) α(común) + β(diferente) (1) Aquí, al comparar dos características, asignamos cero para la disimilitud y uno para la similitud al omitir la cercanía semántica entre los valores de las características. La métrica de similitud de Tversky está diseñada para comparar dos vectores de características. En nuestro sistema, mientras que la lista de servicios que puede ofrecer el productor son cada uno un vector de características, el conjunto más específico S no es un vector de características. S consiste en hipótesis de vectores de características. Por lo tanto, estimamos la similitud de cada hipótesis dentro del conjunto más específico S y luego calculamos el promedio de las similitudes. EJEMPLO 3. Suponga que S contiene las siguientes dos hipótesis: { {Luz, Moderado, (Rojo, Blanco)} , {Completo, Fuerte, Rosa}}. Toma el servicio s como (Ligero, Resistente, Rosa). Entonces, la similitud del primero es igual a 1/3 y la del segundo es igual a 2/3 de acuerdo con la Ecuación (1). Normalmente, tomamos el promedio de ello y obtenemos (1/3 + 2/3)/2, que es igual a 1/2. Sin embargo, la primera hipótesis implica el efecto de dos solicitudes y la segunda hipótesis implica solo una solicitud. Por lo tanto, esperamos que el efecto de la primera hipótesis sea mayor que el de la segunda. Por lo tanto, calculamos la similitud promedio teniendo en cuenta la cantidad de muestras que las hipótesis cubren. Que ch denote el número de muestras que cubre la hipótesis h y (SM(h,servicio)) denote la similitud de la hipótesis h con el servicio dado. Calculamos la similitud de cada hipótesis con el servicio dado y las ponderamos con el número de muestras que cubren. Encontramos la similitud dividiendo la suma ponderada de las similitudes de todas las hipótesis en S con el servicio por el número de todas las muestras que están cubiertas en S. AV G−SM(servicio, S) = |S| |h| (ch ∗ SM(h, servicio)) |S| |h| ch (2) Figura 2: Taxonomía de muestra para estimación de similitud EJEMPLO 4. Para el ejemplo anterior, la similitud de (Luz, Fuerte, Rosa) con el conjunto específico es (2 ∗ 1/3 + 2/3)/3, igual a 4/9. El número posible de muestras que abarca una hipótesis se puede estimar multiplicando las cardinalidades de cada atributo. Por ejemplo, la cardinalidad del primer atributo es dos y la de los demás es igual a uno para la hipótesis dada, como {Luz, Moderado, (Rojo, Blanco)}. Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La métrica de similitud de Lins Un taxonomía puede ser utilizada al estimar la similitud semántica entre dos conceptos. Estimar la similitud semántica en una taxonomía de tipo Es-Un se puede hacer calculando la distancia entre los nodos relacionados con los conceptos comparados. Los enlaces entre los nodos pueden considerarse como distancias. Entonces, la longitud del camino entre los nodos indica qué tan similares son los conceptos. Una estimación alternativa para utilizar el contenido de información en la estimación de la similitud semántica en lugar del método de conteo de aristas, fue propuesta por Lin [8]. La ecuación (3) [8] muestra la similitud de Lin donde c1 y c2 son los conceptos comparados y c0 es el concepto más específico que subsume a ambos. Además, P(C) representa la probabilidad de que un objeto seleccionado arbitrariamente pertenezca al concepto C. La similitud(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Métrica de similitud de Wu y Palmers Diferente de Lin, Wu y Palmer utilizan la distancia entre los nodos en la taxonomía ES-UN [20]. La similitud semántica se representa con la Ecuación (4) [20]. Aquí, se estima la similitud entre c1 y c2 y c0 es el concepto más específico que subsume estas clases. N1 es el número de aristas entre c1 y c0. N2 es el número de aristas entre c2 y c0. N0 es el número de enlaces IS-A de c0 desde la raíz de la taxonomía. Proponemos estimar la distancia relativa en una taxonomía entre dos conceptos utilizando las siguientes intuiciones. Utilizamos la Figura 2 para ilustrar estas intuiciones. • Padre versus abuelo: El padre de un nodo es más similar al nodo que los abuelos de ese. Generalización del Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1305 es un concepto que razonablemente resulta en alejarse más de ese concepto. Cuanto más generales son los conceptos, menos similares son. Por ejemplo, AnyWineColor es el padre de ReddishColor y ReddishColor es el padre de Red. Entonces, esperamos que la similitud entre ReddishColor y Red sea mayor que la similitud entre AnyWineColor y Red. • Padre versus hermano: Un nodo tendría una similitud mayor con su padre que con su hermano. Por ejemplo, Rojo y Rosa son hijos de ColorRojo. En este caso, esperamos que la similitud entre Rojo y ColorRojizo sea mayor que la de Rojo y Rosa. • Hermano versus abuelo: Un nodo es más similar a su hermano que a su abuelo. Para ilustrar, AnyWineColor es el abuelo de Red, y Red y Rose son hermanos. Por lo tanto, posiblemente anticipamos que Rojo y Rosa son más similares que CualquierColorDeVino y Rojo. Como una taxonomía está representada en un árbol, ese árbol puede ser recorrido desde el primer concepto que se está comparando hasta el segundo concepto. En el nodo inicial relacionado con el primer concepto, el valor de similitud es constante y igual a uno. Este valor se reduce por una constante en cada nodo visitado a lo largo del camino que llegará al nodo que incluye el segundo concepto. Cuanto más corto sea el camino entre los conceptos, mayor será la similitud entre los nodos. Algoritmo 2 Estimar-Similitud-RP(c1,c2) Requerido: Las constantes deben ser m > n > m2 donde m, n ∈ R[0, 1] 1: Similitud ← 1 2: si c1 es igual a c2 entonces 3: Devolver Similitud 4: fin si 5: padreComun ← encontrarPadreComun(c1, c2) {padreComun es el concepto más específico que cubre tanto c1 como c2} 6: N1 ← encontrarDistancia(padreComun, c1) 7: N2 ← encontrarDistancia(padreComun, c2) {N1 y N2 son el número de enlaces entre el concepto y el concepto padre} 8: si (padreComun == c1) o (padreComun == c2) entonces 9: Similitud ← Similitud ∗ m(N1+N2) 10: sino 11: Similitud ← Similitud ∗ n ∗ m(N1+N2−2) 12: fin si 13: Devolver Similitud La distancia relativa entre los nodos c1 y c2 se estima de la siguiente manera. Comenzando desde c1, se recorre el árbol para llegar a c2. En cada salto, la similitud disminuye ya que los conceptos se están alejando cada vez más entre sí. Sin embargo, según nuestras intuiciones, no todos los saltos disminuyen la similitud de igual manera. Que m represente el factor para saltar de un hijo a un padre y que n represente el factor para saltar de un hermano a otro hermano. Dado que saltar de un nodo a su abuelo cuenta como dos saltos de padre, el factor de descuento al moverse de un nodo a su abuelo es m2. De acuerdo con las intuiciones anteriores, nuestras constantes deben estar en la forma m > n > m2 donde el valor de m y n debe estar entre cero y uno. El algoritmo 2 muestra el cálculo de la distancia. Según el algoritmo, en primer lugar la similitud se inicializa con el valor de uno (línea 1). Si los conceptos son iguales entre sí, entonces la similitud será uno (líneas 2-4). De lo contrario, calculamos el ancestro común de los dos nodos y la distancia de cada concepto al ancestro común sin considerar al hermano (líneas 5-7). Si uno de los conceptos es igual al padre común, entonces no hay relación de hermanos entre los conceptos. Para cada nivel, multiplicamos la similitud por m y no consideramos el factor de hermanos en la estimación de la similitud. Como resultado, disminuimos la similitud en cada nivel con la tasa de m (línea 9). De lo contrario, tiene que existir una relación de hermanos. Esto significa que debemos considerar el efecto de n al medir la similitud. Recuerde que hemos contado N1+N2 aristas entre los conceptos. Dado que existe una relación de hermanos, dos de estos bordes constituyen la relación de hermanos. Por lo tanto, al calcular el efecto de la relación parental, utilizamos N1+N2 −2 aristas (línea 11). Algunas estimaciones de similitud relacionadas con la taxonomía en la Figura 2 se presentan en la Tabla 2. En este ejemplo, se toma m como 2/3 y n como 4/7. Tabla 2: Estimación de similitud de muestra sobre la taxonomía de muestra. Similitud(ColorRojo, Rosa) = 1 ∗ (2/3) = 0.6666667 Similitud(Rojo, Rosa) = 1 ∗ (4/7) = 0.5714286 Similitud(CualquierColorVino, Rosa) = 1 ∗ (2/3)2 = 0.44444445 Similitud(Blanco, Rosa) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 Para todas las métricas de similitud semántica en nuestra arquitectura, la taxonomía de características se mantiene en la ontología compartida. Para evaluar la similitud del vector de características, primero estimamos la similitud para cada característica individualmente y luego calculamos la suma promedio de estas similitudes. Entonces, el resultado es igual a la similitud semántica promedio de todo el vector de características. 6. SISTEMA DESARROLLADO Hemos implementado nuestra arquitectura en Java. Para facilitar las pruebas del sistema, el agente del consumidor tiene una interfaz de usuario que nos permite ingresar varias solicitudes. El agente productor está completamente automatizado y las operaciones de aprendizaje y oferta de servicios funcionan como se explicó anteriormente. En esta sección, explicamos los detalles de implementación del sistema desarrollado. Utilizamos OWL [11] como nuestro lenguaje de ontología y JENA como nuestro razonador de ontología. La ontología compartida es la versión modificada de la Ontología del Vino [19]. Incluye la descripción del vino como concepto y diferentes tipos de vino. Todos los participantes de la negociación utilizan esta ontología para entenderse mutuamente. Según la ontología, siete propiedades conforman el concepto de vino. El agente consumidor y el agente productor obtienen los valores posibles para estas propiedades consultando la ontología. Por lo tanto, todos los valores posibles para los componentes del concepto del vino, como el color, cuerpo, azúcar, etc., pueden ser alcanzados por ambos agentes. También se describen en esta ontología una variedad de tipos de vino como Borgoña, Chardonnay, Chenin Blanc, entre otros. Intuitivamente, cualquier tipo de vino descrito en la ontología también representa un concepto de vino. Esto nos permite considerar las instancias de vino Chardonnay como instancias de la clase Vino. Además de la descripción del vino, la información jerárquica de algunas características se puede inferir de la ontología. Por ejemplo, podemos representar la información de que el continente europeo abarca países occidentales. El país occidental abarca la región francesa, que incluye algunos territorios como el Loira, Burdeos, entre otros. Esta información jerárquica se utiliza en la estimación de similitud semántica. En esta parte, se pueden hacer algunos razonamientos como si un concepto X abarca Y y Y abarca Z, entonces el concepto X abarca Z. Por ejemplo, el Continente Europeo abarca Burdeos. 1306 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Para algunas características como cuerpo, sabor y azúcar, no hay información jerárquica, pero sus valores están nivelados semánticamente. Cuando eso sucede, proporcionamos los valores de similitud razonables para estas características. Por ejemplo, el cuerpo puede ser ligero, medio o fuerte. En este caso, asumimos que la luz es 0.66 similar a media pero solo 0.33 a fuerte. La ontología de WineStock es el inventario de los productores y describe una clase de producto como WineProduct. Esta clase es necesaria para que el productor registre los vinos que vende. La ontología implica a los individuos de esta clase. Los individuos representan los servicios disponibles que posee el productor. Hemos preparado dos ontologías de WineStock separadas para realizar pruebas. En la primera ontología, hay 19 productos de vino disponibles y en la segunda ontología, hay 50 productos. EVALUACIÓN DEL RENDIMIENTO Evaluamos el rendimiento de los sistemas propuestos en relación con la técnica de aprendizaje que utilizaron, DCEA e ID3, comparándolos con CEA, RO (para oferta aleatoria) y SCR (oferta basada solo en la solicitud actual). Aplicamos una variedad de escenarios en este conjunto de datos para ver las diferencias de rendimiento. Cada escenario de prueba contiene una lista de preferencias para el usuario y el número de coincidencias de la lista de productos. La Tabla 3 muestra estas preferencias y la disponibilidad de esos productos en el inventario para los primeros cinco escenarios. Ten en cuenta que estas preferencias son internas al consumidor y el productor intenta aprenderlas durante la negociación. Tabla 3: Disponibilidad de vinos en diferentes escenarios de prueba ID Preferencia del consumidor Disponibilidad (de 19) 1 Vino seco 15 2 Vino tinto y seco 8 3 Vino tinto, seco y moderado 4 4 Vino tinto y fuerte 2 5 Vino tinto o rosado, y fuerte 3 7.1 Comparación de Algoritmos de Aprendizaje En la comparación de algoritmos de aprendizaje, utilizamos los cinco escenarios de la Tabla 3. Aquí, primero usamos la medida de similitud de Tversky. Con estos casos de prueba, estamos interesados en encontrar el número de iteraciones que se requieren para que el productor genere una oferta aceptable para el consumidor. Dado que el rendimiento también depende de la solicitud inicial, repetimos nuestros experimentos con diferentes solicitudes iniciales. Por consiguiente, para cada caso, ejecutamos los algoritmos cinco veces con varias variaciones de las solicitudes iniciales. En cada experimento, contamos el número de iteraciones necesarias para llegar a un acuerdo. Tomamos el promedio de estos números para evaluar estos sistemas de manera justa. Como es costumbre, probamos cada algoritmo con las mismas solicitudes iniciales. La Tabla 4 compara los enfoques utilizando diferentes algoritmos de aprendizaje. Cuando las partes grandes del inventario son compatibles con las preferencias de los clientes, como en el primer caso de prueba, el rendimiento de todas las técnicas es casi el mismo (por ejemplo, Escenario 1). A medida que el número de servicios compatibles disminuye, RO funciona mal como se esperaba. El segundo peor método es SCR ya que solo considera la solicitud más reciente de los clientes y no aprende de las solicitudes anteriores. CEA da los mejores resultados cuando puede generar una respuesta pero no puede manejar los casos que contienen preferencias disyuntivas, como el que se presenta en el Escenario 5. ID3 y DCEA logran los mejores resultados. Su rendimiento es comparable y pueden manejar todos los casos, incluido el Escenario 5. Tabla 4: Comparación de algoritmos de aprendizaje en términos del número promedio de interacciones. Ejecutar DCEA SCR RO CEA ID3 Escenario 1: 1.2 1.4 1.2 1.2 1.2 Escenario 2: 1.4 1.4 2.6 1.4 1.4 Escenario 3: 1.4 1.8 4.4 1.4 1.4 Escenario 4: 2.2 2.8 9.6 1.8 2 Escenario 5: 2 2.6 7.6 1.75+ Sin oferta 1.8 Promedio de todos los casos: 1.64 2 5.08 1.51+Sin oferta 1.56 7.2 Comparación de Métricas de Similitud Para comparar las métricas de similitud que se explicaron en la Sección 5, fijamos el algoritmo de aprendizaje en DCEA. Además de los escenarios mostrados en la Tabla 3, agregamos los siguientes cinco nuevos escenarios considerando la información jerárquica. • El cliente desea comprar vino cuya bodega esté ubicada en California y cuya uva sea de tipo blanco. Además, la bodega del vino no debería ser costosa. Solo hay cuatro productos que cumplen con estas condiciones. • El cliente quiere comprar vino de color rojo o rosado y de tipo de uva tinta. Además, la ubicación del vino debe ser en Europa. Se desea que el grado de dulzura sea seco o semiseco. El sabor debe ser delicado o moderado, mientras que el cuerpo debe ser medio o ligero. Además, la bodega del vino debería ser una bodega cara. Hay dos productos que cumplen con todos estos requisitos. El cliente quiere comprar vino rosado moderado, que se encuentra alrededor de la región francesa. La categoría de bodega debería ser Bodega Moderada. Solo hay un producto que cumple con estos requisitos. • El cliente quiere comprar vino tinto caro, que se encuentra alrededor de la Región de California o vino blanco barato, que se encuentra alrededor de la Región de Texas. Hay cinco productos disponibles. • El cliente quiere comprar un vino blanco delicado cuyo productor esté en la categoría de Bodega Costosa. Hay dos productos disponibles. Los primeros siete escenarios se prueban con el primer conjunto de datos que contiene un total de 19 servicios y los últimos tres escenarios se prueban con el segundo conjunto de datos que contiene 50 servicios. La Tabla 5 muestra la evaluación del rendimiento en términos del número de interacciones necesarias para llegar a un consenso. La métrica de Tversky da los peores resultados ya que no considera la similitud semántica. El rendimiento de Lins es mejor que el de Tversky pero peor que el de otros. La métrica de Wu-Palmer y la medida de similitud de RP casi ofrecen el mismo rendimiento y son mejores que otras. Cuando se examinan los resultados, considerar la cercanía semántica aumenta el rendimiento. 8. DISCUSIÓN Revisamos la literatura reciente en comparación con nuestro trabajo. Tama et al. [16] proponen un nuevo enfoque basado en ontología para la negociación. Según su enfoque, los protocolos de negociación utilizados en el comercio electrónico pueden ser modelados como ontologías. Por lo tanto, los agentes pueden llevar a cabo un protocolo de negociación utilizando esta ontología compartida sin necesidad de estar codificados con los detalles del protocolo de negociación. Mientras tanto, la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1307 Tabla 5: Comparación de métricas de similitud en términos de número de interacciones. Ejecutar Tversky Lin Wu Palmer RP Escenario 1: 1.2 1.2 1 1 Escenario 2: 1.4 1.4 1.6 1.6 Escenario 3: 1.4 1.8 2 2 Escenario 4: 2.2 1 1.2 1.2 Escenario 5: 2 1.6 1.6 1.6 Escenario 6: 5 3.8 2.4 2.6 Escenario 7: 3.2 1.2 1 1 Escenario 8: 5.6 2 2 2.2 Escenario 9: 2.6 2.2 2.2 2.6 Escenario 10: 4.4 2 2 1.8 Promedio de todos los casos: 2.9 1.82 1.7 1.76 Tama et al. modelan el protocolo de negociación utilizando ontologías, en cambio, nosotros hemos modelado el servicio a ser negociado. Además, hemos construido un sistema con el cual se pueden aprender las preferencias de negociación. El estudio de Sadri et al. analiza la negociación en el contexto de la asignación de recursos [14]. Los agentes tienen recursos limitados y necesitan solicitar recursos faltantes a otros agentes. Se propone un mecanismo basado en secuencias de diálogo entre agentes como solución. El mecanismo se basa en el ciclo de agente de observar-pensar-actuar. Estos diálogos incluyen ofrecer recursos, intercambios de recursos y ofrecer recursos alternativos. Cada agente en el sistema planea sus acciones para alcanzar un estado objetivo. A diferencia de nuestro enfoque, el estudio de Sadri et al. no se preocupa por las preferencias de aprendizaje mutuas. Brzostowski y Kowalczyk proponen un enfoque para seleccionar un socio de negociación adecuado investigando negociaciones previas de múltiples atributos [1]. Para lograr esto, utilizan el razonamiento basado en casos. Su enfoque es probabilístico ya que el comportamiento de los socios puede cambiar en cada iteración. En nuestro enfoque, estamos interesados en negociar el contenido del servicio. Después de que el consumidor y el productor acuerden el servicio, se pueden utilizar mecanismos de negociación orientados al precio para acordar el precio. Fatima et al. estudian los factores que afectan la negociación, como las preferencias, el plazo, el precio, entre otros, ya que el agente que desarrolla una estrategia contra su oponente debe considerar todos ellos [5]. En su enfoque, el objetivo del agente vendedor es vender el servicio al precio más alto posible, mientras que el objetivo del agente comprador es comprar el bien al precio más bajo posible. El intervalo de tiempo afecta a estos agentes de manera diferente. En comparación con Fatima et al., nuestro enfoque es diferente. Mientras ellos estudian el efecto del tiempo en la negociación, nuestro enfoque está en aprender las preferencias para una negociación exitosa. Faratin et al. proponen un mecanismo de negociación multi-tema, donde las variables de servicio para la negociación, como el precio, la calidad del servicio, entre otros, se consideran intercambios entre sí (es decir, un precio más alto por una entrega más temprana) [4]. Generan un modelo heurístico para compensaciones que incluye la estimación de similitud difusa y una exploración de escalada de colina para ofertas posiblemente aceptables. Aunque abordamos un problema similar, aprendemos las preferencias del cliente con la ayuda del aprendizaje inductivo y generamos contraofertas de acuerdo con estas preferencias aprendidas. Faratin et al. solo utilizan la última oferta realizada por el consumidor al calcular la similitud para elegir la contraoferta. A diferencia de ellos, también tenemos en cuenta las solicitudes previas del consumidor. En sus experimentos, Faratin et al. asumen que los pesos de las variables de servicio están fijos a priori. Por el contrario, aprendemos estas preferencias con el tiempo. En nuestro trabajo futuro, planeamos integrar el razonamiento ontológico en el algoritmo de aprendizaje para que la información jerárquica pueda ser aprendida a partir de la jerarquía de subsumpción de relaciones. Además, al utilizar las relaciones entre las características, el productor puede descubrir nuevos conocimientos a partir de los conocimientos existentes. Estas son direcciones interesantes que seguiremos en nuestro trabajo futuro. 9. REFERENCIAS [1] J. Brzostowski y R. Kowalczyk. En el razonamiento basado en casos posibilístico para la selección de socios para la negociación de agentes de múltiples atributos. En Actas del 4to Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 273-278, 2005. [2] L. Busch e I. Horstman. Un comentario sobre negociaciones tema por tema. Juegos y Comportamiento Económico, 19:144-148, 1997. [3] J. K. Debenham. Gestión de la negociación en el mercado electrónico en el contexto de un sistema multiagente. En Actas de la 21ª Conferencia Internacional sobre Sistemas Basados en el Conocimiento e Inteligencia Artificial Aplicada, ES2002:, 2002. [4] P. Faratin, C. Sierra y N. R. Jennings. Utilizando criterios de similitud para hacer compensaciones de problemas en negociaciones automatizadas. Inteligencia Artificial, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge y N. Jennings. Agentes óptimos para negociaciones de múltiples temas. En Actas del 2do Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 129-136, 2003. [6] C. Giraud-Carrier. Una nota sobre la utilidad del aprendizaje incremental. Comunicaciones de IA, 13(4):215-223, 2000. [7] T.-P. Hong y S.-S. Tseng. Dividiendo y fusionando espacios de versiones para aprender conceptos disyuntivos. IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.\n\nTraducción al español:\nIEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin. Una definición de similitud basada en teoría de la información. En Actas de la 15ª Conferencia Internacional sobre Aprendizaje Automático, páginas 296-304. Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, y A. G. Moukas. Agentes que compran y venden. Comunicaciones de la ACM, 42(3):81-91, 1999. [10] T. M. Mitchell. Aprendizaje automático. McGraw Hill, NY, 1997. [11] Búho. OWL: Guía del lenguaje de ontologías web, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal y S. C. K. Shiu. Fundamentos del Razonamiento Basado en Casos Blandos. John Wiley & Sons, Nueva Jersey, 2004. [13] J. R. Quinlan. Inducción de árboles de decisión. Aprendizaje automático, 1(1):81-106, 1986. [14] F. Sadri, F. Toni y P. Torroni. Diálogos para negociación: Variedades de agentes y secuencias de diálogo. En ATAL 2001, Artículos Revisados, volumen 2333 de LNAI, páginas 405-421. Springer-Verlag, 2002. [15] M. P. Singh. \n\nSpringer-Verlag, 2002. [15] M. P. Singh. Comercio electrónico orientado al valor. IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, y M. Wooldridge. Ontologías para apoyar la negociación en el comercio electrónico. Aplicaciones de la Inteligencia Artificial en Ingeniería, 18:223-236, 2005. [17] A. Tversky. Características de similitud. Revisión Psicológica, 84(4):327-352, 1977. [18] P. E. Utgoff. Inducción incremental de árboles de decisión. Aprendizaje automático, 4:161-186, 1989. [19] Vino, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu y M. Palmer. Semántica de verbos y selección léxica. En el 32. Reunión anual de la Asociación de Lingüística Computacional, páginas 133-138, 1994. 1308 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "ontology": {
            "translated_key": "ontología",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared <br>ontology</br> to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an <br>ontology</br> in order to capture the relations between services and to construct the features for a given service.",
                "By using a common <br>ontology</br>, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared <br>ontology</br> provides the necessary vocabulary and hence enables a common language for agents.",
                "This <br>ontology</br> describes the content of the service.",
                "Further, since an <br>ontology</br> can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the <br>ontology</br> used.",
                "However, to make our discussion concrete, we use the well-known Wine <br>ontology</br> [19] with some modification to illustrate our ideas and to test our system.",
                "The wine <br>ontology</br> describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this <br>ontology</br>, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service <br>ontology</br> and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared <br>ontology</br>.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our <br>ontology</br> language and JENA as our <br>ontology</br> reasoner.",
                "The shared <br>ontology</br> is the modified version of the Wine <br>ontology</br> [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this <br>ontology</br> for understanding each other.",
                "According to the <br>ontology</br>, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the <br>ontology</br>.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this <br>ontology</br> such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the <br>ontology</br> also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the <br>ontology</br>.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock <br>ontology</br> is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "<br>ontology</br> involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first <br>ontology</br>, there are 19 available wine products and in the second <br>ontology</br>, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on <br>ontology</br> for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared <br>ontology</br> without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate <br>ontology</br> reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web <br>ontology</br> language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Accordingly, we propose an architecture in which both consumers and producers use a shared <br>ontology</br> to negotiate a service.",
                "We use an <br>ontology</br> in order to capture the relations between services and to construct the features for a given service.",
                "By using a common <br>ontology</br>, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "A shared <br>ontology</br> provides the necessary vocabulary and hence enables a common language for agents.",
                "This <br>ontology</br> describes the content of the service."
            ],
            "translated_annotated_samples": [
                "En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una <br>ontología</br> compartida para negociar un servicio.",
                "Utilizamos una <br>ontología</br> para capturar las relaciones entre servicios y construir las características para un servicio dado.",
                "Al utilizar una <br>ontología</br> común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación.",
                "Una <br>ontología</br> compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes.",
                "Esta <br>ontología</br> describe el contenido del servicio."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una <br>ontología</br> compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una <br>ontología</br> para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una <br>ontología</br> común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una <br>ontología</br> compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta <br>ontología</br> describe el contenido del servicio. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "consumer agent": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The <br>consumer agent</br> represents the customer and hence has access to the preferences of the customer.",
                "The <br>consumer agent</br> generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the <br>consumer agent</br>.",
                "The negotiation takes place in a turn-taking fashion, where the <br>consumer agent</br> starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the <br>consumer agent</br> or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the <br>consumer agent</br> can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the <br>consumer agent</br> whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the <br>consumer agent</br>.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the <br>consumer agent</br> has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The <br>consumer agent</br> and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "The <br>consumer agent</br> represents the customer and hence has access to the preferences of the customer.",
                "The <br>consumer agent</br> generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the <br>consumer agent</br>.",
                "The negotiation takes place in a turn-taking fashion, where the <br>consumer agent</br> starts the negotiation with a particular service request.",
                "This process will continue until some service is accepted by the <br>consumer agent</br> or all possible offers are put forward to the consumer by the producer."
            ],
            "translated_annotated_samples": [
                "El <br>agente del consumidor</br> representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente.",
                "El <br>agente del consumidor</br> genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias.",
                "Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del <br>agente del consumidor</br>.",
                "La negociación se lleva a cabo de manera secuencial, donde el <br>agente consumidor</br> inicia la negociación con una solicitud de servicio particular.",
                "Este proceso continuará hasta que algún servicio sea aceptado por el <br>agente del consumidor</br> o todas las ofertas posibles sean presentadas al consumidor por el productor."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El <br>agente del consumidor</br> representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El <br>agente del consumidor</br> genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del <br>agente del consumidor</br>. La negociación se lleva a cabo de manera secuencial, donde el <br>agente consumidor</br> inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el <br>agente del consumidor</br> o todas las ofertas posibles sean presentadas al consumidor por el productor. ",
            "candidates": [],
            "error": [
                [
                    "agente del consumidor",
                    "agente del consumidor",
                    "agente del consumidor",
                    "agente consumidor",
                    "agente del consumidor"
                ]
            ]
        },
        "datum repository": {
            "translated_key": "repositorio de datos",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "preference learning": {
            "translated_key": "Aprendizaje de preferencias",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "<br>preference learning</br>: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • <br>preference learning</br>: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "<br>preference learning</br> The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "<br>preference learning</br>: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "This challenge can be studied in three stages: • <br>preference learning</br>: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "<br>preference learning</br> The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature."
            ],
            "translated_annotated_samples": [
                "Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo.",
                "Este desafío se puede estudiar en tres etapas: • <br>Aprendizaje de preferencias</br>: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5)",
                "APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor. La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor. Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios. Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las preferencias del consumidor y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta. Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor. Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles. Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor. Este desafío se puede estudiar en tres etapas: • <br>Aprendizaje de preferencias</br>: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5) APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica. Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio. Por ejemplo, un consumidor puede tener preferencia por el vino tinto. Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo. Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo). Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor. Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores. Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa. Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo. Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6]. Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones. Investigamos particularmente dos enfoques. El primero es el aprendizaje inductivo. Esta técnica se aplica para aprender las preferencias como conceptos. Desarrollamos el Algoritmo de Eliminación de Candidatos (CEA) para el Espacio de Versiones [10]. Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva. Curiosamente, la mayoría de las veces las preferencias del consumidor son disyuntivas. Estamos considerando un agente que está comprando vino. El consumidor puede preferir vino tinto o vino rosado pero no vino blanco. Para utilizar CEA con tales preferencias, es necesaria una modificación sólida. El segundo enfoque son los árboles de decisión. Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas. Un árbol de decisión incremental bien conocido es ID5R [18]. Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional. Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados. El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender. El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener. Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto. El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo. Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío. Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo. En cada interacción entre el productor y el consumidor, tanto G como S son modificados. Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas. Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento. Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa. Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí. Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 CEA Disyuntivo Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos. Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos. Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones. Algunos de estos enfoques utilizan múltiples espacios de versión. Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7]. Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir hipótesis disyuntivas además de las conjunciones y la negación. Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica. EJEMPLO 1. Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco). El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor. Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco. En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}. Solo cuando todos los valores existan en la lista, serán reemplazados por ?. En otras palabras, permitimos que el algoritmo generalice más lentamente que antes. Modificamos el algoritmo CEA para hacer frente a este cambio. El algoritmo modificado, DCEA, se presenta como Algoritmo 1. Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de múltiples espacios de versiones. La fase de inicialización es la misma que el algoritmo original (líneas 1, 2). Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4). Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí. Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran. Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7). Cuando llega una muestra negativa, no cambiamos S como antes. Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15). A diferencia del CEA original, intentamos especializar el G mínimamente. El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13). Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada. Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez. Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14). Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar. Por ejemplo, considera el atributo del color. Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2. La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA. El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra. Después de la primera muestra positiva, S se generaliza para cubrir también la instancia. La segunda muestra es negativa. Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada. En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general. La tercera muestra es positiva y generaliza S aún más. Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco). Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis. Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}. Por el Algoritmo 1, no perdemos ninguna información. 3.3 ID3 ID3 [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor. Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos. El algoritmo ID3 se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas. Hay dos clases: positiva y negativa. Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor. Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas. El árbol de decisión tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba. El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio. Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos. Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja. El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender. Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos. Después de cada solicitud entrante, el árbol de decisiones se reconstruye. Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso. Sin embargo, en la práctica hemos evaluado que el ID3 es rápido y el costo de reconstrucción es insignificante. 4. OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA. El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes. Cuando el productor recibe una solicitud del consumidor, el conjunto de aprendizaje del productor se entrena con esta solicitud como una muestra positiva. Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios. El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor. En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor. El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores. Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios. Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios. Esta lista constituye el inventario de servicios de los proveedores. Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace. Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor. Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación. Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S). Calculamos las similitudes de varias maneras, como se explicará en la Sección 5. Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud. El agente productor puede romper el empate de varias maneras. Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de ID3 Si el productor aprende las preferencias de los consumidores con ID3, se aplica un mecanismo similar con dos diferencias. Primero, dado que ID3 no mantiene G, se eliminan de la lista de servicios aquellos no aceptados que se clasifican como negativos. Segundo, las similitudes de los posibles servicios no se miden con respecto a S, sino en cambio a todas las solicitudes previamente realizadas. 4.3 Mecanismos Alternativos de Oferta de Servicios Además de estos tres mecanismos de oferta de servicios (Oferta de Servicio con CEA, Oferta de Servicio con DCEA y Oferta de Servicio con ID3), incluimos otros dos mecanismos. 1304 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) • Oferta de Servicio Aleatoria (RO): El productor genera una contraoferta aleatoriamente de la lista de servicios disponibles, sin considerar las preferencias de los consumidores. • Oferta de Servicio considerando solo la solicitud actual (SCR): El productor selecciona una contraoferta de acuerdo con la similitud de la solicitud actual del consumidor pero no considera solicitudes anteriores. 5. ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una métrica de similitud que toma dos entradas y devuelve qué tan similares son. Existen varios métricos de similitud utilizados en sistemas de razonamiento basado en casos, como la suma ponderada de la distancia euclidiana, la distancia de Hamming, entre otros [12]. La métrica de similitud afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor. Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de similitud semántica llamada Similitud RP. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente. En la Ecuación (1), común representa la cantidad de atributos coincidentes, mientras que diferente representa la cantidad de atributos diferentes. Nuestra suposición actual es que α y β son iguales entre sí. SMpq = α(común) α(común) + β(diferente) (1) Aquí, al comparar dos características, asignamos cero para la disimilitud y uno para la similitud al omitir la cercanía semántica entre los valores de las características. La métrica de similitud de Tversky está diseñada para comparar dos vectores de características. En nuestro sistema, mientras que la lista de servicios que puede ofrecer el productor son cada uno un vector de características, el conjunto más específico S no es un vector de características. S consiste en hipótesis de vectores de características. Por lo tanto, estimamos la similitud de cada hipótesis dentro del conjunto más específico S y luego calculamos el promedio de las similitudes. EJEMPLO 3. Suponga que S contiene las siguientes dos hipótesis: { {Luz, Moderado, (Rojo, Blanco)} , {Completo, Fuerte, Rosa}}. Toma el servicio s como (Ligero, Resistente, Rosa). Entonces, la similitud del primero es igual a 1/3 y la del segundo es igual a 2/3 de acuerdo con la Ecuación (1). Normalmente, tomamos el promedio de ello y obtenemos (1/3 + 2/3)/2, que es igual a 1/2. Sin embargo, la primera hipótesis implica el efecto de dos solicitudes y la segunda hipótesis implica solo una solicitud. Por lo tanto, esperamos que el efecto de la primera hipótesis sea mayor que el de la segunda. Por lo tanto, calculamos la similitud promedio teniendo en cuenta la cantidad de muestras que las hipótesis cubren. Que ch denote el número de muestras que cubre la hipótesis h y (SM(h,servicio)) denote la similitud de la hipótesis h con el servicio dado. Calculamos la similitud de cada hipótesis con el servicio dado y las ponderamos con el número de muestras que cubren. Encontramos la similitud dividiendo la suma ponderada de las similitudes de todas las hipótesis en S con el servicio por el número de todas las muestras que están cubiertas en S. AV G−SM(servicio, S) = |S| |h| (ch ∗ SM(h, servicio)) |S| |h| ch (2) Figura 2: Taxonomía de muestra para estimación de similitud EJEMPLO 4. Para el ejemplo anterior, la similitud de (Luz, Fuerte, Rosa) con el conjunto específico es (2 ∗ 1/3 + 2/3)/3, igual a 4/9. El número posible de muestras que abarca una hipótesis se puede estimar multiplicando las cardinalidades de cada atributo. Por ejemplo, la cardinalidad del primer atributo es dos y la de los demás es igual a uno para la hipótesis dada, como {Luz, Moderado, (Rojo, Blanco)}. Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La métrica de similitud de Lins Un taxonomía puede ser utilizada al estimar la similitud semántica entre dos conceptos. Estimar la similitud semántica en una taxonomía de tipo Es-Un se puede hacer calculando la distancia entre los nodos relacionados con los conceptos comparados. Los enlaces entre los nodos pueden considerarse como distancias. Entonces, la longitud del camino entre los nodos indica qué tan similares son los conceptos. Una estimación alternativa para utilizar el contenido de información en la estimación de la similitud semántica en lugar del método de conteo de aristas, fue propuesta por Lin [8]. La ecuación (3) [8] muestra la similitud de Lin donde c1 y c2 son los conceptos comparados y c0 es el concepto más específico que subsume a ambos. Además, P(C) representa la probabilidad de que un objeto seleccionado arbitrariamente pertenezca al concepto C. La similitud(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Métrica de similitud de Wu y Palmers Diferente de Lin, Wu y Palmer utilizan la distancia entre los nodos en la taxonomía ES-UN [20]. La similitud semántica se representa con la Ecuación (4) [20]. Aquí, se estima la similitud entre c1 y c2 y c0 es el concepto más específico que subsume estas clases. N1 es el número de aristas entre c1 y c0. N2 es el número de aristas entre c2 y c0. N0 es el número de enlaces IS-A de c0 desde la raíz de la taxonomía. Proponemos estimar la distancia relativa en una taxonomía entre dos conceptos utilizando las siguientes intuiciones. Utilizamos la Figura 2 para ilustrar estas intuiciones. • Padre versus abuelo: El padre de un nodo es más similar al nodo que los abuelos de ese. Generalización del Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1305 es un concepto que razonablemente resulta en alejarse más de ese concepto. Cuanto más generales son los conceptos, menos similares son. Por ejemplo, AnyWineColor es el padre de ReddishColor y ReddishColor es el padre de Red. Entonces, esperamos que la similitud entre ReddishColor y Red sea mayor que la similitud entre AnyWineColor y Red. • Padre versus hermano: Un nodo tendría una similitud mayor con su padre que con su hermano. Por ejemplo, Rojo y Rosa son hijos de ColorRojo. En este caso, esperamos que la similitud entre Rojo y ColorRojizo sea mayor que la de Rojo y Rosa. • Hermano versus abuelo: Un nodo es más similar a su hermano que a su abuelo. Para ilustrar, AnyWineColor es el abuelo de Red, y Red y Rose son hermanos. Por lo tanto, posiblemente anticipamos que Rojo y Rosa son más similares que CualquierColorDeVino y Rojo. Como una taxonomía está representada en un árbol, ese árbol puede ser recorrido desde el primer concepto que se está comparando hasta el segundo concepto. En el nodo inicial relacionado con el primer concepto, el valor de similitud es constante y igual a uno. Este valor se reduce por una constante en cada nodo visitado a lo largo del camino que llegará al nodo que incluye el segundo concepto. Cuanto más corto sea el camino entre los conceptos, mayor será la similitud entre los nodos. Algoritmo 2 Estimar-Similitud-RP(c1,c2) Requerido: Las constantes deben ser m > n > m2 donde m, n ∈ R[0, 1] 1: Similitud ← 1 2: si c1 es igual a c2 entonces 3: Devolver Similitud 4: fin si 5: padreComun ← encontrarPadreComun(c1, c2) {padreComun es el concepto más específico que cubre tanto c1 como c2} 6: N1 ← encontrarDistancia(padreComun, c1) 7: N2 ← encontrarDistancia(padreComun, c2) {N1 y N2 son el número de enlaces entre el concepto y el concepto padre} 8: si (padreComun == c1) o (padreComun == c2) entonces 9: Similitud ← Similitud ∗ m(N1+N2) 10: sino 11: Similitud ← Similitud ∗ n ∗ m(N1+N2−2) 12: fin si 13: Devolver Similitud La distancia relativa entre los nodos c1 y c2 se estima de la siguiente manera. Comenzando desde c1, se recorre el árbol para llegar a c2. En cada salto, la similitud disminuye ya que los conceptos se están alejando cada vez más entre sí. Sin embargo, según nuestras intuiciones, no todos los saltos disminuyen la similitud de igual manera. Que m represente el factor para saltar de un hijo a un padre y que n represente el factor para saltar de un hermano a otro hermano. Dado que saltar de un nodo a su abuelo cuenta como dos saltos de padre, el factor de descuento al moverse de un nodo a su abuelo es m2. De acuerdo con las intuiciones anteriores, nuestras constantes deben estar en la forma m > n > m2 donde el valor de m y n debe estar entre cero y uno. El algoritmo 2 muestra el cálculo de la distancia. Según el algoritmo, en primer lugar la similitud se inicializa con el valor de uno (línea 1). Si los conceptos son iguales entre sí, entonces la similitud será uno (líneas 2-4). De lo contrario, calculamos el ancestro común de los dos nodos y la distancia de cada concepto al ancestro común sin considerar al hermano (líneas 5-7). Si uno de los conceptos es igual al padre común, entonces no hay relación de hermanos entre los conceptos. Para cada nivel, multiplicamos la similitud por m y no consideramos el factor de hermanos en la estimación de la similitud. Como resultado, disminuimos la similitud en cada nivel con la tasa de m (línea 9). De lo contrario, tiene que existir una relación de hermanos. Esto significa que debemos considerar el efecto de n al medir la similitud. Recuerde que hemos contado N1+N2 aristas entre los conceptos. Dado que existe una relación de hermanos, dos de estos bordes constituyen la relación de hermanos. Por lo tanto, al calcular el efecto de la relación parental, utilizamos N1+N2 −2 aristas (línea 11). Algunas estimaciones de similitud relacionadas con la taxonomía en la Figura 2 se presentan en la Tabla 2. En este ejemplo, se toma m como 2/3 y n como 4/7. Tabla 2: Estimación de similitud de muestra sobre la taxonomía de muestra. Similitud(ColorRojo, Rosa) = 1 ∗ (2/3) = 0.6666667 Similitud(Rojo, Rosa) = 1 ∗ (4/7) = 0.5714286 Similitud(CualquierColorVino, Rosa) = 1 ∗ (2/3)2 = 0.44444445 Similitud(Blanco, Rosa) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 Para todas las métricas de similitud semántica en nuestra arquitectura, la taxonomía de características se mantiene en la ontología compartida. Para evaluar la similitud del vector de características, primero estimamos la similitud para cada característica individualmente y luego calculamos la suma promedio de estas similitudes. Entonces, el resultado es igual a la similitud semántica promedio de todo el vector de características. 6. SISTEMA DESARROLLADO Hemos implementado nuestra arquitectura en Java. Para facilitar las pruebas del sistema, el agente del consumidor tiene una interfaz de usuario que nos permite ingresar varias solicitudes. El agente productor está completamente automatizado y las operaciones de aprendizaje y oferta de servicios funcionan como se explicó anteriormente. En esta sección, explicamos los detalles de implementación del sistema desarrollado. Utilizamos OWL [11] como nuestro lenguaje de ontología y JENA como nuestro razonador de ontología. La ontología compartida es la versión modificada de la Ontología del Vino [19]. Incluye la descripción del vino como concepto y diferentes tipos de vino. Todos los participantes de la negociación utilizan esta ontología para entenderse mutuamente. Según la ontología, siete propiedades conforman el concepto de vino. El agente consumidor y el agente productor obtienen los valores posibles para estas propiedades consultando la ontología. Por lo tanto, todos los valores posibles para los componentes del concepto del vino, como el color, cuerpo, azúcar, etc., pueden ser alcanzados por ambos agentes. También se describen en esta ontología una variedad de tipos de vino como Borgoña, Chardonnay, Chenin Blanc, entre otros. Intuitivamente, cualquier tipo de vino descrito en la ontología también representa un concepto de vino. Esto nos permite considerar las instancias de vino Chardonnay como instancias de la clase Vino. Además de la descripción del vino, la información jerárquica de algunas características se puede inferir de la ontología. Por ejemplo, podemos representar la información de que el continente europeo abarca países occidentales. El país occidental abarca la región francesa, que incluye algunos territorios como el Loira, Burdeos, entre otros. Esta información jerárquica se utiliza en la estimación de similitud semántica. En esta parte, se pueden hacer algunos razonamientos como si un concepto X abarca Y y Y abarca Z, entonces el concepto X abarca Z. Por ejemplo, el Continente Europeo abarca Burdeos. 1306 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Para algunas características como cuerpo, sabor y azúcar, no hay información jerárquica, pero sus valores están nivelados semánticamente. Cuando eso sucede, proporcionamos los valores de similitud razonables para estas características. Por ejemplo, el cuerpo puede ser ligero, medio o fuerte. En este caso, asumimos que la luz es 0.66 similar a media pero solo 0.33 a fuerte. La ontología de WineStock es el inventario de los productores y describe una clase de producto como WineProduct. Esta clase es necesaria para que el productor registre los vinos que vende. La ontología implica a los individuos de esta clase. Los individuos representan los servicios disponibles que posee el productor. Hemos preparado dos ontologías de WineStock separadas para realizar pruebas. En la primera ontología, hay 19 productos de vino disponibles y en la segunda ontología, hay 50 productos. EVALUACIÓN DEL RENDIMIENTO Evaluamos el rendimiento de los sistemas propuestos en relación con la técnica de aprendizaje que utilizaron, DCEA e ID3, comparándolos con CEA, RO (para oferta aleatoria) y SCR (oferta basada solo en la solicitud actual). Aplicamos una variedad de escenarios en este conjunto de datos para ver las diferencias de rendimiento. Cada escenario de prueba contiene una lista de preferencias para el usuario y el número de coincidencias de la lista de productos. La Tabla 3 muestra estas preferencias y la disponibilidad de esos productos en el inventario para los primeros cinco escenarios. Ten en cuenta que estas preferencias son internas al consumidor y el productor intenta aprenderlas durante la negociación. Tabla 3: Disponibilidad de vinos en diferentes escenarios de prueba ID Preferencia del consumidor Disponibilidad (de 19) 1 Vino seco 15 2 Vino tinto y seco 8 3 Vino tinto, seco y moderado 4 4 Vino tinto y fuerte 2 5 Vino tinto o rosado, y fuerte 3 7.1 Comparación de Algoritmos de Aprendizaje En la comparación de algoritmos de aprendizaje, utilizamos los cinco escenarios de la Tabla 3. Aquí, primero usamos la medida de similitud de Tversky. Con estos casos de prueba, estamos interesados en encontrar el número de iteraciones que se requieren para que el productor genere una oferta aceptable para el consumidor. Dado que el rendimiento también depende de la solicitud inicial, repetimos nuestros experimentos con diferentes solicitudes iniciales. Por consiguiente, para cada caso, ejecutamos los algoritmos cinco veces con varias variaciones de las solicitudes iniciales. En cada experimento, contamos el número de iteraciones necesarias para llegar a un acuerdo. Tomamos el promedio de estos números para evaluar estos sistemas de manera justa. Como es costumbre, probamos cada algoritmo con las mismas solicitudes iniciales. La Tabla 4 compara los enfoques utilizando diferentes algoritmos de aprendizaje. Cuando las partes grandes del inventario son compatibles con las preferencias de los clientes, como en el primer caso de prueba, el rendimiento de todas las técnicas es casi el mismo (por ejemplo, Escenario 1). A medida que el número de servicios compatibles disminuye, RO funciona mal como se esperaba. El segundo peor método es SCR ya que solo considera la solicitud más reciente de los clientes y no aprende de las solicitudes anteriores. CEA da los mejores resultados cuando puede generar una respuesta pero no puede manejar los casos que contienen preferencias disyuntivas, como el que se presenta en el Escenario 5. ID3 y DCEA logran los mejores resultados. Su rendimiento es comparable y pueden manejar todos los casos, incluido el Escenario 5. Tabla 4: Comparación de algoritmos de aprendizaje en términos del número promedio de interacciones. Ejecutar DCEA SCR RO CEA ID3 Escenario 1: 1.2 1.4 1.2 1.2 1.2 Escenario 2: 1.4 1.4 2.6 1.4 1.4 Escenario 3: 1.4 1.8 4.4 1.4 1.4 Escenario 4: 2.2 2.8 9.6 1.8 2 Escenario 5: 2 2.6 7.6 1.75+ Sin oferta 1.8 Promedio de todos los casos: 1.64 2 5.08 1.51+Sin oferta 1.56 7.2 Comparación de Métricas de Similitud Para comparar las métricas de similitud que se explicaron en la Sección 5, fijamos el algoritmo de aprendizaje en DCEA. Además de los escenarios mostrados en la Tabla 3, agregamos los siguientes cinco nuevos escenarios considerando la información jerárquica. • El cliente desea comprar vino cuya bodega esté ubicada en California y cuya uva sea de tipo blanco. Además, la bodega del vino no debería ser costosa. Solo hay cuatro productos que cumplen con estas condiciones. • El cliente quiere comprar vino de color rojo o rosado y de tipo de uva tinta. Además, la ubicación del vino debe ser en Europa. Se desea que el grado de dulzura sea seco o semiseco. El sabor debe ser delicado o moderado, mientras que el cuerpo debe ser medio o ligero. Además, la bodega del vino debería ser una bodega cara. Hay dos productos que cumplen con todos estos requisitos. El cliente quiere comprar vino rosado moderado, que se encuentra alrededor de la región francesa. La categoría de bodega debería ser Bodega Moderada. Solo hay un producto que cumple con estos requisitos. • El cliente quiere comprar vino tinto caro, que se encuentra alrededor de la Región de California o vino blanco barato, que se encuentra alrededor de la Región de Texas. Hay cinco productos disponibles. • El cliente quiere comprar un vino blanco delicado cuyo productor esté en la categoría de Bodega Costosa. Hay dos productos disponibles. Los primeros siete escenarios se prueban con el primer conjunto de datos que contiene un total de 19 servicios y los últimos tres escenarios se prueban con el segundo conjunto de datos que contiene 50 servicios. La Tabla 5 muestra la evaluación del rendimiento en términos del número de interacciones necesarias para llegar a un consenso. La métrica de Tversky da los peores resultados ya que no considera la similitud semántica. El rendimiento de Lins es mejor que el de Tversky pero peor que el de otros. La métrica de Wu-Palmer y la medida de similitud de RP casi ofrecen el mismo rendimiento y son mejores que otras. Cuando se examinan los resultados, considerar la cercanía semántica aumenta el rendimiento. 8. DISCUSIÓN Revisamos la literatura reciente en comparación con nuestro trabajo. Tama et al. [16] proponen un nuevo enfoque basado en ontología para la negociación. Según su enfoque, los protocolos de negociación utilizados en el comercio electrónico pueden ser modelados como ontologías. Por lo tanto, los agentes pueden llevar a cabo un protocolo de negociación utilizando esta ontología compartida sin necesidad de estar codificados con los detalles del protocolo de negociación. Mientras tanto, la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1307 Tabla 5: Comparación de métricas de similitud en términos de número de interacciones. Ejecutar Tversky Lin Wu Palmer RP Escenario 1: 1.2 1.2 1 1 Escenario 2: 1.4 1.4 1.6 1.6 Escenario 3: 1.4 1.8 2 2 Escenario 4: 2.2 1 1.2 1.2 Escenario 5: 2 1.6 1.6 1.6 Escenario 6: 5 3.8 2.4 2.6 Escenario 7: 3.2 1.2 1 1 Escenario 8: 5.6 2 2 2.2 Escenario 9: 2.6 2.2 2.2 2.6 Escenario 10: 4.4 2 2 1.8 Promedio de todos los casos: 2.9 1.82 1.7 1.76 Tama et al. modelan el protocolo de negociación utilizando ontologías, en cambio, nosotros hemos modelado el servicio a ser negociado. Además, hemos construido un sistema con el cual se pueden aprender las preferencias de negociación. El estudio de Sadri et al. analiza la negociación en el contexto de la asignación de recursos [14]. Los agentes tienen recursos limitados y necesitan solicitar recursos faltantes a otros agentes. Se propone un mecanismo basado en secuencias de diálogo entre agentes como solución. El mecanismo se basa en el ciclo de agente de observar-pensar-actuar. Estos diálogos incluyen ofrecer recursos, intercambios de recursos y ofrecer recursos alternativos. Cada agente en el sistema planea sus acciones para alcanzar un estado objetivo. A diferencia de nuestro enfoque, el estudio de Sadri et al. no se preocupa por las preferencias de aprendizaje mutuas. Brzostowski y Kowalczyk proponen un enfoque para seleccionar un socio de negociación adecuado investigando negociaciones previas de múltiples atributos [1]. Para lograr esto, utilizan el razonamiento basado en casos. Su enfoque es probabilístico ya que el comportamiento de los socios puede cambiar en cada iteración. En nuestro enfoque, estamos interesados en negociar el contenido del servicio. Después de que el consumidor y el productor acuerden el servicio, se pueden utilizar mecanismos de negociación orientados al precio para acordar el precio. Fatima et al. estudian los factores que afectan la negociación, como las preferencias, el plazo, el precio, entre otros, ya que el agente que desarrolla una estrategia contra su oponente debe considerar todos ellos [5]. En su enfoque, el objetivo del agente vendedor es vender el servicio al precio más alto posible, mientras que el objetivo del agente comprador es comprar el bien al precio más bajo posible. El intervalo de tiempo afecta a estos agentes de manera diferente. En comparación con Fatima et al., nuestro enfoque es diferente. Mientras ellos estudian el efecto del tiempo en la negociación, nuestro enfoque está en aprender las preferencias para una negociación exitosa. Faratin et al. proponen un mecanismo de negociación multi-tema, donde las variables de servicio para la negociación, como el precio, la calidad del servicio, entre otros, se consideran intercambios entre sí (es decir, un precio más alto por una entrega más temprana) [4]. Generan un modelo heurístico para compensaciones que incluye la estimación de similitud difusa y una exploración de escalada de colina para ofertas posiblemente aceptables. Aunque abordamos un problema similar, aprendemos las preferencias del cliente con la ayuda del aprendizaje inductivo y generamos contraofertas de acuerdo con estas preferencias aprendidas. Faratin et al. solo utilizan la última oferta realizada por el consumidor al calcular la similitud para elegir la contraoferta. A diferencia de ellos, también tenemos en cuenta las solicitudes previas del consumidor. En sus experimentos, Faratin et al. asumen que los pesos de las variables de servicio están fijos a priori. Por el contrario, aprendemos estas preferencias con el tiempo. En nuestro trabajo futuro, planeamos integrar el razonamiento ontológico en el algoritmo de aprendizaje para que la información jerárquica pueda ser aprendida a partir de la jerarquía de subsumpción de relaciones. Además, al utilizar las relaciones entre las características, el productor puede descubrir nuevos conocimientos a partir de los conocimientos existentes. Estas son direcciones interesantes que seguiremos en nuestro trabajo futuro. 9. REFERENCIAS [1] J. Brzostowski y R. Kowalczyk. En el razonamiento basado en casos posibilístico para la selección de socios para la negociación de agentes de múltiples atributos. En Actas del 4to Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 273-278, 2005. [2] L. Busch e I. Horstman. Un comentario sobre negociaciones tema por tema. Juegos y Comportamiento Económico, 19:144-148, 1997. [3] J. K. Debenham. Gestión de la negociación en el mercado electrónico en el contexto de un sistema multiagente. En Actas de la 21ª Conferencia Internacional sobre Sistemas Basados en el Conocimiento e Inteligencia Artificial Aplicada, ES2002:, 2002. [4] P. Faratin, C. Sierra y N. R. Jennings. Utilizando criterios de similitud para hacer compensaciones de problemas en negociaciones automatizadas. Inteligencia Artificial, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge y N. Jennings. Agentes óptimos para negociaciones de múltiples temas. En Actas del 2do Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 129-136, 2003. [6] C. Giraud-Carrier. Una nota sobre la utilidad del aprendizaje incremental. Comunicaciones de IA, 13(4):215-223, 2000. [7] T.-P. Hong y S.-S. Tseng. Dividiendo y fusionando espacios de versiones para aprender conceptos disyuntivos. IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.\n\nTraducción al español:\nIEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin. Una definición de similitud basada en teoría de la información. En Actas de la 15ª Conferencia Internacional sobre Aprendizaje Automático, páginas 296-304. Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, y A. G. Moukas. Agentes que compran y venden. Comunicaciones de la ACM, 42(3):81-91, 1999. [10] T. M. Mitchell. Aprendizaje automático. McGraw Hill, NY, 1997. [11] Búho. OWL: Guía del lenguaje de ontologías web, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal y S. C. K. Shiu. Fundamentos del Razonamiento Basado en Casos Blandos. John Wiley & Sons, Nueva Jersey, 2004. [13] J. R. Quinlan. Inducción de árboles de decisión. Aprendizaje automático, 1(1):81-106, 1986. [14] F. Sadri, F. Toni y P. Torroni. Diálogos para negociación: Variedades de agentes y secuencias de diálogo. En ATAL 2001, Artículos Revisados, volumen 2333 de LNAI, páginas 405-421. Springer-Verlag, 2002. [15] M. P. Singh. \n\nSpringer-Verlag, 2002. [15] M. P. Singh. Comercio electrónico orientado al valor. IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, y M. Wooldridge. Ontologías para apoyar la negociación en el comercio electrónico. Aplicaciones de la Inteligencia Artificial en Ingeniería, 18:223-236, 2005. [17] A. Tversky. Características de similitud. Revisión Psicológica, 84(4):327-352, 1977. [18] P. E. Utgoff. Inducción incremental de árboles de decisión. Aprendizaje automático, 4:161-186, 1989. [19] Vino, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu y M. Palmer. Semántica de verbos y selección léxica. En el 32. Reunión anual de la Asociación de Lingüística Computacional, páginas 133-138, 1994. 1308 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "candidate elimination algorithm": {
            "translated_key": "Algoritmo de Eliminación de Candidatos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on <br>candidate elimination algorithm</br> (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive <br>candidate elimination algorithm</br> 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "We elaborate on <br>candidate elimination algorithm</br> (CEA) for Version Space [10].",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive <br>candidate elimination algorithm</br> 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2."
            ],
            "translated_annotated_samples": [
                "Desarrollamos el <br>Algoritmo de Eliminación de Candidatos</br> (CEA) para el Espacio de Versiones [10].",
                "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor. La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor. Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios. Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las preferencias del consumidor y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta. Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor. Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles. Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor. Este desafío se puede estudiar en tres etapas: • Aprendizaje de preferencias: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5) APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica. Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio. Por ejemplo, un consumidor puede tener preferencia por el vino tinto. Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo. Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo). Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor. Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores. Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa. Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo. Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6]. Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones. Investigamos particularmente dos enfoques. El primero es el aprendizaje inductivo. Esta técnica se aplica para aprender las preferencias como conceptos. Desarrollamos el <br>Algoritmo de Eliminación de Candidatos</br> (CEA) para el Espacio de Versiones [10]. Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva. Curiosamente, la mayoría de las veces las preferencias del consumidor son disyuntivas. Estamos considerando un agente que está comprando vino. El consumidor puede preferir vino tinto o vino rosado pero no vino blanco. Para utilizar CEA con tales preferencias, es necesaria una modificación sólida. El segundo enfoque son los árboles de decisión. Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas. Un árbol de decisión incremental bien conocido es ID5R [18]. Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional. Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados. El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender. El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener. Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto. El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo. Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío. Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo. En cada interacción entre el productor y el consumidor, tanto G como S son modificados. Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas. Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento. Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa. Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí. Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 CEA Disyuntivo Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos. Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos. Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones. Algunos de estos enfoques utilizan múltiples espacios de versión. Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7]. Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir hipótesis disyuntivas además de las conjunciones y la negación. Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica. EJEMPLO 1. Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco). El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor. Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco. En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}. Solo cuando todos los valores existan en la lista, serán reemplazados por ?. En otras palabras, permitimos que el algoritmo generalice más lentamente que antes. Modificamos el algoritmo CEA para hacer frente a este cambio. El algoritmo modificado, DCEA, se presenta como Algoritmo 1. Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de múltiples espacios de versiones. La fase de inicialización es la misma que el algoritmo original (líneas 1, 2). Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4). Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí. Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran. Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7). Cuando llega una muestra negativa, no cambiamos S como antes. Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15). A diferencia del CEA original, intentamos especializar el G mínimamente. El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13). Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada. Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez. Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14). Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar. Por ejemplo, considera el atributo del color. Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2. La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA. El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra. Después de la primera muestra positiva, S se generaliza para cubrir también la instancia. La segunda muestra es negativa. Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada. En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general. La tercera muestra es positiva y generaliza S aún más. Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco). Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis. Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}. Por el Algoritmo 1, no perdemos ninguna información. 3.3 ID3 ID3 [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor. Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos. El algoritmo ID3 se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas. Hay dos clases: positiva y negativa. Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor. Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas. El árbol de decisión tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba. El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio. Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos. Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja. El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender. Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos. Después de cada solicitud entrante, el árbol de decisiones se reconstruye. Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso. Sin embargo, en la práctica hemos evaluado que el ID3 es rápido y el costo de reconstrucción es insignificante. 4. OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA. El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes. Cuando el productor recibe una solicitud del consumidor, el conjunto de aprendizaje del productor se entrena con esta solicitud como una muestra positiva. Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios. El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor. En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor. El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores. Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios. Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios. Esta lista constituye el inventario de servicios de los proveedores. Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace. Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor. Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación. Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S). Calculamos las similitudes de varias maneras, como se explicará en la Sección 5. Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud. El agente productor puede romper el empate de varias maneras. Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de ID3 Si el productor aprende las preferencias de los consumidores con ID3, se aplica un mecanismo similar con dos diferencias. Primero, dado que ID3 no mantiene G, se eliminan de la lista de servicios aquellos no aceptados que se clasifican como negativos. Segundo, las similitudes de los posibles servicios no se miden con respecto a S, sino en cambio a todas las solicitudes previamente realizadas. 4.3 Mecanismos Alternativos de Oferta de Servicios Además de estos tres mecanismos de oferta de servicios (Oferta de Servicio con CEA, Oferta de Servicio con DCEA y Oferta de Servicio con ID3), incluimos otros dos mecanismos. 1304 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) • Oferta de Servicio Aleatoria (RO): El productor genera una contraoferta aleatoriamente de la lista de servicios disponibles, sin considerar las preferencias de los consumidores. • Oferta de Servicio considerando solo la solicitud actual (SCR): El productor selecciona una contraoferta de acuerdo con la similitud de la solicitud actual del consumidor pero no considera solicitudes anteriores. 5. ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una métrica de similitud que toma dos entradas y devuelve qué tan similares son. Existen varios métricos de similitud utilizados en sistemas de razonamiento basado en casos, como la suma ponderada de la distancia euclidiana, la distancia de Hamming, entre otros [12]. La métrica de similitud afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor. Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de similitud semántica llamada Similitud RP. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente. En la Ecuación (1), común representa la cantidad de atributos coincidentes, mientras que diferente representa la cantidad de atributos diferentes. Nuestra suposición actual es que α y β son iguales entre sí. SMpq = α(común) α(común) + β(diferente) (1) Aquí, al comparar dos características, asignamos cero para la disimilitud y uno para la similitud al omitir la cercanía semántica entre los valores de las características. La métrica de similitud de Tversky está diseñada para comparar dos vectores de características. En nuestro sistema, mientras que la lista de servicios que puede ofrecer el productor son cada uno un vector de características, el conjunto más específico S no es un vector de características. S consiste en hipótesis de vectores de características. Por lo tanto, estimamos la similitud de cada hipótesis dentro del conjunto más específico S y luego calculamos el promedio de las similitudes. EJEMPLO 3. Suponga que S contiene las siguientes dos hipótesis: { {Luz, Moderado, (Rojo, Blanco)} , {Completo, Fuerte, Rosa}}. Toma el servicio s como (Ligero, Resistente, Rosa). Entonces, la similitud del primero es igual a 1/3 y la del segundo es igual a 2/3 de acuerdo con la Ecuación (1). Normalmente, tomamos el promedio de ello y obtenemos (1/3 + 2/3)/2, que es igual a 1/2. Sin embargo, la primera hipótesis implica el efecto de dos solicitudes y la segunda hipótesis implica solo una solicitud. Por lo tanto, esperamos que el efecto de la primera hipótesis sea mayor que el de la segunda. Por lo tanto, calculamos la similitud promedio teniendo en cuenta la cantidad de muestras que las hipótesis cubren. Que ch denote el número de muestras que cubre la hipótesis h y (SM(h,servicio)) denote la similitud de la hipótesis h con el servicio dado. Calculamos la similitud de cada hipótesis con el servicio dado y las ponderamos con el número de muestras que cubren. Encontramos la similitud dividiendo la suma ponderada de las similitudes de todas las hipótesis en S con el servicio por el número de todas las muestras que están cubiertas en S. AV G−SM(servicio, S) = |S| |h| (ch ∗ SM(h, servicio)) |S| |h| ch (2) Figura 2: Taxonomía de muestra para estimación de similitud EJEMPLO 4. Para el ejemplo anterior, la similitud de (Luz, Fuerte, Rosa) con el conjunto específico es (2 ∗ 1/3 + 2/3)/3, igual a 4/9. El número posible de muestras que abarca una hipótesis se puede estimar multiplicando las cardinalidades de cada atributo. Por ejemplo, la cardinalidad del primer atributo es dos y la de los demás es igual a uno para la hipótesis dada, como {Luz, Moderado, (Rojo, Blanco)}. Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La métrica de similitud de Lins Un taxonomía puede ser utilizada al estimar la similitud semántica entre dos conceptos. Estimar la similitud semántica en una taxonomía de tipo Es-Un se puede hacer calculando la distancia entre los nodos relacionados con los conceptos comparados. Los enlaces entre los nodos pueden considerarse como distancias. Entonces, la longitud del camino entre los nodos indica qué tan similares son los conceptos. Una estimación alternativa para utilizar el contenido de información en la estimación de la similitud semántica en lugar del método de conteo de aristas, fue propuesta por Lin [8]. La ecuación (3) [8] muestra la similitud de Lin donde c1 y c2 son los conceptos comparados y c0 es el concepto más específico que subsume a ambos. Además, P(C) representa la probabilidad de que un objeto seleccionado arbitrariamente pertenezca al concepto C. La similitud(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Métrica de similitud de Wu y Palmers Diferente de Lin, Wu y Palmer utilizan la distancia entre los nodos en la taxonomía ES-UN [20]. La similitud semántica se representa con la Ecuación (4) [20]. Aquí, se estima la similitud entre c1 y c2 y c0 es el concepto más específico que subsume estas clases. N1 es el número de aristas entre c1 y c0. N2 es el número de aristas entre c2 y c0. N0 es el número de enlaces IS-A de c0 desde la raíz de la taxonomía. Proponemos estimar la distancia relativa en una taxonomía entre dos conceptos utilizando las siguientes intuiciones. Utilizamos la Figura 2 para ilustrar estas intuiciones. • Padre versus abuelo: El padre de un nodo es más similar al nodo que los abuelos de ese. Generalización del Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1305 es un concepto que razonablemente resulta en alejarse más de ese concepto. Cuanto más generales son los conceptos, menos similares son. Por ejemplo, AnyWineColor es el padre de ReddishColor y ReddishColor es el padre de Red. Entonces, esperamos que la similitud entre ReddishColor y Red sea mayor que la similitud entre AnyWineColor y Red. • Padre versus hermano: Un nodo tendría una similitud mayor con su padre que con su hermano. Por ejemplo, Rojo y Rosa son hijos de ColorRojo. En este caso, esperamos que la similitud entre Rojo y ColorRojizo sea mayor que la de Rojo y Rosa. • Hermano versus abuelo: Un nodo es más similar a su hermano que a su abuelo. Para ilustrar, AnyWineColor es el abuelo de Red, y Red y Rose son hermanos. Por lo tanto, posiblemente anticipamos que Rojo y Rosa son más similares que CualquierColorDeVino y Rojo. Como una taxonomía está representada en un árbol, ese árbol puede ser recorrido desde el primer concepto que se está comparando hasta el segundo concepto. En el nodo inicial relacionado con el primer concepto, el valor de similitud es constante y igual a uno. Este valor se reduce por una constante en cada nodo visitado a lo largo del camino que llegará al nodo que incluye el segundo concepto. Cuanto más corto sea el camino entre los conceptos, mayor será la similitud entre los nodos. Algoritmo 2 Estimar-Similitud-RP(c1,c2) Requerido: Las constantes deben ser m > n > m2 donde m, n ∈ R[0, 1] 1: Similitud ← 1 2: si c1 es igual a c2 entonces 3: Devolver Similitud 4: fin si 5: padreComun ← encontrarPadreComun(c1, c2) {padreComun es el concepto más específico que cubre tanto c1 como c2} 6: N1 ← encontrarDistancia(padreComun, c1) 7: N2 ← encontrarDistancia(padreComun, c2) {N1 y N2 son el número de enlaces entre el concepto y el concepto padre} 8: si (padreComun == c1) o (padreComun == c2) entonces 9: Similitud ← Similitud ∗ m(N1+N2) 10: sino 11: Similitud ← Similitud ∗ n ∗ m(N1+N2−2) 12: fin si 13: Devolver Similitud La distancia relativa entre los nodos c1 y c2 se estima de la siguiente manera. Comenzando desde c1, se recorre el árbol para llegar a c2. En cada salto, la similitud disminuye ya que los conceptos se están alejando cada vez más entre sí. Sin embargo, según nuestras intuiciones, no todos los saltos disminuyen la similitud de igual manera. Que m represente el factor para saltar de un hijo a un padre y que n represente el factor para saltar de un hermano a otro hermano. Dado que saltar de un nodo a su abuelo cuenta como dos saltos de padre, el factor de descuento al moverse de un nodo a su abuelo es m2. De acuerdo con las intuiciones anteriores, nuestras constantes deben estar en la forma m > n > m2 donde el valor de m y n debe estar entre cero y uno. El algoritmo 2 muestra el cálculo de la distancia. Según el algoritmo, en primer lugar la similitud se inicializa con el valor de uno (línea 1). Si los conceptos son iguales entre sí, entonces la similitud será uno (líneas 2-4). De lo contrario, calculamos el ancestro común de los dos nodos y la distancia de cada concepto al ancestro común sin considerar al hermano (líneas 5-7). Si uno de los conceptos es igual al padre común, entonces no hay relación de hermanos entre los conceptos. Para cada nivel, multiplicamos la similitud por m y no consideramos el factor de hermanos en la estimación de la similitud. Como resultado, disminuimos la similitud en cada nivel con la tasa de m (línea 9). De lo contrario, tiene que existir una relación de hermanos. Esto significa que debemos considerar el efecto de n al medir la similitud. Recuerde que hemos contado N1+N2 aristas entre los conceptos. Dado que existe una relación de hermanos, dos de estos bordes constituyen la relación de hermanos. Por lo tanto, al calcular el efecto de la relación parental, utilizamos N1+N2 −2 aristas (línea 11). Algunas estimaciones de similitud relacionadas con la taxonomía en la Figura 2 se presentan en la Tabla 2. En este ejemplo, se toma m como 2/3 y n como 4/7. Tabla 2: Estimación de similitud de muestra sobre la taxonomía de muestra. Similitud(ColorRojo, Rosa) = 1 ∗ (2/3) = 0.6666667 Similitud(Rojo, Rosa) = 1 ∗ (4/7) = 0.5714286 Similitud(CualquierColorVino, Rosa) = 1 ∗ (2/3)2 = 0.44444445 Similitud(Blanco, Rosa) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 Para todas las métricas de similitud semántica en nuestra arquitectura, la taxonomía de características se mantiene en la ontología compartida. Para evaluar la similitud del vector de características, primero estimamos la similitud para cada característica individualmente y luego calculamos la suma promedio de estas similitudes. Entonces, el resultado es igual a la similitud semántica promedio de todo el vector de características. 6. SISTEMA DESARROLLADO Hemos implementado nuestra arquitectura en Java. Para facilitar las pruebas del sistema, el agente del consumidor tiene una interfaz de usuario que nos permite ingresar varias solicitudes. El agente productor está completamente automatizado y las operaciones de aprendizaje y oferta de servicios funcionan como se explicó anteriormente. En esta sección, explicamos los detalles de implementación del sistema desarrollado. Utilizamos OWL [11] como nuestro lenguaje de ontología y JENA como nuestro razonador de ontología. La ontología compartida es la versión modificada de la Ontología del Vino [19]. Incluye la descripción del vino como concepto y diferentes tipos de vino. Todos los participantes de la negociación utilizan esta ontología para entenderse mutuamente. Según la ontología, siete propiedades conforman el concepto de vino. El agente consumidor y el agente productor obtienen los valores posibles para estas propiedades consultando la ontología. Por lo tanto, todos los valores posibles para los componentes del concepto del vino, como el color, cuerpo, azúcar, etc., pueden ser alcanzados por ambos agentes. También se describen en esta ontología una variedad de tipos de vino como Borgoña, Chardonnay, Chenin Blanc, entre otros. Intuitivamente, cualquier tipo de vino descrito en la ontología también representa un concepto de vino. Esto nos permite considerar las instancias de vino Chardonnay como instancias de la clase Vino. Además de la descripción del vino, la información jerárquica de algunas características se puede inferir de la ontología. Por ejemplo, podemos representar la información de que el continente europeo abarca países occidentales. El país occidental abarca la región francesa, que incluye algunos territorios como el Loira, Burdeos, entre otros. Esta información jerárquica se utiliza en la estimación de similitud semántica. En esta parte, se pueden hacer algunos razonamientos como si un concepto X abarca Y y Y abarca Z, entonces el concepto X abarca Z. Por ejemplo, el Continente Europeo abarca Burdeos. 1306 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Para algunas características como cuerpo, sabor y azúcar, no hay información jerárquica, pero sus valores están nivelados semánticamente. Cuando eso sucede, proporcionamos los valores de similitud razonables para estas características. Por ejemplo, el cuerpo puede ser ligero, medio o fuerte. En este caso, asumimos que la luz es 0.66 similar a media pero solo 0.33 a fuerte. La ontología de WineStock es el inventario de los productores y describe una clase de producto como WineProduct. Esta clase es necesaria para que el productor registre los vinos que vende. La ontología implica a los individuos de esta clase. Los individuos representan los servicios disponibles que posee el productor. Hemos preparado dos ontologías de WineStock separadas para realizar pruebas. En la primera ontología, hay 19 productos de vino disponibles y en la segunda ontología, hay 50 productos. EVALUACIÓN DEL RENDIMIENTO Evaluamos el rendimiento de los sistemas propuestos en relación con la técnica de aprendizaje que utilizaron, DCEA e ID3, comparándolos con CEA, RO (para oferta aleatoria) y SCR (oferta basada solo en la solicitud actual). Aplicamos una variedad de escenarios en este conjunto de datos para ver las diferencias de rendimiento. Cada escenario de prueba contiene una lista de preferencias para el usuario y el número de coincidencias de la lista de productos. La Tabla 3 muestra estas preferencias y la disponibilidad de esos productos en el inventario para los primeros cinco escenarios. Ten en cuenta que estas preferencias son internas al consumidor y el productor intenta aprenderlas durante la negociación. Tabla 3: Disponibilidad de vinos en diferentes escenarios de prueba ID Preferencia del consumidor Disponibilidad (de 19) 1 Vino seco 15 2 Vino tinto y seco 8 3 Vino tinto, seco y moderado 4 4 Vino tinto y fuerte 2 5 Vino tinto o rosado, y fuerte 3 7.1 Comparación de Algoritmos de Aprendizaje En la comparación de algoritmos de aprendizaje, utilizamos los cinco escenarios de la Tabla 3. Aquí, primero usamos la medida de similitud de Tversky. Con estos casos de prueba, estamos interesados en encontrar el número de iteraciones que se requieren para que el productor genere una oferta aceptable para el consumidor. Dado que el rendimiento también depende de la solicitud inicial, repetimos nuestros experimentos con diferentes solicitudes iniciales. Por consiguiente, para cada caso, ejecutamos los algoritmos cinco veces con varias variaciones de las solicitudes iniciales. En cada experimento, contamos el número de iteraciones necesarias para llegar a un acuerdo. Tomamos el promedio de estos números para evaluar estos sistemas de manera justa. Como es costumbre, probamos cada algoritmo con las mismas solicitudes iniciales. La Tabla 4 compara los enfoques utilizando diferentes algoritmos de aprendizaje. Cuando las partes grandes del inventario son compatibles con las preferencias de los clientes, como en el primer caso de prueba, el rendimiento de todas las técnicas es casi el mismo (por ejemplo, Escenario 1). A medida que el número de servicios compatibles disminuye, RO funciona mal como se esperaba. El segundo peor método es SCR ya que solo considera la solicitud más reciente de los clientes y no aprende de las solicitudes anteriores. CEA da los mejores resultados cuando puede generar una respuesta pero no puede manejar los casos que contienen preferencias disyuntivas, como el que se presenta en el Escenario 5. ID3 y DCEA logran los mejores resultados. Su rendimiento es comparable y pueden manejar todos los casos, incluido el Escenario 5. Tabla 4: Comparación de algoritmos de aprendizaje en términos del número promedio de interacciones. Ejecutar DCEA SCR RO CEA ID3 Escenario 1: 1.2 1.4 1.2 1.2 1.2 Escenario 2: 1.4 1.4 2.6 1.4 1.4 Escenario 3: 1.4 1.8 4.4 1.4 1.4 Escenario 4: 2.2 2.8 9.6 1.8 2 Escenario 5: 2 2.6 7.6 1.75+ Sin oferta 1.8 Promedio de todos los casos: 1.64 2 5.08 1.51+Sin oferta 1.56 7.2 Comparación de Métricas de Similitud Para comparar las métricas de similitud que se explicaron en la Sección 5, fijamos el algoritmo de aprendizaje en DCEA. Además de los escenarios mostrados en la Tabla 3, agregamos los siguientes cinco nuevos escenarios considerando la información jerárquica. • El cliente desea comprar vino cuya bodega esté ubicada en California y cuya uva sea de tipo blanco. Además, la bodega del vino no debería ser costosa. Solo hay cuatro productos que cumplen con estas condiciones. • El cliente quiere comprar vino de color rojo o rosado y de tipo de uva tinta. Además, la ubicación del vino debe ser en Europa. Se desea que el grado de dulzura sea seco o semiseco. El sabor debe ser delicado o moderado, mientras que el cuerpo debe ser medio o ligero. Además, la bodega del vino debería ser una bodega cara. Hay dos productos que cumplen con todos estos requisitos. El cliente quiere comprar vino rosado moderado, que se encuentra alrededor de la región francesa. La categoría de bodega debería ser Bodega Moderada. Solo hay un producto que cumple con estos requisitos. • El cliente quiere comprar vino tinto caro, que se encuentra alrededor de la Región de California o vino blanco barato, que se encuentra alrededor de la Región de Texas. Hay cinco productos disponibles. • El cliente quiere comprar un vino blanco delicado cuyo productor esté en la categoría de Bodega Costosa. Hay dos productos disponibles. Los primeros siete escenarios se prueban con el primer conjunto de datos que contiene un total de 19 servicios y los últimos tres escenarios se prueban con el segundo conjunto de datos que contiene 50 servicios. La Tabla 5 muestra la evaluación del rendimiento en términos del número de interacciones necesarias para llegar a un consenso. La métrica de Tversky da los peores resultados ya que no considera la similitud semántica. El rendimiento de Lins es mejor que el de Tversky pero peor que el de otros. La métrica de Wu-Palmer y la medida de similitud de RP casi ofrecen el mismo rendimiento y son mejores que otras. Cuando se examinan los resultados, considerar la cercanía semántica aumenta el rendimiento. 8. DISCUSIÓN Revisamos la literatura reciente en comparación con nuestro trabajo. Tama et al. [16] proponen un nuevo enfoque basado en ontología para la negociación. Según su enfoque, los protocolos de negociación utilizados en el comercio electrónico pueden ser modelados como ontologías. Por lo tanto, los agentes pueden llevar a cabo un protocolo de negociación utilizando esta ontología compartida sin necesidad de estar codificados con los detalles del protocolo de negociación. Mientras tanto, la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1307 Tabla 5: Comparación de métricas de similitud en términos de número de interacciones. Ejecutar Tversky Lin Wu Palmer RP Escenario 1: 1.2 1.2 1 1 Escenario 2: 1.4 1.4 1.6 1.6 Escenario 3: 1.4 1.8 2 2 Escenario 4: 2.2 1 1.2 1.2 Escenario 5: 2 1.6 1.6 1.6 Escenario 6: 5 3.8 2.4 2.6 Escenario 7: 3.2 1.2 1 1 Escenario 8: 5.6 2 2 2.2 Escenario 9: 2.6 2.2 2.2 2.6 Escenario 10: 4.4 2 2 1.8 Promedio de todos los casos: 2.9 1.82 1.7 1.76 Tama et al. modelan el protocolo de negociación utilizando ontologías, en cambio, nosotros hemos modelado el servicio a ser negociado. Además, hemos construido un sistema con el cual se pueden aprender las preferencias de negociación. El estudio de Sadri et al. analiza la negociación en el contexto de la asignación de recursos [14]. Los agentes tienen recursos limitados y necesitan solicitar recursos faltantes a otros agentes. Se propone un mecanismo basado en secuencias de diálogo entre agentes como solución. El mecanismo se basa en el ciclo de agente de observar-pensar-actuar. Estos diálogos incluyen ofrecer recursos, intercambios de recursos y ofrecer recursos alternativos. Cada agente en el sistema planea sus acciones para alcanzar un estado objetivo. A diferencia de nuestro enfoque, el estudio de Sadri et al. no se preocupa por las preferencias de aprendizaje mutuas. Brzostowski y Kowalczyk proponen un enfoque para seleccionar un socio de negociación adecuado investigando negociaciones previas de múltiples atributos [1]. Para lograr esto, utilizan el razonamiento basado en casos. Su enfoque es probabilístico ya que el comportamiento de los socios puede cambiar en cada iteración. En nuestro enfoque, estamos interesados en negociar el contenido del servicio. Después de que el consumidor y el productor acuerden el servicio, se pueden utilizar mecanismos de negociación orientados al precio para acordar el precio. Fatima et al. estudian los factores que afectan la negociación, como las preferencias, el plazo, el precio, entre otros, ya que el agente que desarrolla una estrategia contra su oponente debe considerar todos ellos [5]. En su enfoque, el objetivo del agente vendedor es vender el servicio al precio más alto posible, mientras que el objetivo del agente comprador es comprar el bien al precio más bajo posible. El intervalo de tiempo afecta a estos agentes de manera diferente. En comparación con Fatima et al., nuestro enfoque es diferente. Mientras ellos estudian el efecto del tiempo en la negociación, nuestro enfoque está en aprender las preferencias para una negociación exitosa. Faratin et al. proponen un mecanismo de negociación multi-tema, donde las variables de servicio para la negociación, como el precio, la calidad del servicio, entre otros, se consideran intercambios entre sí (es decir, un precio más alto por una entrega más temprana) [4]. Generan un modelo heurístico para compensaciones que incluye la estimación de similitud difusa y una exploración de escalada de colina para ofertas posiblemente aceptables. Aunque abordamos un problema similar, aprendemos las preferencias del cliente con la ayuda del aprendizaje inductivo y generamos contraofertas de acuerdo con estas preferencias aprendidas. Faratin et al. solo utilizan la última oferta realizada por el consumidor al calcular la similitud para elegir la contraoferta. A diferencia de ellos, también tenemos en cuenta las solicitudes previas del consumidor. En sus experimentos, Faratin et al. asumen que los pesos de las variables de servicio están fijos a priori. Por el contrario, aprendemos estas preferencias con el tiempo. En nuestro trabajo futuro, planeamos integrar el razonamiento ontológico en el algoritmo de aprendizaje para que la información jerárquica pueda ser aprendida a partir de la jerarquía de subsumpción de relaciones. Además, al utilizar las relaciones entre las características, el productor puede descubrir nuevos conocimientos a partir de los conocimientos existentes. Estas son direcciones interesantes que seguiremos en nuestro trabajo futuro. 9. REFERENCIAS [1] J. Brzostowski y R. Kowalczyk. En el razonamiento basado en casos posibilístico para la selección de socios para la negociación de agentes de múltiples atributos. En Actas del 4to Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 273-278, 2005. [2] L. Busch e I. Horstman. Un comentario sobre negociaciones tema por tema. Juegos y Comportamiento Económico, 19:144-148, 1997. [3] J. K. Debenham. Gestión de la negociación en el mercado electrónico en el contexto de un sistema multiagente. En Actas de la 21ª Conferencia Internacional sobre Sistemas Basados en el Conocimiento e Inteligencia Artificial Aplicada, ES2002:, 2002. [4] P. Faratin, C. Sierra y N. R. Jennings. Utilizando criterios de similitud para hacer compensaciones de problemas en negociaciones automatizadas. Inteligencia Artificial, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge y N. Jennings. Agentes óptimos para negociaciones de múltiples temas. En Actas del 2do Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 129-136, 2003. [6] C. Giraud-Carrier. Una nota sobre la utilidad del aprendizaje incremental. Comunicaciones de IA, 13(4):215-223, 2000. [7] T.-P. Hong y S.-S. Tseng. Dividiendo y fusionando espacios de versiones para aprender conceptos disyuntivos. IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.\n\nTraducción al español:\nIEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin. Una definición de similitud basada en teoría de la información. En Actas de la 15ª Conferencia Internacional sobre Aprendizaje Automático, páginas 296-304. Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, y A. G. Moukas. Agentes que compran y venden. Comunicaciones de la ACM, 42(3):81-91, 1999. [10] T. M. Mitchell. Aprendizaje automático. McGraw Hill, NY, 1997. [11] Búho. OWL: Guía del lenguaje de ontologías web, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal y S. C. K. Shiu. Fundamentos del Razonamiento Basado en Casos Blandos. John Wiley & Sons, Nueva Jersey, 2004. [13] J. R. Quinlan. Inducción de árboles de decisión. Aprendizaje automático, 1(1):81-106, 1986. [14] F. Sadri, F. Toni y P. Torroni. Diálogos para negociación: Variedades de agentes y secuencias de diálogo. En ATAL 2001, Artículos Revisados, volumen 2333 de LNAI, páginas 405-421. Springer-Verlag, 2002. [15] M. P. Singh. \n\nSpringer-Verlag, 2002. [15] M. P. Singh. Comercio electrónico orientado al valor. IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, y M. Wooldridge. Ontologías para apoyar la negociación en el comercio electrónico. Aplicaciones de la Inteligencia Artificial en Ingeniería, 18:223-236, 2005. [17] A. Tversky. Características de similitud. Revisión Psicológica, 84(4):327-352, 1977. [18] P. E. Utgoff. Inducción incremental de árboles de decisión. Aprendizaje automático, 4:161-186, 1989. [19] Vino, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu y M. Palmer. Semántica de verbos y selección léxica. En el 32. Reunión anual de la Asociación de Lingüística Computacional, páginas 133-138, 1994. 1308 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "decision tree": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental <br>decision tree</br> is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The <br>decision tree</br> has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the <br>decision tree</br> is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "A well-known incremental <br>decision tree</br> is ID5R [18].",
                "The <br>decision tree</br> has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "After each coming request, the <br>decision tree</br> is rebuilt."
            ],
            "translated_annotated_samples": [
                "Un <br>árbol de decisión</br> incremental bien conocido es ID5R [18].",
                "El <br>árbol de decisión</br> tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba.",
                "Después de cada solicitud entrante, el <br>árbol de decisiones</br> se reconstruye."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor. La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor. Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios. Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las preferencias del consumidor y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta. Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor. Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles. Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor. Este desafío se puede estudiar en tres etapas: • Aprendizaje de preferencias: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5) APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica. Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio. Por ejemplo, un consumidor puede tener preferencia por el vino tinto. Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo. Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo). Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor. Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores. Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa. Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo. Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6]. Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones. Investigamos particularmente dos enfoques. El primero es el aprendizaje inductivo. Esta técnica se aplica para aprender las preferencias como conceptos. Desarrollamos el Algoritmo de Eliminación de Candidatos (CEA) para el Espacio de Versiones [10]. Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva. Curiosamente, la mayoría de las veces las preferencias del consumidor son disyuntivas. Estamos considerando un agente que está comprando vino. El consumidor puede preferir vino tinto o vino rosado pero no vino blanco. Para utilizar CEA con tales preferencias, es necesaria una modificación sólida. El segundo enfoque son los árboles de decisión. Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas. Un <br>árbol de decisión</br> incremental bien conocido es ID5R [18]. Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional. Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados. El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender. El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener. Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto. El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo. Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío. Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo. En cada interacción entre el productor y el consumidor, tanto G como S son modificados. Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas. Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento. Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa. Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí. Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 CEA Disyuntivo Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos. Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos. Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones. Algunos de estos enfoques utilizan múltiples espacios de versión. Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7]. Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir hipótesis disyuntivas además de las conjunciones y la negación. Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica. EJEMPLO 1. Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco). El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor. Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco. En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}. Solo cuando todos los valores existan en la lista, serán reemplazados por ?. En otras palabras, permitimos que el algoritmo generalice más lentamente que antes. Modificamos el algoritmo CEA para hacer frente a este cambio. El algoritmo modificado, DCEA, se presenta como Algoritmo 1. Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de múltiples espacios de versiones. La fase de inicialización es la misma que el algoritmo original (líneas 1, 2). Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4). Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí. Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran. Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7). Cuando llega una muestra negativa, no cambiamos S como antes. Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15). A diferencia del CEA original, intentamos especializar el G mínimamente. El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13). Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada. Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez. Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14). Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar. Por ejemplo, considera el atributo del color. Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2. La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA. El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra. Después de la primera muestra positiva, S se generaliza para cubrir también la instancia. La segunda muestra es negativa. Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada. En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general. La tercera muestra es positiva y generaliza S aún más. Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco). Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis. Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}. Por el Algoritmo 1, no perdemos ninguna información. 3.3 ID3 ID3 [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor. Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos. El algoritmo ID3 se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas. Hay dos clases: positiva y negativa. Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor. Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas. El <br>árbol de decisión</br> tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba. El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio. Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos. Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja. El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender. Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos. Después de cada solicitud entrante, el <br>árbol de decisiones</br> se reconstruye. Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso. Sin embargo, en la práctica hemos evaluado que el ID3 es rápido y el costo de reconstrucción es insignificante. 4. OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA. El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes. Cuando el productor recibe una solicitud del consumidor, el conjunto de aprendizaje del productor se entrena con esta solicitud como una muestra positiva. Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios. El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor. En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor. El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores. Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios. Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios. Esta lista constituye el inventario de servicios de los proveedores. Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace. Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor. Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación. Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S). Calculamos las similitudes de varias maneras, como se explicará en la Sección 5. Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud. El agente productor puede romper el empate de varias maneras. Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de ID3 Si el productor aprende las preferencias de los consumidores con ID3, se aplica un mecanismo similar con dos diferencias. Primero, dado que ID3 no mantiene G, se eliminan de la lista de servicios aquellos no aceptados que se clasifican como negativos. Segundo, las similitudes de los posibles servicios no se miden con respecto a S, sino en cambio a todas las solicitudes previamente realizadas. 4.3 Mecanismos Alternativos de Oferta de Servicios Además de estos tres mecanismos de oferta de servicios (Oferta de Servicio con CEA, Oferta de Servicio con DCEA y Oferta de Servicio con ID3), incluimos otros dos mecanismos. 1304 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) • Oferta de Servicio Aleatoria (RO): El productor genera una contraoferta aleatoriamente de la lista de servicios disponibles, sin considerar las preferencias de los consumidores. • Oferta de Servicio considerando solo la solicitud actual (SCR): El productor selecciona una contraoferta de acuerdo con la similitud de la solicitud actual del consumidor pero no considera solicitudes anteriores. 5. ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una métrica de similitud que toma dos entradas y devuelve qué tan similares son. Existen varios métricos de similitud utilizados en sistemas de razonamiento basado en casos, como la suma ponderada de la distancia euclidiana, la distancia de Hamming, entre otros [12]. La métrica de similitud afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor. Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de similitud semántica llamada Similitud RP. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente. En la Ecuación (1), común representa la cantidad de atributos coincidentes, mientras que diferente representa la cantidad de atributos diferentes. Nuestra suposición actual es que α y β son iguales entre sí. SMpq = α(común) α(común) + β(diferente) (1) Aquí, al comparar dos características, asignamos cero para la disimilitud y uno para la similitud al omitir la cercanía semántica entre los valores de las características. La métrica de similitud de Tversky está diseñada para comparar dos vectores de características. En nuestro sistema, mientras que la lista de servicios que puede ofrecer el productor son cada uno un vector de características, el conjunto más específico S no es un vector de características. S consiste en hipótesis de vectores de características. Por lo tanto, estimamos la similitud de cada hipótesis dentro del conjunto más específico S y luego calculamos el promedio de las similitudes. EJEMPLO 3. Suponga que S contiene las siguientes dos hipótesis: { {Luz, Moderado, (Rojo, Blanco)} , {Completo, Fuerte, Rosa}}. Toma el servicio s como (Ligero, Resistente, Rosa). Entonces, la similitud del primero es igual a 1/3 y la del segundo es igual a 2/3 de acuerdo con la Ecuación (1). Normalmente, tomamos el promedio de ello y obtenemos (1/3 + 2/3)/2, que es igual a 1/2. Sin embargo, la primera hipótesis implica el efecto de dos solicitudes y la segunda hipótesis implica solo una solicitud. Por lo tanto, esperamos que el efecto de la primera hipótesis sea mayor que el de la segunda. Por lo tanto, calculamos la similitud promedio teniendo en cuenta la cantidad de muestras que las hipótesis cubren. Que ch denote el número de muestras que cubre la hipótesis h y (SM(h,servicio)) denote la similitud de la hipótesis h con el servicio dado. Calculamos la similitud de cada hipótesis con el servicio dado y las ponderamos con el número de muestras que cubren. Encontramos la similitud dividiendo la suma ponderada de las similitudes de todas las hipótesis en S con el servicio por el número de todas las muestras que están cubiertas en S. AV G−SM(servicio, S) = |S| |h| (ch ∗ SM(h, servicio)) |S| |h| ch (2) Figura 2: Taxonomía de muestra para estimación de similitud EJEMPLO 4. Para el ejemplo anterior, la similitud de (Luz, Fuerte, Rosa) con el conjunto específico es (2 ∗ 1/3 + 2/3)/3, igual a 4/9. El número posible de muestras que abarca una hipótesis se puede estimar multiplicando las cardinalidades de cada atributo. Por ejemplo, la cardinalidad del primer atributo es dos y la de los demás es igual a uno para la hipótesis dada, como {Luz, Moderado, (Rojo, Blanco)}. Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La métrica de similitud de Lins Un taxonomía puede ser utilizada al estimar la similitud semántica entre dos conceptos. Estimar la similitud semántica en una taxonomía de tipo Es-Un se puede hacer calculando la distancia entre los nodos relacionados con los conceptos comparados. Los enlaces entre los nodos pueden considerarse como distancias. Entonces, la longitud del camino entre los nodos indica qué tan similares son los conceptos. Una estimación alternativa para utilizar el contenido de información en la estimación de la similitud semántica en lugar del método de conteo de aristas, fue propuesta por Lin [8]. La ecuación (3) [8] muestra la similitud de Lin donde c1 y c2 son los conceptos comparados y c0 es el concepto más específico que subsume a ambos. Además, P(C) representa la probabilidad de que un objeto seleccionado arbitrariamente pertenezca al concepto C. La similitud(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Métrica de similitud de Wu y Palmers Diferente de Lin, Wu y Palmer utilizan la distancia entre los nodos en la taxonomía ES-UN [20]. La similitud semántica se representa con la Ecuación (4) [20]. Aquí, se estima la similitud entre c1 y c2 y c0 es el concepto más específico que subsume estas clases. N1 es el número de aristas entre c1 y c0. N2 es el número de aristas entre c2 y c0. N0 es el número de enlaces IS-A de c0 desde la raíz de la taxonomía. Proponemos estimar la distancia relativa en una taxonomía entre dos conceptos utilizando las siguientes intuiciones. Utilizamos la Figura 2 para ilustrar estas intuiciones. • Padre versus abuelo: El padre de un nodo es más similar al nodo que los abuelos de ese. Generalización del Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1305 es un concepto que razonablemente resulta en alejarse más de ese concepto. Cuanto más generales son los conceptos, menos similares son. Por ejemplo, AnyWineColor es el padre de ReddishColor y ReddishColor es el padre de Red. Entonces, esperamos que la similitud entre ReddishColor y Red sea mayor que la similitud entre AnyWineColor y Red. • Padre versus hermano: Un nodo tendría una similitud mayor con su padre que con su hermano. Por ejemplo, Rojo y Rosa son hijos de ColorRojo. En este caso, esperamos que la similitud entre Rojo y ColorRojizo sea mayor que la de Rojo y Rosa. • Hermano versus abuelo: Un nodo es más similar a su hermano que a su abuelo. Para ilustrar, AnyWineColor es el abuelo de Red, y Red y Rose son hermanos. Por lo tanto, posiblemente anticipamos que Rojo y Rosa son más similares que CualquierColorDeVino y Rojo. Como una taxonomía está representada en un árbol, ese árbol puede ser recorrido desde el primer concepto que se está comparando hasta el segundo concepto. En el nodo inicial relacionado con el primer concepto, el valor de similitud es constante y igual a uno. Este valor se reduce por una constante en cada nodo visitado a lo largo del camino que llegará al nodo que incluye el segundo concepto. Cuanto más corto sea el camino entre los conceptos, mayor será la similitud entre los nodos. Algoritmo 2 Estimar-Similitud-RP(c1,c2) Requerido: Las constantes deben ser m > n > m2 donde m, n ∈ R[0, 1] 1: Similitud ← 1 2: si c1 es igual a c2 entonces 3: Devolver Similitud 4: fin si 5: padreComun ← encontrarPadreComun(c1, c2) {padreComun es el concepto más específico que cubre tanto c1 como c2} 6: N1 ← encontrarDistancia(padreComun, c1) 7: N2 ← encontrarDistancia(padreComun, c2) {N1 y N2 son el número de enlaces entre el concepto y el concepto padre} 8: si (padreComun == c1) o (padreComun == c2) entonces 9: Similitud ← Similitud ∗ m(N1+N2) 10: sino 11: Similitud ← Similitud ∗ n ∗ m(N1+N2−2) 12: fin si 13: Devolver Similitud La distancia relativa entre los nodos c1 y c2 se estima de la siguiente manera. Comenzando desde c1, se recorre el árbol para llegar a c2. En cada salto, la similitud disminuye ya que los conceptos se están alejando cada vez más entre sí. Sin embargo, según nuestras intuiciones, no todos los saltos disminuyen la similitud de igual manera. Que m represente el factor para saltar de un hijo a un padre y que n represente el factor para saltar de un hermano a otro hermano. Dado que saltar de un nodo a su abuelo cuenta como dos saltos de padre, el factor de descuento al moverse de un nodo a su abuelo es m2. De acuerdo con las intuiciones anteriores, nuestras constantes deben estar en la forma m > n > m2 donde el valor de m y n debe estar entre cero y uno. El algoritmo 2 muestra el cálculo de la distancia. Según el algoritmo, en primer lugar la similitud se inicializa con el valor de uno (línea 1). Si los conceptos son iguales entre sí, entonces la similitud será uno (líneas 2-4). De lo contrario, calculamos el ancestro común de los dos nodos y la distancia de cada concepto al ancestro común sin considerar al hermano (líneas 5-7). Si uno de los conceptos es igual al padre común, entonces no hay relación de hermanos entre los conceptos. Para cada nivel, multiplicamos la similitud por m y no consideramos el factor de hermanos en la estimación de la similitud. Como resultado, disminuimos la similitud en cada nivel con la tasa de m (línea 9). De lo contrario, tiene que existir una relación de hermanos. Esto significa que debemos considerar el efecto de n al medir la similitud. Recuerde que hemos contado N1+N2 aristas entre los conceptos. Dado que existe una relación de hermanos, dos de estos bordes constituyen la relación de hermanos. Por lo tanto, al calcular el efecto de la relación parental, utilizamos N1+N2 −2 aristas (línea 11). Algunas estimaciones de similitud relacionadas con la taxonomía en la Figura 2 se presentan en la Tabla 2. En este ejemplo, se toma m como 2/3 y n como 4/7. Tabla 2: Estimación de similitud de muestra sobre la taxonomía de muestra. Similitud(ColorRojo, Rosa) = 1 ∗ (2/3) = 0.6666667 Similitud(Rojo, Rosa) = 1 ∗ (4/7) = 0.5714286 Similitud(CualquierColorVino, Rosa) = 1 ∗ (2/3)2 = 0.44444445 Similitud(Blanco, Rosa) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 Para todas las métricas de similitud semántica en nuestra arquitectura, la taxonomía de características se mantiene en la ontología compartida. Para evaluar la similitud del vector de características, primero estimamos la similitud para cada característica individualmente y luego calculamos la suma promedio de estas similitudes. Entonces, el resultado es igual a la similitud semántica promedio de todo el vector de características. 6. SISTEMA DESARROLLADO Hemos implementado nuestra arquitectura en Java. Para facilitar las pruebas del sistema, el agente del consumidor tiene una interfaz de usuario que nos permite ingresar varias solicitudes. El agente productor está completamente automatizado y las operaciones de aprendizaje y oferta de servicios funcionan como se explicó anteriormente. En esta sección, explicamos los detalles de implementación del sistema desarrollado. Utilizamos OWL [11] como nuestro lenguaje de ontología y JENA como nuestro razonador de ontología. La ontología compartida es la versión modificada de la Ontología del Vino [19]. Incluye la descripción del vino como concepto y diferentes tipos de vino. Todos los participantes de la negociación utilizan esta ontología para entenderse mutuamente. Según la ontología, siete propiedades conforman el concepto de vino. El agente consumidor y el agente productor obtienen los valores posibles para estas propiedades consultando la ontología. Por lo tanto, todos los valores posibles para los componentes del concepto del vino, como el color, cuerpo, azúcar, etc., pueden ser alcanzados por ambos agentes. También se describen en esta ontología una variedad de tipos de vino como Borgoña, Chardonnay, Chenin Blanc, entre otros. Intuitivamente, cualquier tipo de vino descrito en la ontología también representa un concepto de vino. Esto nos permite considerar las instancias de vino Chardonnay como instancias de la clase Vino. Además de la descripción del vino, la información jerárquica de algunas características se puede inferir de la ontología. Por ejemplo, podemos representar la información de que el continente europeo abarca países occidentales. El país occidental abarca la región francesa, que incluye algunos territorios como el Loira, Burdeos, entre otros. Esta información jerárquica se utiliza en la estimación de similitud semántica. En esta parte, se pueden hacer algunos razonamientos como si un concepto X abarca Y y Y abarca Z, entonces el concepto X abarca Z. Por ejemplo, el Continente Europeo abarca Burdeos. 1306 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Para algunas características como cuerpo, sabor y azúcar, no hay información jerárquica, pero sus valores están nivelados semánticamente. Cuando eso sucede, proporcionamos los valores de similitud razonables para estas características. Por ejemplo, el cuerpo puede ser ligero, medio o fuerte. En este caso, asumimos que la luz es 0.66 similar a media pero solo 0.33 a fuerte. La ontología de WineStock es el inventario de los productores y describe una clase de producto como WineProduct. Esta clase es necesaria para que el productor registre los vinos que vende. La ontología implica a los individuos de esta clase. Los individuos representan los servicios disponibles que posee el productor. Hemos preparado dos ontologías de WineStock separadas para realizar pruebas. En la primera ontología, hay 19 productos de vino disponibles y en la segunda ontología, hay 50 productos. EVALUACIÓN DEL RENDIMIENTO Evaluamos el rendimiento de los sistemas propuestos en relación con la técnica de aprendizaje que utilizaron, DCEA e ID3, comparándolos con CEA, RO (para oferta aleatoria) y SCR (oferta basada solo en la solicitud actual). Aplicamos una variedad de escenarios en este conjunto de datos para ver las diferencias de rendimiento. Cada escenario de prueba contiene una lista de preferencias para el usuario y el número de coincidencias de la lista de productos. La Tabla 3 muestra estas preferencias y la disponibilidad de esos productos en el inventario para los primeros cinco escenarios. Ten en cuenta que estas preferencias son internas al consumidor y el productor intenta aprenderlas durante la negociación. Tabla 3: Disponibilidad de vinos en diferentes escenarios de prueba ID Preferencia del consumidor Disponibilidad (de 19) 1 Vino seco 15 2 Vino tinto y seco 8 3 Vino tinto, seco y moderado 4 4 Vino tinto y fuerte 2 5 Vino tinto o rosado, y fuerte 3 7.1 Comparación de Algoritmos de Aprendizaje En la comparación de algoritmos de aprendizaje, utilizamos los cinco escenarios de la Tabla 3. Aquí, primero usamos la medida de similitud de Tversky. Con estos casos de prueba, estamos interesados en encontrar el número de iteraciones que se requieren para que el productor genere una oferta aceptable para el consumidor. Dado que el rendimiento también depende de la solicitud inicial, repetimos nuestros experimentos con diferentes solicitudes iniciales. Por consiguiente, para cada caso, ejecutamos los algoritmos cinco veces con varias variaciones de las solicitudes iniciales. En cada experimento, contamos el número de iteraciones necesarias para llegar a un acuerdo. Tomamos el promedio de estos números para evaluar estos sistemas de manera justa. Como es costumbre, probamos cada algoritmo con las mismas solicitudes iniciales. La Tabla 4 compara los enfoques utilizando diferentes algoritmos de aprendizaje. Cuando las partes grandes del inventario son compatibles con las preferencias de los clientes, como en el primer caso de prueba, el rendimiento de todas las técnicas es casi el mismo (por ejemplo, Escenario 1). A medida que el número de servicios compatibles disminuye, RO funciona mal como se esperaba. El segundo peor método es SCR ya que solo considera la solicitud más reciente de los clientes y no aprende de las solicitudes anteriores. CEA da los mejores resultados cuando puede generar una respuesta pero no puede manejar los casos que contienen preferencias disyuntivas, como el que se presenta en el Escenario 5. ID3 y DCEA logran los mejores resultados. Su rendimiento es comparable y pueden manejar todos los casos, incluido el Escenario 5. Tabla 4: Comparación de algoritmos de aprendizaje en términos del número promedio de interacciones. Ejecutar DCEA SCR RO CEA ID3 Escenario 1: 1.2 1.4 1.2 1.2 1.2 Escenario 2: 1.4 1.4 2.6 1.4 1.4 Escenario 3: 1.4 1.8 4.4 1.4 1.4 Escenario 4: 2.2 2.8 9.6 1.8 2 Escenario 5: 2 2.6 7.6 1.75+ Sin oferta 1.8 Promedio de todos los casos: 1.64 2 5.08 1.51+Sin oferta 1.56 7.2 Comparación de Métricas de Similitud Para comparar las métricas de similitud que se explicaron en la Sección 5, fijamos el algoritmo de aprendizaje en DCEA. Además de los escenarios mostrados en la Tabla 3, agregamos los siguientes cinco nuevos escenarios considerando la información jerárquica. • El cliente desea comprar vino cuya bodega esté ubicada en California y cuya uva sea de tipo blanco. Además, la bodega del vino no debería ser costosa. Solo hay cuatro productos que cumplen con estas condiciones. • El cliente quiere comprar vino de color rojo o rosado y de tipo de uva tinta. Además, la ubicación del vino debe ser en Europa. Se desea que el grado de dulzura sea seco o semiseco. El sabor debe ser delicado o moderado, mientras que el cuerpo debe ser medio o ligero. Además, la bodega del vino debería ser una bodega cara. Hay dos productos que cumplen con todos estos requisitos. El cliente quiere comprar vino rosado moderado, que se encuentra alrededor de la región francesa. La categoría de bodega debería ser Bodega Moderada. Solo hay un producto que cumple con estos requisitos. • El cliente quiere comprar vino tinto caro, que se encuentra alrededor de la Región de California o vino blanco barato, que se encuentra alrededor de la Región de Texas. Hay cinco productos disponibles. • El cliente quiere comprar un vino blanco delicado cuyo productor esté en la categoría de Bodega Costosa. Hay dos productos disponibles. Los primeros siete escenarios se prueban con el primer conjunto de datos que contiene un total de 19 servicios y los últimos tres escenarios se prueban con el segundo conjunto de datos que contiene 50 servicios. La Tabla 5 muestra la evaluación del rendimiento en términos del número de interacciones necesarias para llegar a un consenso. La métrica de Tversky da los peores resultados ya que no considera la similitud semántica. El rendimiento de Lins es mejor que el de Tversky pero peor que el de otros. La métrica de Wu-Palmer y la medida de similitud de RP casi ofrecen el mismo rendimiento y son mejores que otras. Cuando se examinan los resultados, considerar la cercanía semántica aumenta el rendimiento. 8. DISCUSIÓN Revisamos la literatura reciente en comparación con nuestro trabajo. Tama et al. [16] proponen un nuevo enfoque basado en ontología para la negociación. Según su enfoque, los protocolos de negociación utilizados en el comercio electrónico pueden ser modelados como ontologías. Por lo tanto, los agentes pueden llevar a cabo un protocolo de negociación utilizando esta ontología compartida sin necesidad de estar codificados con los detalles del protocolo de negociación. Mientras tanto, la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1307 Tabla 5: Comparación de métricas de similitud en términos de número de interacciones. Ejecutar Tversky Lin Wu Palmer RP Escenario 1: 1.2 1.2 1 1 Escenario 2: 1.4 1.4 1.6 1.6 Escenario 3: 1.4 1.8 2 2 Escenario 4: 2.2 1 1.2 1.2 Escenario 5: 2 1.6 1.6 1.6 Escenario 6: 5 3.8 2.4 2.6 Escenario 7: 3.2 1.2 1 1 Escenario 8: 5.6 2 2 2.2 Escenario 9: 2.6 2.2 2.2 2.6 Escenario 10: 4.4 2 2 1.8 Promedio de todos los casos: 2.9 1.82 1.7 1.76 Tama et al. modelan el protocolo de negociación utilizando ontologías, en cambio, nosotros hemos modelado el servicio a ser negociado. Además, hemos construido un sistema con el cual se pueden aprender las preferencias de negociación. El estudio de Sadri et al. analiza la negociación en el contexto de la asignación de recursos [14]. Los agentes tienen recursos limitados y necesitan solicitar recursos faltantes a otros agentes. Se propone un mecanismo basado en secuencias de diálogo entre agentes como solución. El mecanismo se basa en el ciclo de agente de observar-pensar-actuar. Estos diálogos incluyen ofrecer recursos, intercambios de recursos y ofrecer recursos alternativos. Cada agente en el sistema planea sus acciones para alcanzar un estado objetivo. A diferencia de nuestro enfoque, el estudio de Sadri et al. no se preocupa por las preferencias de aprendizaje mutuas. Brzostowski y Kowalczyk proponen un enfoque para seleccionar un socio de negociación adecuado investigando negociaciones previas de múltiples atributos [1]. Para lograr esto, utilizan el razonamiento basado en casos. Su enfoque es probabilístico ya que el comportamiento de los socios puede cambiar en cada iteración. En nuestro enfoque, estamos interesados en negociar el contenido del servicio. Después de que el consumidor y el productor acuerden el servicio, se pueden utilizar mecanismos de negociación orientados al precio para acordar el precio. Fatima et al. estudian los factores que afectan la negociación, como las preferencias, el plazo, el precio, entre otros, ya que el agente que desarrolla una estrategia contra su oponente debe considerar todos ellos [5]. En su enfoque, el objetivo del agente vendedor es vender el servicio al precio más alto posible, mientras que el objetivo del agente comprador es comprar el bien al precio más bajo posible. El intervalo de tiempo afecta a estos agentes de manera diferente. En comparación con Fatima et al., nuestro enfoque es diferente. Mientras ellos estudian el efecto del tiempo en la negociación, nuestro enfoque está en aprender las preferencias para una negociación exitosa. Faratin et al. proponen un mecanismo de negociación multi-tema, donde las variables de servicio para la negociación, como el precio, la calidad del servicio, entre otros, se consideran intercambios entre sí (es decir, un precio más alto por una entrega más temprana) [4]. Generan un modelo heurístico para compensaciones que incluye la estimación de similitud difusa y una exploración de escalada de colina para ofertas posiblemente aceptables. Aunque abordamos un problema similar, aprendemos las preferencias del cliente con la ayuda del aprendizaje inductivo y generamos contraofertas de acuerdo con estas preferencias aprendidas. Faratin et al. solo utilizan la última oferta realizada por el consumidor al calcular la similitud para elegir la contraoferta. A diferencia de ellos, también tenemos en cuenta las solicitudes previas del consumidor. En sus experimentos, Faratin et al. asumen que los pesos de las variables de servicio están fijos a priori. Por el contrario, aprendemos estas preferencias con el tiempo. En nuestro trabajo futuro, planeamos integrar el razonamiento ontológico en el algoritmo de aprendizaje para que la información jerárquica pueda ser aprendida a partir de la jerarquía de subsumpción de relaciones. Además, al utilizar las relaciones entre las características, el productor puede descubrir nuevos conocimientos a partir de los conocimientos existentes. Estas son direcciones interesantes que seguiremos en nuestro trabajo futuro. 9. REFERENCIAS [1] J. Brzostowski y R. Kowalczyk. En el razonamiento basado en casos posibilístico para la selección de socios para la negociación de agentes de múltiples atributos. En Actas del 4to Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 273-278, 2005. [2] L. Busch e I. Horstman. Un comentario sobre negociaciones tema por tema. Juegos y Comportamiento Económico, 19:144-148, 1997. [3] J. K. Debenham. Gestión de la negociación en el mercado electrónico en el contexto de un sistema multiagente. En Actas de la 21ª Conferencia Internacional sobre Sistemas Basados en el Conocimiento e Inteligencia Artificial Aplicada, ES2002:, 2002. [4] P. Faratin, C. Sierra y N. R. Jennings. Utilizando criterios de similitud para hacer compensaciones de problemas en negociaciones automatizadas. Inteligencia Artificial, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge y N. Jennings. Agentes óptimos para negociaciones de múltiples temas. En Actas del 2do Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 129-136, 2003. [6] C. Giraud-Carrier. Una nota sobre la utilidad del aprendizaje incremental. Comunicaciones de IA, 13(4):215-223, 2000. [7] T.-P. Hong y S.-S. Tseng. Dividiendo y fusionando espacios de versiones para aprender conceptos disyuntivos. IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.\n\nTraducción al español:\nIEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin. Una definición de similitud basada en teoría de la información. En Actas de la 15ª Conferencia Internacional sobre Aprendizaje Automático, páginas 296-304. Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, y A. G. Moukas. Agentes que compran y venden. Comunicaciones de la ACM, 42(3):81-91, 1999. [10] T. M. Mitchell. Aprendizaje automático. McGraw Hill, NY, 1997. [11] Búho. OWL: Guía del lenguaje de ontologías web, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal y S. C. K. Shiu. Fundamentos del Razonamiento Basado en Casos Blandos. John Wiley & Sons, Nueva Jersey, 2004. [13] J. R. Quinlan. Inducción de árboles de decisión. Aprendizaje automático, 1(1):81-106, 1986. [14] F. Sadri, F. Toni y P. Torroni. Diálogos para negociación: Variedades de agentes y secuencias de diálogo. En ATAL 2001, Artículos Revisados, volumen 2333 de LNAI, páginas 405-421. Springer-Verlag, 2002. [15] M. P. Singh. \n\nSpringer-Verlag, 2002. [15] M. P. Singh. Comercio electrónico orientado al valor. IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, y M. Wooldridge. Ontologías para apoyar la negociación en el comercio electrónico. Aplicaciones de la Inteligencia Artificial en Ingeniería, 18:223-236, 2005. [17] A. Tversky. Características de similitud. Revisión Psicológica, 84(4):327-352, 1977. [18] P. E. Utgoff. Inducción incremental de árboles de decisión. Aprendizaje automático, 4:161-186, 1989. [19] Vino, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu y M. Palmer. Semántica de verbos y selección léxica. En el 32. Reunión anual de la Asociación de Lingüística Computacional, páginas 133-138, 1994. 1308 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    "árbol de decisión",
                    "árbol de decisión",
                    "árbol de decisiones"
                ]
            ]
        },
        "incremental decision tree": {
            "translated_key": "árbol de decisión incremental",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known <br>incremental decision tree</br> is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "A well-known <br>incremental decision tree</br> is ID5R [18]."
            ],
            "translated_annotated_samples": [
                "Un <br>árbol de decisión incremental</br> bien conocido es ID5R [18]."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor. La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor. Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios. Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las preferencias del consumidor y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta. Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor. Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles. Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor. Este desafío se puede estudiar en tres etapas: • Aprendizaje de preferencias: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5) APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica. Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio. Por ejemplo, un consumidor puede tener preferencia por el vino tinto. Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo. Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo). Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor. Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores. Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa. Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo. Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6]. Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones. Investigamos particularmente dos enfoques. El primero es el aprendizaje inductivo. Esta técnica se aplica para aprender las preferencias como conceptos. Desarrollamos el Algoritmo de Eliminación de Candidatos (CEA) para el Espacio de Versiones [10]. Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva. Curiosamente, la mayoría de las veces las preferencias del consumidor son disyuntivas. Estamos considerando un agente que está comprando vino. El consumidor puede preferir vino tinto o vino rosado pero no vino blanco. Para utilizar CEA con tales preferencias, es necesaria una modificación sólida. El segundo enfoque son los árboles de decisión. Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas. Un <br>árbol de decisión incremental</br> bien conocido es ID5R [18]. Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional. Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados. El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender. El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener. Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto. El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo. Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío. Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo. En cada interacción entre el productor y el consumidor, tanto G como S son modificados. Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas. Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento. Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa. Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí. Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 CEA Disyuntivo Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos. Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos. Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones. Algunos de estos enfoques utilizan múltiples espacios de versión. Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7]. Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir hipótesis disyuntivas además de las conjunciones y la negación. Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica. EJEMPLO 1. Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco). El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor. Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco. En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}. Solo cuando todos los valores existan en la lista, serán reemplazados por ?. En otras palabras, permitimos que el algoritmo generalice más lentamente que antes. Modificamos el algoritmo CEA para hacer frente a este cambio. El algoritmo modificado, DCEA, se presenta como Algoritmo 1. Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de múltiples espacios de versiones. La fase de inicialización es la misma que el algoritmo original (líneas 1, 2). Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4). Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí. Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran. Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7). Cuando llega una muestra negativa, no cambiamos S como antes. Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15). A diferencia del CEA original, intentamos especializar el G mínimamente. El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13). Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada. Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez. Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14). Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar. Por ejemplo, considera el atributo del color. Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2. La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA. El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra. Después de la primera muestra positiva, S se generaliza para cubrir también la instancia. La segunda muestra es negativa. Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada. En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general. La tercera muestra es positiva y generaliza S aún más. Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco). Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis. Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}. Por el Algoritmo 1, no perdemos ninguna información. 3.3 ID3 ID3 [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor. Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos. El algoritmo ID3 se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas. Hay dos clases: positiva y negativa. Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor. Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas. El árbol de decisión tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba. El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio. Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos. Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja. El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender. Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos. Después de cada solicitud entrante, el árbol de decisiones se reconstruye. Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso. Sin embargo, en la práctica hemos evaluado que el ID3 es rápido y el costo de reconstrucción es insignificante. 4. OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA. El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes. Cuando el productor recibe una solicitud del consumidor, el conjunto de aprendizaje del productor se entrena con esta solicitud como una muestra positiva. Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios. El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor. En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor. El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores. Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios. Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios. Esta lista constituye el inventario de servicios de los proveedores. Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace. Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor. Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación. Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S). Calculamos las similitudes de varias maneras, como se explicará en la Sección 5. Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud. El agente productor puede romper el empate de varias maneras. Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de ID3 Si el productor aprende las preferencias de los consumidores con ID3, se aplica un mecanismo similar con dos diferencias. Primero, dado que ID3 no mantiene G, se eliminan de la lista de servicios aquellos no aceptados que se clasifican como negativos. Segundo, las similitudes de los posibles servicios no se miden con respecto a S, sino en cambio a todas las solicitudes previamente realizadas. 4.3 Mecanismos Alternativos de Oferta de Servicios Además de estos tres mecanismos de oferta de servicios (Oferta de Servicio con CEA, Oferta de Servicio con DCEA y Oferta de Servicio con ID3), incluimos otros dos mecanismos. 1304 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) • Oferta de Servicio Aleatoria (RO): El productor genera una contraoferta aleatoriamente de la lista de servicios disponibles, sin considerar las preferencias de los consumidores. • Oferta de Servicio considerando solo la solicitud actual (SCR): El productor selecciona una contraoferta de acuerdo con la similitud de la solicitud actual del consumidor pero no considera solicitudes anteriores. 5. ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una métrica de similitud que toma dos entradas y devuelve qué tan similares son. Existen varios métricos de similitud utilizados en sistemas de razonamiento basado en casos, como la suma ponderada de la distancia euclidiana, la distancia de Hamming, entre otros [12]. La métrica de similitud afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor. Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de similitud semántica llamada Similitud RP. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente. En la Ecuación (1), común representa la cantidad de atributos coincidentes, mientras que diferente representa la cantidad de atributos diferentes. Nuestra suposición actual es que α y β son iguales entre sí. SMpq = α(común) α(común) + β(diferente) (1) Aquí, al comparar dos características, asignamos cero para la disimilitud y uno para la similitud al omitir la cercanía semántica entre los valores de las características. La métrica de similitud de Tversky está diseñada para comparar dos vectores de características. En nuestro sistema, mientras que la lista de servicios que puede ofrecer el productor son cada uno un vector de características, el conjunto más específico S no es un vector de características. S consiste en hipótesis de vectores de características. Por lo tanto, estimamos la similitud de cada hipótesis dentro del conjunto más específico S y luego calculamos el promedio de las similitudes. EJEMPLO 3. Suponga que S contiene las siguientes dos hipótesis: { {Luz, Moderado, (Rojo, Blanco)} , {Completo, Fuerte, Rosa}}. Toma el servicio s como (Ligero, Resistente, Rosa). Entonces, la similitud del primero es igual a 1/3 y la del segundo es igual a 2/3 de acuerdo con la Ecuación (1). Normalmente, tomamos el promedio de ello y obtenemos (1/3 + 2/3)/2, que es igual a 1/2. Sin embargo, la primera hipótesis implica el efecto de dos solicitudes y la segunda hipótesis implica solo una solicitud. Por lo tanto, esperamos que el efecto de la primera hipótesis sea mayor que el de la segunda. Por lo tanto, calculamos la similitud promedio teniendo en cuenta la cantidad de muestras que las hipótesis cubren. Que ch denote el número de muestras que cubre la hipótesis h y (SM(h,servicio)) denote la similitud de la hipótesis h con el servicio dado. Calculamos la similitud de cada hipótesis con el servicio dado y las ponderamos con el número de muestras que cubren. Encontramos la similitud dividiendo la suma ponderada de las similitudes de todas las hipótesis en S con el servicio por el número de todas las muestras que están cubiertas en S. AV G−SM(servicio, S) = |S| |h| (ch ∗ SM(h, servicio)) |S| |h| ch (2) Figura 2: Taxonomía de muestra para estimación de similitud EJEMPLO 4. Para el ejemplo anterior, la similitud de (Luz, Fuerte, Rosa) con el conjunto específico es (2 ∗ 1/3 + 2/3)/3, igual a 4/9. El número posible de muestras que abarca una hipótesis se puede estimar multiplicando las cardinalidades de cada atributo. Por ejemplo, la cardinalidad del primer atributo es dos y la de los demás es igual a uno para la hipótesis dada, como {Luz, Moderado, (Rojo, Blanco)}. Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La métrica de similitud de Lins Un taxonomía puede ser utilizada al estimar la similitud semántica entre dos conceptos. Estimar la similitud semántica en una taxonomía de tipo Es-Un se puede hacer calculando la distancia entre los nodos relacionados con los conceptos comparados. Los enlaces entre los nodos pueden considerarse como distancias. Entonces, la longitud del camino entre los nodos indica qué tan similares son los conceptos. Una estimación alternativa para utilizar el contenido de información en la estimación de la similitud semántica en lugar del método de conteo de aristas, fue propuesta por Lin [8]. La ecuación (3) [8] muestra la similitud de Lin donde c1 y c2 son los conceptos comparados y c0 es el concepto más específico que subsume a ambos. Además, P(C) representa la probabilidad de que un objeto seleccionado arbitrariamente pertenezca al concepto C. La similitud(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Métrica de similitud de Wu y Palmers Diferente de Lin, Wu y Palmer utilizan la distancia entre los nodos en la taxonomía ES-UN [20]. La similitud semántica se representa con la Ecuación (4) [20]. Aquí, se estima la similitud entre c1 y c2 y c0 es el concepto más específico que subsume estas clases. N1 es el número de aristas entre c1 y c0. N2 es el número de aristas entre c2 y c0. N0 es el número de enlaces IS-A de c0 desde la raíz de la taxonomía. Proponemos estimar la distancia relativa en una taxonomía entre dos conceptos utilizando las siguientes intuiciones. Utilizamos la Figura 2 para ilustrar estas intuiciones. • Padre versus abuelo: El padre de un nodo es más similar al nodo que los abuelos de ese. Generalización del Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1305 es un concepto que razonablemente resulta en alejarse más de ese concepto. Cuanto más generales son los conceptos, menos similares son. Por ejemplo, AnyWineColor es el padre de ReddishColor y ReddishColor es el padre de Red. Entonces, esperamos que la similitud entre ReddishColor y Red sea mayor que la similitud entre AnyWineColor y Red. • Padre versus hermano: Un nodo tendría una similitud mayor con su padre que con su hermano. Por ejemplo, Rojo y Rosa son hijos de ColorRojo. En este caso, esperamos que la similitud entre Rojo y ColorRojizo sea mayor que la de Rojo y Rosa. • Hermano versus abuelo: Un nodo es más similar a su hermano que a su abuelo. Para ilustrar, AnyWineColor es el abuelo de Red, y Red y Rose son hermanos. Por lo tanto, posiblemente anticipamos que Rojo y Rosa son más similares que CualquierColorDeVino y Rojo. Como una taxonomía está representada en un árbol, ese árbol puede ser recorrido desde el primer concepto que se está comparando hasta el segundo concepto. En el nodo inicial relacionado con el primer concepto, el valor de similitud es constante y igual a uno. Este valor se reduce por una constante en cada nodo visitado a lo largo del camino que llegará al nodo que incluye el segundo concepto. Cuanto más corto sea el camino entre los conceptos, mayor será la similitud entre los nodos. Algoritmo 2 Estimar-Similitud-RP(c1,c2) Requerido: Las constantes deben ser m > n > m2 donde m, n ∈ R[0, 1] 1: Similitud ← 1 2: si c1 es igual a c2 entonces 3: Devolver Similitud 4: fin si 5: padreComun ← encontrarPadreComun(c1, c2) {padreComun es el concepto más específico que cubre tanto c1 como c2} 6: N1 ← encontrarDistancia(padreComun, c1) 7: N2 ← encontrarDistancia(padreComun, c2) {N1 y N2 son el número de enlaces entre el concepto y el concepto padre} 8: si (padreComun == c1) o (padreComun == c2) entonces 9: Similitud ← Similitud ∗ m(N1+N2) 10: sino 11: Similitud ← Similitud ∗ n ∗ m(N1+N2−2) 12: fin si 13: Devolver Similitud La distancia relativa entre los nodos c1 y c2 se estima de la siguiente manera. Comenzando desde c1, se recorre el árbol para llegar a c2. En cada salto, la similitud disminuye ya que los conceptos se están alejando cada vez más entre sí. Sin embargo, según nuestras intuiciones, no todos los saltos disminuyen la similitud de igual manera. Que m represente el factor para saltar de un hijo a un padre y que n represente el factor para saltar de un hermano a otro hermano. Dado que saltar de un nodo a su abuelo cuenta como dos saltos de padre, el factor de descuento al moverse de un nodo a su abuelo es m2. De acuerdo con las intuiciones anteriores, nuestras constantes deben estar en la forma m > n > m2 donde el valor de m y n debe estar entre cero y uno. El algoritmo 2 muestra el cálculo de la distancia. Según el algoritmo, en primer lugar la similitud se inicializa con el valor de uno (línea 1). Si los conceptos son iguales entre sí, entonces la similitud será uno (líneas 2-4). De lo contrario, calculamos el ancestro común de los dos nodos y la distancia de cada concepto al ancestro común sin considerar al hermano (líneas 5-7). Si uno de los conceptos es igual al padre común, entonces no hay relación de hermanos entre los conceptos. Para cada nivel, multiplicamos la similitud por m y no consideramos el factor de hermanos en la estimación de la similitud. Como resultado, disminuimos la similitud en cada nivel con la tasa de m (línea 9). De lo contrario, tiene que existir una relación de hermanos. Esto significa que debemos considerar el efecto de n al medir la similitud. Recuerde que hemos contado N1+N2 aristas entre los conceptos. Dado que existe una relación de hermanos, dos de estos bordes constituyen la relación de hermanos. Por lo tanto, al calcular el efecto de la relación parental, utilizamos N1+N2 −2 aristas (línea 11). Algunas estimaciones de similitud relacionadas con la taxonomía en la Figura 2 se presentan en la Tabla 2. En este ejemplo, se toma m como 2/3 y n como 4/7. Tabla 2: Estimación de similitud de muestra sobre la taxonomía de muestra. Similitud(ColorRojo, Rosa) = 1 ∗ (2/3) = 0.6666667 Similitud(Rojo, Rosa) = 1 ∗ (4/7) = 0.5714286 Similitud(CualquierColorVino, Rosa) = 1 ∗ (2/3)2 = 0.44444445 Similitud(Blanco, Rosa) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 Para todas las métricas de similitud semántica en nuestra arquitectura, la taxonomía de características se mantiene en la ontología compartida. Para evaluar la similitud del vector de características, primero estimamos la similitud para cada característica individualmente y luego calculamos la suma promedio de estas similitudes. Entonces, el resultado es igual a la similitud semántica promedio de todo el vector de características. 6. SISTEMA DESARROLLADO Hemos implementado nuestra arquitectura en Java. Para facilitar las pruebas del sistema, el agente del consumidor tiene una interfaz de usuario que nos permite ingresar varias solicitudes. El agente productor está completamente automatizado y las operaciones de aprendizaje y oferta de servicios funcionan como se explicó anteriormente. En esta sección, explicamos los detalles de implementación del sistema desarrollado. Utilizamos OWL [11] como nuestro lenguaje de ontología y JENA como nuestro razonador de ontología. La ontología compartida es la versión modificada de la Ontología del Vino [19]. Incluye la descripción del vino como concepto y diferentes tipos de vino. Todos los participantes de la negociación utilizan esta ontología para entenderse mutuamente. Según la ontología, siete propiedades conforman el concepto de vino. El agente consumidor y el agente productor obtienen los valores posibles para estas propiedades consultando la ontología. Por lo tanto, todos los valores posibles para los componentes del concepto del vino, como el color, cuerpo, azúcar, etc., pueden ser alcanzados por ambos agentes. También se describen en esta ontología una variedad de tipos de vino como Borgoña, Chardonnay, Chenin Blanc, entre otros. Intuitivamente, cualquier tipo de vino descrito en la ontología también representa un concepto de vino. Esto nos permite considerar las instancias de vino Chardonnay como instancias de la clase Vino. Además de la descripción del vino, la información jerárquica de algunas características se puede inferir de la ontología. Por ejemplo, podemos representar la información de que el continente europeo abarca países occidentales. El país occidental abarca la región francesa, que incluye algunos territorios como el Loira, Burdeos, entre otros. Esta información jerárquica se utiliza en la estimación de similitud semántica. En esta parte, se pueden hacer algunos razonamientos como si un concepto X abarca Y y Y abarca Z, entonces el concepto X abarca Z. Por ejemplo, el Continente Europeo abarca Burdeos. 1306 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Para algunas características como cuerpo, sabor y azúcar, no hay información jerárquica, pero sus valores están nivelados semánticamente. Cuando eso sucede, proporcionamos los valores de similitud razonables para estas características. Por ejemplo, el cuerpo puede ser ligero, medio o fuerte. En este caso, asumimos que la luz es 0.66 similar a media pero solo 0.33 a fuerte. La ontología de WineStock es el inventario de los productores y describe una clase de producto como WineProduct. Esta clase es necesaria para que el productor registre los vinos que vende. La ontología implica a los individuos de esta clase. Los individuos representan los servicios disponibles que posee el productor. Hemos preparado dos ontologías de WineStock separadas para realizar pruebas. En la primera ontología, hay 19 productos de vino disponibles y en la segunda ontología, hay 50 productos. EVALUACIÓN DEL RENDIMIENTO Evaluamos el rendimiento de los sistemas propuestos en relación con la técnica de aprendizaje que utilizaron, DCEA e ID3, comparándolos con CEA, RO (para oferta aleatoria) y SCR (oferta basada solo en la solicitud actual). Aplicamos una variedad de escenarios en este conjunto de datos para ver las diferencias de rendimiento. Cada escenario de prueba contiene una lista de preferencias para el usuario y el número de coincidencias de la lista de productos. La Tabla 3 muestra estas preferencias y la disponibilidad de esos productos en el inventario para los primeros cinco escenarios. Ten en cuenta que estas preferencias son internas al consumidor y el productor intenta aprenderlas durante la negociación. Tabla 3: Disponibilidad de vinos en diferentes escenarios de prueba ID Preferencia del consumidor Disponibilidad (de 19) 1 Vino seco 15 2 Vino tinto y seco 8 3 Vino tinto, seco y moderado 4 4 Vino tinto y fuerte 2 5 Vino tinto o rosado, y fuerte 3 7.1 Comparación de Algoritmos de Aprendizaje En la comparación de algoritmos de aprendizaje, utilizamos los cinco escenarios de la Tabla 3. Aquí, primero usamos la medida de similitud de Tversky. Con estos casos de prueba, estamos interesados en encontrar el número de iteraciones que se requieren para que el productor genere una oferta aceptable para el consumidor. Dado que el rendimiento también depende de la solicitud inicial, repetimos nuestros experimentos con diferentes solicitudes iniciales. Por consiguiente, para cada caso, ejecutamos los algoritmos cinco veces con varias variaciones de las solicitudes iniciales. En cada experimento, contamos el número de iteraciones necesarias para llegar a un acuerdo. Tomamos el promedio de estos números para evaluar estos sistemas de manera justa. Como es costumbre, probamos cada algoritmo con las mismas solicitudes iniciales. La Tabla 4 compara los enfoques utilizando diferentes algoritmos de aprendizaje. Cuando las partes grandes del inventario son compatibles con las preferencias de los clientes, como en el primer caso de prueba, el rendimiento de todas las técnicas es casi el mismo (por ejemplo, Escenario 1). A medida que el número de servicios compatibles disminuye, RO funciona mal como se esperaba. El segundo peor método es SCR ya que solo considera la solicitud más reciente de los clientes y no aprende de las solicitudes anteriores. CEA da los mejores resultados cuando puede generar una respuesta pero no puede manejar los casos que contienen preferencias disyuntivas, como el que se presenta en el Escenario 5. ID3 y DCEA logran los mejores resultados. Su rendimiento es comparable y pueden manejar todos los casos, incluido el Escenario 5. Tabla 4: Comparación de algoritmos de aprendizaje en términos del número promedio de interacciones. Ejecutar DCEA SCR RO CEA ID3 Escenario 1: 1.2 1.4 1.2 1.2 1.2 Escenario 2: 1.4 1.4 2.6 1.4 1.4 Escenario 3: 1.4 1.8 4.4 1.4 1.4 Escenario 4: 2.2 2.8 9.6 1.8 2 Escenario 5: 2 2.6 7.6 1.75+ Sin oferta 1.8 Promedio de todos los casos: 1.64 2 5.08 1.51+Sin oferta 1.56 7.2 Comparación de Métricas de Similitud Para comparar las métricas de similitud que se explicaron en la Sección 5, fijamos el algoritmo de aprendizaje en DCEA. Además de los escenarios mostrados en la Tabla 3, agregamos los siguientes cinco nuevos escenarios considerando la información jerárquica. • El cliente desea comprar vino cuya bodega esté ubicada en California y cuya uva sea de tipo blanco. Además, la bodega del vino no debería ser costosa. Solo hay cuatro productos que cumplen con estas condiciones. • El cliente quiere comprar vino de color rojo o rosado y de tipo de uva tinta. Además, la ubicación del vino debe ser en Europa. Se desea que el grado de dulzura sea seco o semiseco. El sabor debe ser delicado o moderado, mientras que el cuerpo debe ser medio o ligero. Además, la bodega del vino debería ser una bodega cara. Hay dos productos que cumplen con todos estos requisitos. El cliente quiere comprar vino rosado moderado, que se encuentra alrededor de la región francesa. La categoría de bodega debería ser Bodega Moderada. Solo hay un producto que cumple con estos requisitos. • El cliente quiere comprar vino tinto caro, que se encuentra alrededor de la Región de California o vino blanco barato, que se encuentra alrededor de la Región de Texas. Hay cinco productos disponibles. • El cliente quiere comprar un vino blanco delicado cuyo productor esté en la categoría de Bodega Costosa. Hay dos productos disponibles. Los primeros siete escenarios se prueban con el primer conjunto de datos que contiene un total de 19 servicios y los últimos tres escenarios se prueban con el segundo conjunto de datos que contiene 50 servicios. La Tabla 5 muestra la evaluación del rendimiento en términos del número de interacciones necesarias para llegar a un consenso. La métrica de Tversky da los peores resultados ya que no considera la similitud semántica. El rendimiento de Lins es mejor que el de Tversky pero peor que el de otros. La métrica de Wu-Palmer y la medida de similitud de RP casi ofrecen el mismo rendimiento y son mejores que otras. Cuando se examinan los resultados, considerar la cercanía semántica aumenta el rendimiento. 8. DISCUSIÓN Revisamos la literatura reciente en comparación con nuestro trabajo. Tama et al. [16] proponen un nuevo enfoque basado en ontología para la negociación. Según su enfoque, los protocolos de negociación utilizados en el comercio electrónico pueden ser modelados como ontologías. Por lo tanto, los agentes pueden llevar a cabo un protocolo de negociación utilizando esta ontología compartida sin necesidad de estar codificados con los detalles del protocolo de negociación. Mientras tanto, la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1307 Tabla 5: Comparación de métricas de similitud en términos de número de interacciones. Ejecutar Tversky Lin Wu Palmer RP Escenario 1: 1.2 1.2 1 1 Escenario 2: 1.4 1.4 1.6 1.6 Escenario 3: 1.4 1.8 2 2 Escenario 4: 2.2 1 1.2 1.2 Escenario 5: 2 1.6 1.6 1.6 Escenario 6: 5 3.8 2.4 2.6 Escenario 7: 3.2 1.2 1 1 Escenario 8: 5.6 2 2 2.2 Escenario 9: 2.6 2.2 2.2 2.6 Escenario 10: 4.4 2 2 1.8 Promedio de todos los casos: 2.9 1.82 1.7 1.76 Tama et al. modelan el protocolo de negociación utilizando ontologías, en cambio, nosotros hemos modelado el servicio a ser negociado. Además, hemos construido un sistema con el cual se pueden aprender las preferencias de negociación. El estudio de Sadri et al. analiza la negociación en el contexto de la asignación de recursos [14]. Los agentes tienen recursos limitados y necesitan solicitar recursos faltantes a otros agentes. Se propone un mecanismo basado en secuencias de diálogo entre agentes como solución. El mecanismo se basa en el ciclo de agente de observar-pensar-actuar. Estos diálogos incluyen ofrecer recursos, intercambios de recursos y ofrecer recursos alternativos. Cada agente en el sistema planea sus acciones para alcanzar un estado objetivo. A diferencia de nuestro enfoque, el estudio de Sadri et al. no se preocupa por las preferencias de aprendizaje mutuas. Brzostowski y Kowalczyk proponen un enfoque para seleccionar un socio de negociación adecuado investigando negociaciones previas de múltiples atributos [1]. Para lograr esto, utilizan el razonamiento basado en casos. Su enfoque es probabilístico ya que el comportamiento de los socios puede cambiar en cada iteración. En nuestro enfoque, estamos interesados en negociar el contenido del servicio. Después de que el consumidor y el productor acuerden el servicio, se pueden utilizar mecanismos de negociación orientados al precio para acordar el precio. Fatima et al. estudian los factores que afectan la negociación, como las preferencias, el plazo, el precio, entre otros, ya que el agente que desarrolla una estrategia contra su oponente debe considerar todos ellos [5]. En su enfoque, el objetivo del agente vendedor es vender el servicio al precio más alto posible, mientras que el objetivo del agente comprador es comprar el bien al precio más bajo posible. El intervalo de tiempo afecta a estos agentes de manera diferente. En comparación con Fatima et al., nuestro enfoque es diferente. Mientras ellos estudian el efecto del tiempo en la negociación, nuestro enfoque está en aprender las preferencias para una negociación exitosa. Faratin et al. proponen un mecanismo de negociación multi-tema, donde las variables de servicio para la negociación, como el precio, la calidad del servicio, entre otros, se consideran intercambios entre sí (es decir, un precio más alto por una entrega más temprana) [4]. Generan un modelo heurístico para compensaciones que incluye la estimación de similitud difusa y una exploración de escalada de colina para ofertas posiblemente aceptables. Aunque abordamos un problema similar, aprendemos las preferencias del cliente con la ayuda del aprendizaje inductivo y generamos contraofertas de acuerdo con estas preferencias aprendidas. Faratin et al. solo utilizan la última oferta realizada por el consumidor al calcular la similitud para elegir la contraoferta. A diferencia de ellos, también tenemos en cuenta las solicitudes previas del consumidor. En sus experimentos, Faratin et al. asumen que los pesos de las variables de servicio están fijos a priori. Por el contrario, aprendemos estas preferencias con el tiempo. En nuestro trabajo futuro, planeamos integrar el razonamiento ontológico en el algoritmo de aprendizaje para que la información jerárquica pueda ser aprendida a partir de la jerarquía de subsumpción de relaciones. Además, al utilizar las relaciones entre las características, el productor puede descubrir nuevos conocimientos a partir de los conocimientos existentes. Estas son direcciones interesantes que seguiremos en nuestro trabajo futuro. 9. REFERENCIAS [1] J. Brzostowski y R. Kowalczyk. En el razonamiento basado en casos posibilístico para la selección de socios para la negociación de agentes de múltiples atributos. En Actas del 4to Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 273-278, 2005. [2] L. Busch e I. Horstman. Un comentario sobre negociaciones tema por tema. Juegos y Comportamiento Económico, 19:144-148, 1997. [3] J. K. Debenham. Gestión de la negociación en el mercado electrónico en el contexto de un sistema multiagente. En Actas de la 21ª Conferencia Internacional sobre Sistemas Basados en el Conocimiento e Inteligencia Artificial Aplicada, ES2002:, 2002. [4] P. Faratin, C. Sierra y N. R. Jennings. Utilizando criterios de similitud para hacer compensaciones de problemas en negociaciones automatizadas. Inteligencia Artificial, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge y N. Jennings. Agentes óptimos para negociaciones de múltiples temas. En Actas del 2do Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 129-136, 2003. [6] C. Giraud-Carrier. Una nota sobre la utilidad del aprendizaje incremental. Comunicaciones de IA, 13(4):215-223, 2000. [7] T.-P. Hong y S.-S. Tseng. Dividiendo y fusionando espacios de versiones para aprender conceptos disyuntivos. IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.\n\nTraducción al español:\nIEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin. Una definición de similitud basada en teoría de la información. En Actas de la 15ª Conferencia Internacional sobre Aprendizaje Automático, páginas 296-304. Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, y A. G. Moukas. Agentes que compran y venden. Comunicaciones de la ACM, 42(3):81-91, 1999. [10] T. M. Mitchell. Aprendizaje automático. McGraw Hill, NY, 1997. [11] Búho. OWL: Guía del lenguaje de ontologías web, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal y S. C. K. Shiu. Fundamentos del Razonamiento Basado en Casos Blandos. John Wiley & Sons, Nueva Jersey, 2004. [13] J. R. Quinlan. Inducción de árboles de decisión. Aprendizaje automático, 1(1):81-106, 1986. [14] F. Sadri, F. Toni y P. Torroni. Diálogos para negociación: Variedades de agentes y secuencias de diálogo. En ATAL 2001, Artículos Revisados, volumen 2333 de LNAI, páginas 405-421. Springer-Verlag, 2002. [15] M. P. Singh. \n\nSpringer-Verlag, 2002. [15] M. P. Singh. Comercio electrónico orientado al valor. IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, y M. Wooldridge. Ontologías para apoyar la negociación en el comercio electrónico. Aplicaciones de la Inteligencia Artificial en Ingeniería, 18:223-236, 2005. [17] A. Tversky. Características de similitud. Revisión Psicológica, 84(4):327-352, 1977. [18] P. E. Utgoff. Inducción incremental de árboles de decisión. Aprendizaje automático, 4:161-186, 1989. [19] Vino, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu y M. Palmer. Semántica de verbos y selección léxica. En el 32. Reunión anual de la Asociación de Lingüística Computacional, páginas 133-138, 1994. 1308 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "disjunctive cea": {
            "translated_key": "CEA Disyuntivo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 <br>disjunctive cea</br> Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 <br>disjunctive cea</br> Unfortunately, CEA is primarily targeted for conjunctive concepts."
            ],
            "translated_annotated_samples": [
                "Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 <br>CEA Disyuntivo</br> Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor. La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor. Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios. Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las preferencias del consumidor y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta. Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor. Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles. Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor. Este desafío se puede estudiar en tres etapas: • Aprendizaje de preferencias: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5) APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica. Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio. Por ejemplo, un consumidor puede tener preferencia por el vino tinto. Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo. Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo). Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor. Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores. Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa. Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo. Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6]. Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones. Investigamos particularmente dos enfoques. El primero es el aprendizaje inductivo. Esta técnica se aplica para aprender las preferencias como conceptos. Desarrollamos el Algoritmo de Eliminación de Candidatos (CEA) para el Espacio de Versiones [10]. Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva. Curiosamente, la mayoría de las veces las preferencias del consumidor son disyuntivas. Estamos considerando un agente que está comprando vino. El consumidor puede preferir vino tinto o vino rosado pero no vino blanco. Para utilizar CEA con tales preferencias, es necesaria una modificación sólida. El segundo enfoque son los árboles de decisión. Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas. Un árbol de decisión incremental bien conocido es ID5R [18]. Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional. Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados. El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender. El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener. Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto. El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo. Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío. Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo. En cada interacción entre el productor y el consumidor, tanto G como S son modificados. Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas. Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento. Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa. Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí. Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 <br>CEA Disyuntivo</br> Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos. Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos. Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones. Algunos de estos enfoques utilizan múltiples espacios de versión. Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7]. Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir hipótesis disyuntivas además de las conjunciones y la negación. Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica. EJEMPLO 1. Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco). El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor. Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco. En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}. Solo cuando todos los valores existan en la lista, serán reemplazados por ?. En otras palabras, permitimos que el algoritmo generalice más lentamente que antes. Modificamos el algoritmo CEA para hacer frente a este cambio. El algoritmo modificado, DCEA, se presenta como Algoritmo 1. Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de múltiples espacios de versiones. La fase de inicialización es la misma que el algoritmo original (líneas 1, 2). Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4). Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí. Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran. Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7). Cuando llega una muestra negativa, no cambiamos S como antes. Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15). A diferencia del CEA original, intentamos especializar el G mínimamente. El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13). Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada. Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez. Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14). Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar. Por ejemplo, considera el atributo del color. Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2. La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA. El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra. Después de la primera muestra positiva, S se generaliza para cubrir también la instancia. La segunda muestra es negativa. Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada. En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general. La tercera muestra es positiva y generaliza S aún más. Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco). Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis. Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}. Por el Algoritmo 1, no perdemos ninguna información. 3.3 ID3 ID3 [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor. Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos. El algoritmo ID3 se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas. Hay dos clases: positiva y negativa. Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor. Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas. El árbol de decisión tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba. El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio. Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos. Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja. El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender. Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos. Después de cada solicitud entrante, el árbol de decisiones se reconstruye. Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso. Sin embargo, en la práctica hemos evaluado que el ID3 es rápido y el costo de reconstrucción es insignificante. 4. OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA. El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes. Cuando el productor recibe una solicitud del consumidor, el conjunto de aprendizaje del productor se entrena con esta solicitud como una muestra positiva. Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios. El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor. En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor. El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores. Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios. Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios. Esta lista constituye el inventario de servicios de los proveedores. Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace. Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor. Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación. Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S). Calculamos las similitudes de varias maneras, como se explicará en la Sección 5. Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud. El agente productor puede romper el empate de varias maneras. Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de ID3 Si el productor aprende las preferencias de los consumidores con ID3, se aplica un mecanismo similar con dos diferencias. Primero, dado que ID3 no mantiene G, se eliminan de la lista de servicios aquellos no aceptados que se clasifican como negativos. Segundo, las similitudes de los posibles servicios no se miden con respecto a S, sino en cambio a todas las solicitudes previamente realizadas. 4.3 Mecanismos Alternativos de Oferta de Servicios Además de estos tres mecanismos de oferta de servicios (Oferta de Servicio con CEA, Oferta de Servicio con DCEA y Oferta de Servicio con ID3), incluimos otros dos mecanismos. 1304 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) • Oferta de Servicio Aleatoria (RO): El productor genera una contraoferta aleatoriamente de la lista de servicios disponibles, sin considerar las preferencias de los consumidores. • Oferta de Servicio considerando solo la solicitud actual (SCR): El productor selecciona una contraoferta de acuerdo con la similitud de la solicitud actual del consumidor pero no considera solicitudes anteriores. 5. ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una métrica de similitud que toma dos entradas y devuelve qué tan similares son. Existen varios métricos de similitud utilizados en sistemas de razonamiento basado en casos, como la suma ponderada de la distancia euclidiana, la distancia de Hamming, entre otros [12]. La métrica de similitud afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor. Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de similitud semántica llamada Similitud RP. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente. En la Ecuación (1), común representa la cantidad de atributos coincidentes, mientras que diferente representa la cantidad de atributos diferentes. Nuestra suposición actual es que α y β son iguales entre sí. SMpq = α(común) α(común) + β(diferente) (1) Aquí, al comparar dos características, asignamos cero para la disimilitud y uno para la similitud al omitir la cercanía semántica entre los valores de las características. La métrica de similitud de Tversky está diseñada para comparar dos vectores de características. En nuestro sistema, mientras que la lista de servicios que puede ofrecer el productor son cada uno un vector de características, el conjunto más específico S no es un vector de características. S consiste en hipótesis de vectores de características. Por lo tanto, estimamos la similitud de cada hipótesis dentro del conjunto más específico S y luego calculamos el promedio de las similitudes. EJEMPLO 3. Suponga que S contiene las siguientes dos hipótesis: { {Luz, Moderado, (Rojo, Blanco)} , {Completo, Fuerte, Rosa}}. Toma el servicio s como (Ligero, Resistente, Rosa). Entonces, la similitud del primero es igual a 1/3 y la del segundo es igual a 2/3 de acuerdo con la Ecuación (1). Normalmente, tomamos el promedio de ello y obtenemos (1/3 + 2/3)/2, que es igual a 1/2. Sin embargo, la primera hipótesis implica el efecto de dos solicitudes y la segunda hipótesis implica solo una solicitud. Por lo tanto, esperamos que el efecto de la primera hipótesis sea mayor que el de la segunda. Por lo tanto, calculamos la similitud promedio teniendo en cuenta la cantidad de muestras que las hipótesis cubren. Que ch denote el número de muestras que cubre la hipótesis h y (SM(h,servicio)) denote la similitud de la hipótesis h con el servicio dado. Calculamos la similitud de cada hipótesis con el servicio dado y las ponderamos con el número de muestras que cubren. Encontramos la similitud dividiendo la suma ponderada de las similitudes de todas las hipótesis en S con el servicio por el número de todas las muestras que están cubiertas en S. AV G−SM(servicio, S) = |S| |h| (ch ∗ SM(h, servicio)) |S| |h| ch (2) Figura 2: Taxonomía de muestra para estimación de similitud EJEMPLO 4. Para el ejemplo anterior, la similitud de (Luz, Fuerte, Rosa) con el conjunto específico es (2 ∗ 1/3 + 2/3)/3, igual a 4/9. El número posible de muestras que abarca una hipótesis se puede estimar multiplicando las cardinalidades de cada atributo. Por ejemplo, la cardinalidad del primer atributo es dos y la de los demás es igual a uno para la hipótesis dada, como {Luz, Moderado, (Rojo, Blanco)}. Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La métrica de similitud de Lins Un taxonomía puede ser utilizada al estimar la similitud semántica entre dos conceptos. Estimar la similitud semántica en una taxonomía de tipo Es-Un se puede hacer calculando la distancia entre los nodos relacionados con los conceptos comparados. Los enlaces entre los nodos pueden considerarse como distancias. Entonces, la longitud del camino entre los nodos indica qué tan similares son los conceptos. Una estimación alternativa para utilizar el contenido de información en la estimación de la similitud semántica en lugar del método de conteo de aristas, fue propuesta por Lin [8]. La ecuación (3) [8] muestra la similitud de Lin donde c1 y c2 son los conceptos comparados y c0 es el concepto más específico que subsume a ambos. Además, P(C) representa la probabilidad de que un objeto seleccionado arbitrariamente pertenezca al concepto C. La similitud(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Métrica de similitud de Wu y Palmers Diferente de Lin, Wu y Palmer utilizan la distancia entre los nodos en la taxonomía ES-UN [20]. La similitud semántica se representa con la Ecuación (4) [20]. Aquí, se estima la similitud entre c1 y c2 y c0 es el concepto más específico que subsume estas clases. N1 es el número de aristas entre c1 y c0. N2 es el número de aristas entre c2 y c0. N0 es el número de enlaces IS-A de c0 desde la raíz de la taxonomía. Proponemos estimar la distancia relativa en una taxonomía entre dos conceptos utilizando las siguientes intuiciones. Utilizamos la Figura 2 para ilustrar estas intuiciones. • Padre versus abuelo: El padre de un nodo es más similar al nodo que los abuelos de ese. Generalización del Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1305 es un concepto que razonablemente resulta en alejarse más de ese concepto. Cuanto más generales son los conceptos, menos similares son. Por ejemplo, AnyWineColor es el padre de ReddishColor y ReddishColor es el padre de Red. Entonces, esperamos que la similitud entre ReddishColor y Red sea mayor que la similitud entre AnyWineColor y Red. • Padre versus hermano: Un nodo tendría una similitud mayor con su padre que con su hermano. Por ejemplo, Rojo y Rosa son hijos de ColorRojo. En este caso, esperamos que la similitud entre Rojo y ColorRojizo sea mayor que la de Rojo y Rosa. • Hermano versus abuelo: Un nodo es más similar a su hermano que a su abuelo. Para ilustrar, AnyWineColor es el abuelo de Red, y Red y Rose son hermanos. Por lo tanto, posiblemente anticipamos que Rojo y Rosa son más similares que CualquierColorDeVino y Rojo. Como una taxonomía está representada en un árbol, ese árbol puede ser recorrido desde el primer concepto que se está comparando hasta el segundo concepto. En el nodo inicial relacionado con el primer concepto, el valor de similitud es constante y igual a uno. Este valor se reduce por una constante en cada nodo visitado a lo largo del camino que llegará al nodo que incluye el segundo concepto. Cuanto más corto sea el camino entre los conceptos, mayor será la similitud entre los nodos. Algoritmo 2 Estimar-Similitud-RP(c1,c2) Requerido: Las constantes deben ser m > n > m2 donde m, n ∈ R[0, 1] 1: Similitud ← 1 2: si c1 es igual a c2 entonces 3: Devolver Similitud 4: fin si 5: padreComun ← encontrarPadreComun(c1, c2) {padreComun es el concepto más específico que cubre tanto c1 como c2} 6: N1 ← encontrarDistancia(padreComun, c1) 7: N2 ← encontrarDistancia(padreComun, c2) {N1 y N2 son el número de enlaces entre el concepto y el concepto padre} 8: si (padreComun == c1) o (padreComun == c2) entonces 9: Similitud ← Similitud ∗ m(N1+N2) 10: sino 11: Similitud ← Similitud ∗ n ∗ m(N1+N2−2) 12: fin si 13: Devolver Similitud La distancia relativa entre los nodos c1 y c2 se estima de la siguiente manera. Comenzando desde c1, se recorre el árbol para llegar a c2. En cada salto, la similitud disminuye ya que los conceptos se están alejando cada vez más entre sí. Sin embargo, según nuestras intuiciones, no todos los saltos disminuyen la similitud de igual manera. Que m represente el factor para saltar de un hijo a un padre y que n represente el factor para saltar de un hermano a otro hermano. Dado que saltar de un nodo a su abuelo cuenta como dos saltos de padre, el factor de descuento al moverse de un nodo a su abuelo es m2. De acuerdo con las intuiciones anteriores, nuestras constantes deben estar en la forma m > n > m2 donde el valor de m y n debe estar entre cero y uno. El algoritmo 2 muestra el cálculo de la distancia. Según el algoritmo, en primer lugar la similitud se inicializa con el valor de uno (línea 1). Si los conceptos son iguales entre sí, entonces la similitud será uno (líneas 2-4). De lo contrario, calculamos el ancestro común de los dos nodos y la distancia de cada concepto al ancestro común sin considerar al hermano (líneas 5-7). Si uno de los conceptos es igual al padre común, entonces no hay relación de hermanos entre los conceptos. Para cada nivel, multiplicamos la similitud por m y no consideramos el factor de hermanos en la estimación de la similitud. Como resultado, disminuimos la similitud en cada nivel con la tasa de m (línea 9). De lo contrario, tiene que existir una relación de hermanos. Esto significa que debemos considerar el efecto de n al medir la similitud. Recuerde que hemos contado N1+N2 aristas entre los conceptos. Dado que existe una relación de hermanos, dos de estos bordes constituyen la relación de hermanos. Por lo tanto, al calcular el efecto de la relación parental, utilizamos N1+N2 −2 aristas (línea 11). Algunas estimaciones de similitud relacionadas con la taxonomía en la Figura 2 se presentan en la Tabla 2. En este ejemplo, se toma m como 2/3 y n como 4/7. Tabla 2: Estimación de similitud de muestra sobre la taxonomía de muestra. Similitud(ColorRojo, Rosa) = 1 ∗ (2/3) = 0.6666667 Similitud(Rojo, Rosa) = 1 ∗ (4/7) = 0.5714286 Similitud(CualquierColorVino, Rosa) = 1 ∗ (2/3)2 = 0.44444445 Similitud(Blanco, Rosa) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 Para todas las métricas de similitud semántica en nuestra arquitectura, la taxonomía de características se mantiene en la ontología compartida. Para evaluar la similitud del vector de características, primero estimamos la similitud para cada característica individualmente y luego calculamos la suma promedio de estas similitudes. Entonces, el resultado es igual a la similitud semántica promedio de todo el vector de características. 6. SISTEMA DESARROLLADO Hemos implementado nuestra arquitectura en Java. Para facilitar las pruebas del sistema, el agente del consumidor tiene una interfaz de usuario que nos permite ingresar varias solicitudes. El agente productor está completamente automatizado y las operaciones de aprendizaje y oferta de servicios funcionan como se explicó anteriormente. En esta sección, explicamos los detalles de implementación del sistema desarrollado. Utilizamos OWL [11] como nuestro lenguaje de ontología y JENA como nuestro razonador de ontología. La ontología compartida es la versión modificada de la Ontología del Vino [19]. Incluye la descripción del vino como concepto y diferentes tipos de vino. Todos los participantes de la negociación utilizan esta ontología para entenderse mutuamente. Según la ontología, siete propiedades conforman el concepto de vino. El agente consumidor y el agente productor obtienen los valores posibles para estas propiedades consultando la ontología. Por lo tanto, todos los valores posibles para los componentes del concepto del vino, como el color, cuerpo, azúcar, etc., pueden ser alcanzados por ambos agentes. También se describen en esta ontología una variedad de tipos de vino como Borgoña, Chardonnay, Chenin Blanc, entre otros. Intuitivamente, cualquier tipo de vino descrito en la ontología también representa un concepto de vino. Esto nos permite considerar las instancias de vino Chardonnay como instancias de la clase Vino. Además de la descripción del vino, la información jerárquica de algunas características se puede inferir de la ontología. Por ejemplo, podemos representar la información de que el continente europeo abarca países occidentales. El país occidental abarca la región francesa, que incluye algunos territorios como el Loira, Burdeos, entre otros. Esta información jerárquica se utiliza en la estimación de similitud semántica. En esta parte, se pueden hacer algunos razonamientos como si un concepto X abarca Y y Y abarca Z, entonces el concepto X abarca Z. Por ejemplo, el Continente Europeo abarca Burdeos. 1306 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Para algunas características como cuerpo, sabor y azúcar, no hay información jerárquica, pero sus valores están nivelados semánticamente. Cuando eso sucede, proporcionamos los valores de similitud razonables para estas características. Por ejemplo, el cuerpo puede ser ligero, medio o fuerte. En este caso, asumimos que la luz es 0.66 similar a media pero solo 0.33 a fuerte. La ontología de WineStock es el inventario de los productores y describe una clase de producto como WineProduct. Esta clase es necesaria para que el productor registre los vinos que vende. La ontología implica a los individuos de esta clase. Los individuos representan los servicios disponibles que posee el productor. Hemos preparado dos ontologías de WineStock separadas para realizar pruebas. En la primera ontología, hay 19 productos de vino disponibles y en la segunda ontología, hay 50 productos. EVALUACIÓN DEL RENDIMIENTO Evaluamos el rendimiento de los sistemas propuestos en relación con la técnica de aprendizaje que utilizaron, DCEA e ID3, comparándolos con CEA, RO (para oferta aleatoria) y SCR (oferta basada solo en la solicitud actual). Aplicamos una variedad de escenarios en este conjunto de datos para ver las diferencias de rendimiento. Cada escenario de prueba contiene una lista de preferencias para el usuario y el número de coincidencias de la lista de productos. La Tabla 3 muestra estas preferencias y la disponibilidad de esos productos en el inventario para los primeros cinco escenarios. Ten en cuenta que estas preferencias son internas al consumidor y el productor intenta aprenderlas durante la negociación. Tabla 3: Disponibilidad de vinos en diferentes escenarios de prueba ID Preferencia del consumidor Disponibilidad (de 19) 1 Vino seco 15 2 Vino tinto y seco 8 3 Vino tinto, seco y moderado 4 4 Vino tinto y fuerte 2 5 Vino tinto o rosado, y fuerte 3 7.1 Comparación de Algoritmos de Aprendizaje En la comparación de algoritmos de aprendizaje, utilizamos los cinco escenarios de la Tabla 3. Aquí, primero usamos la medida de similitud de Tversky. Con estos casos de prueba, estamos interesados en encontrar el número de iteraciones que se requieren para que el productor genere una oferta aceptable para el consumidor. Dado que el rendimiento también depende de la solicitud inicial, repetimos nuestros experimentos con diferentes solicitudes iniciales. Por consiguiente, para cada caso, ejecutamos los algoritmos cinco veces con varias variaciones de las solicitudes iniciales. En cada experimento, contamos el número de iteraciones necesarias para llegar a un acuerdo. Tomamos el promedio de estos números para evaluar estos sistemas de manera justa. Como es costumbre, probamos cada algoritmo con las mismas solicitudes iniciales. La Tabla 4 compara los enfoques utilizando diferentes algoritmos de aprendizaje. Cuando las partes grandes del inventario son compatibles con las preferencias de los clientes, como en el primer caso de prueba, el rendimiento de todas las técnicas es casi el mismo (por ejemplo, Escenario 1). A medida que el número de servicios compatibles disminuye, RO funciona mal como se esperaba. El segundo peor método es SCR ya que solo considera la solicitud más reciente de los clientes y no aprende de las solicitudes anteriores. CEA da los mejores resultados cuando puede generar una respuesta pero no puede manejar los casos que contienen preferencias disyuntivas, como el que se presenta en el Escenario 5. ID3 y DCEA logran los mejores resultados. Su rendimiento es comparable y pueden manejar todos los casos, incluido el Escenario 5. Tabla 4: Comparación de algoritmos de aprendizaje en términos del número promedio de interacciones. Ejecutar DCEA SCR RO CEA ID3 Escenario 1: 1.2 1.4 1.2 1.2 1.2 Escenario 2: 1.4 1.4 2.6 1.4 1.4 Escenario 3: 1.4 1.8 4.4 1.4 1.4 Escenario 4: 2.2 2.8 9.6 1.8 2 Escenario 5: 2 2.6 7.6 1.75+ Sin oferta 1.8 Promedio de todos los casos: 1.64 2 5.08 1.51+Sin oferta 1.56 7.2 Comparación de Métricas de Similitud Para comparar las métricas de similitud que se explicaron en la Sección 5, fijamos el algoritmo de aprendizaje en DCEA. Además de los escenarios mostrados en la Tabla 3, agregamos los siguientes cinco nuevos escenarios considerando la información jerárquica. • El cliente desea comprar vino cuya bodega esté ubicada en California y cuya uva sea de tipo blanco. Además, la bodega del vino no debería ser costosa. Solo hay cuatro productos que cumplen con estas condiciones. • El cliente quiere comprar vino de color rojo o rosado y de tipo de uva tinta. Además, la ubicación del vino debe ser en Europa. Se desea que el grado de dulzura sea seco o semiseco. El sabor debe ser delicado o moderado, mientras que el cuerpo debe ser medio o ligero. Además, la bodega del vino debería ser una bodega cara. Hay dos productos que cumplen con todos estos requisitos. El cliente quiere comprar vino rosado moderado, que se encuentra alrededor de la región francesa. La categoría de bodega debería ser Bodega Moderada. Solo hay un producto que cumple con estos requisitos. • El cliente quiere comprar vino tinto caro, que se encuentra alrededor de la Región de California o vino blanco barato, que se encuentra alrededor de la Región de Texas. Hay cinco productos disponibles. • El cliente quiere comprar un vino blanco delicado cuyo productor esté en la categoría de Bodega Costosa. Hay dos productos disponibles. Los primeros siete escenarios se prueban con el primer conjunto de datos que contiene un total de 19 servicios y los últimos tres escenarios se prueban con el segundo conjunto de datos que contiene 50 servicios. La Tabla 5 muestra la evaluación del rendimiento en términos del número de interacciones necesarias para llegar a un consenso. La métrica de Tversky da los peores resultados ya que no considera la similitud semántica. El rendimiento de Lins es mejor que el de Tversky pero peor que el de otros. La métrica de Wu-Palmer y la medida de similitud de RP casi ofrecen el mismo rendimiento y son mejores que otras. Cuando se examinan los resultados, considerar la cercanía semántica aumenta el rendimiento. 8. DISCUSIÓN Revisamos la literatura reciente en comparación con nuestro trabajo. Tama et al. [16] proponen un nuevo enfoque basado en ontología para la negociación. Según su enfoque, los protocolos de negociación utilizados en el comercio electrónico pueden ser modelados como ontologías. Por lo tanto, los agentes pueden llevar a cabo un protocolo de negociación utilizando esta ontología compartida sin necesidad de estar codificados con los detalles del protocolo de negociación. Mientras tanto, la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1307 Tabla 5: Comparación de métricas de similitud en términos de número de interacciones. Ejecutar Tversky Lin Wu Palmer RP Escenario 1: 1.2 1.2 1 1 Escenario 2: 1.4 1.4 1.6 1.6 Escenario 3: 1.4 1.8 2 2 Escenario 4: 2.2 1 1.2 1.2 Escenario 5: 2 1.6 1.6 1.6 Escenario 6: 5 3.8 2.4 2.6 Escenario 7: 3.2 1.2 1 1 Escenario 8: 5.6 2 2 2.2 Escenario 9: 2.6 2.2 2.2 2.6 Escenario 10: 4.4 2 2 1.8 Promedio de todos los casos: 2.9 1.82 1.7 1.76 Tama et al. modelan el protocolo de negociación utilizando ontologías, en cambio, nosotros hemos modelado el servicio a ser negociado. Además, hemos construido un sistema con el cual se pueden aprender las preferencias de negociación. El estudio de Sadri et al. analiza la negociación en el contexto de la asignación de recursos [14]. Los agentes tienen recursos limitados y necesitan solicitar recursos faltantes a otros agentes. Se propone un mecanismo basado en secuencias de diálogo entre agentes como solución. El mecanismo se basa en el ciclo de agente de observar-pensar-actuar. Estos diálogos incluyen ofrecer recursos, intercambios de recursos y ofrecer recursos alternativos. Cada agente en el sistema planea sus acciones para alcanzar un estado objetivo. A diferencia de nuestro enfoque, el estudio de Sadri et al. no se preocupa por las preferencias de aprendizaje mutuas. Brzostowski y Kowalczyk proponen un enfoque para seleccionar un socio de negociación adecuado investigando negociaciones previas de múltiples atributos [1]. Para lograr esto, utilizan el razonamiento basado en casos. Su enfoque es probabilístico ya que el comportamiento de los socios puede cambiar en cada iteración. En nuestro enfoque, estamos interesados en negociar el contenido del servicio. Después de que el consumidor y el productor acuerden el servicio, se pueden utilizar mecanismos de negociación orientados al precio para acordar el precio. Fatima et al. estudian los factores que afectan la negociación, como las preferencias, el plazo, el precio, entre otros, ya que el agente que desarrolla una estrategia contra su oponente debe considerar todos ellos [5]. En su enfoque, el objetivo del agente vendedor es vender el servicio al precio más alto posible, mientras que el objetivo del agente comprador es comprar el bien al precio más bajo posible. El intervalo de tiempo afecta a estos agentes de manera diferente. En comparación con Fatima et al., nuestro enfoque es diferente. Mientras ellos estudian el efecto del tiempo en la negociación, nuestro enfoque está en aprender las preferencias para una negociación exitosa. Faratin et al. proponen un mecanismo de negociación multi-tema, donde las variables de servicio para la negociación, como el precio, la calidad del servicio, entre otros, se consideran intercambios entre sí (es decir, un precio más alto por una entrega más temprana) [4]. Generan un modelo heurístico para compensaciones que incluye la estimación de similitud difusa y una exploración de escalada de colina para ofertas posiblemente aceptables. Aunque abordamos un problema similar, aprendemos las preferencias del cliente con la ayuda del aprendizaje inductivo y generamos contraofertas de acuerdo con estas preferencias aprendidas. Faratin et al. solo utilizan la última oferta realizada por el consumidor al calcular la similitud para elegir la contraoferta. A diferencia de ellos, también tenemos en cuenta las solicitudes previas del consumidor. En sus experimentos, Faratin et al. asumen que los pesos de las variables de servicio están fijos a priori. Por el contrario, aprendemos estas preferencias con el tiempo. En nuestro trabajo futuro, planeamos integrar el razonamiento ontológico en el algoritmo de aprendizaje para que la información jerárquica pueda ser aprendida a partir de la jerarquía de subsumpción de relaciones. Además, al utilizar las relaciones entre las características, el productor puede descubrir nuevos conocimientos a partir de los conocimientos existentes. Estas son direcciones interesantes que seguiremos en nuestro trabajo futuro. 9. REFERENCIAS [1] J. Brzostowski y R. Kowalczyk. En el razonamiento basado en casos posibilístico para la selección de socios para la negociación de agentes de múltiples atributos. En Actas del 4to Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 273-278, 2005. [2] L. Busch e I. Horstman. Un comentario sobre negociaciones tema por tema. Juegos y Comportamiento Económico, 19:144-148, 1997. [3] J. K. Debenham. Gestión de la negociación en el mercado electrónico en el contexto de un sistema multiagente. En Actas de la 21ª Conferencia Internacional sobre Sistemas Basados en el Conocimiento e Inteligencia Artificial Aplicada, ES2002:, 2002. [4] P. Faratin, C. Sierra y N. R. Jennings. Utilizando criterios de similitud para hacer compensaciones de problemas en negociaciones automatizadas. Inteligencia Artificial, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge y N. Jennings. Agentes óptimos para negociaciones de múltiples temas. En Actas del 2do Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 129-136, 2003. [6] C. Giraud-Carrier. Una nota sobre la utilidad del aprendizaje incremental. Comunicaciones de IA, 13(4):215-223, 2000. [7] T.-P. Hong y S.-S. Tseng. Dividiendo y fusionando espacios de versiones para aprender conceptos disyuntivos. IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.\n\nTraducción al español:\nIEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin. Una definición de similitud basada en teoría de la información. En Actas de la 15ª Conferencia Internacional sobre Aprendizaje Automático, páginas 296-304. Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, y A. G. Moukas. Agentes que compran y venden. Comunicaciones de la ACM, 42(3):81-91, 1999. [10] T. M. Mitchell. Aprendizaje automático. McGraw Hill, NY, 1997. [11] Búho. OWL: Guía del lenguaje de ontologías web, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal y S. C. K. Shiu. Fundamentos del Razonamiento Basado en Casos Blandos. John Wiley & Sons, Nueva Jersey, 2004. [13] J. R. Quinlan. Inducción de árboles de decisión. Aprendizaje automático, 1(1):81-106, 1986. [14] F. Sadri, F. Toni y P. Torroni. Diálogos para negociación: Variedades de agentes y secuencias de diálogo. En ATAL 2001, Artículos Revisados, volumen 2333 de LNAI, páginas 405-421. Springer-Verlag, 2002. [15] M. P. Singh. \n\nSpringer-Verlag, 2002. [15] M. P. Singh. Comercio electrónico orientado al valor. IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, y M. Wooldridge. Ontologías para apoyar la negociación en el comercio electrónico. Aplicaciones de la Inteligencia Artificial en Ingeniería, 18:223-236, 2005. [17] A. Tversky. Características de similitud. Revisión Psicológica, 84(4):327-352, 1977. [18] P. E. Utgoff. Inducción incremental de árboles de decisión. Aprendizaje automático, 4:161-186, 1989. [19] Vino, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu y M. Palmer. Semántica de verbos y selección léxica. En el 32. Reunión anual de la Asociación de Lingüística Computacional, páginas 133-138, 1994. 1308 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "multiple version space": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use <br>multiple version space</br>.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than <br>multiple version space</br>.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Some of these approaches use <br>multiple version space</br>.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than <br>multiple version space</br>."
            ],
            "translated_annotated_samples": [
                "Algunos de estos enfoques utilizan <br>múltiples espacios de versión</br>.",
                "Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de <br>múltiples espacios de versiones</br>."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor. La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor. Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios. Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las preferencias del consumidor y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta. Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor. Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles. Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor. Este desafío se puede estudiar en tres etapas: • Aprendizaje de preferencias: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5) APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica. Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio. Por ejemplo, un consumidor puede tener preferencia por el vino tinto. Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo. Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo). Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor. Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores. Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa. Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo. Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6]. Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones. Investigamos particularmente dos enfoques. El primero es el aprendizaje inductivo. Esta técnica se aplica para aprender las preferencias como conceptos. Desarrollamos el Algoritmo de Eliminación de Candidatos (CEA) para el Espacio de Versiones [10]. Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva. Curiosamente, la mayoría de las veces las preferencias del consumidor son disyuntivas. Estamos considerando un agente que está comprando vino. El consumidor puede preferir vino tinto o vino rosado pero no vino blanco. Para utilizar CEA con tales preferencias, es necesaria una modificación sólida. El segundo enfoque son los árboles de decisión. Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas. Un árbol de decisión incremental bien conocido es ID5R [18]. Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional. Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados. El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender. El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener. Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto. El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo. Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío. Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo. En cada interacción entre el productor y el consumidor, tanto G como S son modificados. Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas. Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento. Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa. Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí. Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 CEA Disyuntivo Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos. Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos. Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones. Algunos de estos enfoques utilizan <br>múltiples espacios de versión</br>. Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7]. Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir hipótesis disyuntivas además de las conjunciones y la negación. Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica. EJEMPLO 1. Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco). El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor. Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco. En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}. Solo cuando todos los valores existan en la lista, serán reemplazados por ?. En otras palabras, permitimos que el algoritmo generalice más lentamente que antes. Modificamos el algoritmo CEA para hacer frente a este cambio. El algoritmo modificado, DCEA, se presenta como Algoritmo 1. Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de <br>múltiples espacios de versiones</br>. La fase de inicialización es la misma que el algoritmo original (líneas 1, 2). Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4). Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí. Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran. Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7). Cuando llega una muestra negativa, no cambiamos S como antes. Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15). A diferencia del CEA original, intentamos especializar el G mínimamente. El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13). Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada. Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez. Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14). Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar. Por ejemplo, considera el atributo del color. Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2. La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA. El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra. Después de la primera muestra positiva, S se generaliza para cubrir también la instancia. La segunda muestra es negativa. Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada. En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general. La tercera muestra es positiva y generaliza S aún más. Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco). Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis. Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}. Por el Algoritmo 1, no perdemos ninguna información. 3.3 ID3 ID3 [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor. Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos. El algoritmo ID3 se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas. Hay dos clases: positiva y negativa. Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor. Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas. El árbol de decisión tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba. El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio. Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos. Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja. El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender. Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos. Después de cada solicitud entrante, el árbol de decisiones se reconstruye. Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso. Sin embargo, en la práctica hemos evaluado que el ID3 es rápido y el costo de reconstrucción es insignificante. 4. OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA. El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes. Cuando el productor recibe una solicitud del consumidor, el conjunto de aprendizaje del productor se entrena con esta solicitud como una muestra positiva. Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios. El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor. En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor. El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores. Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios. Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios. Esta lista constituye el inventario de servicios de los proveedores. Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace. Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor. Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación. Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S). Calculamos las similitudes de varias maneras, como se explicará en la Sección 5. Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud. El agente productor puede romper el empate de varias maneras. Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de ID3 Si el productor aprende las preferencias de los consumidores con ID3, se aplica un mecanismo similar con dos diferencias. Primero, dado que ID3 no mantiene G, se eliminan de la lista de servicios aquellos no aceptados que se clasifican como negativos. Segundo, las similitudes de los posibles servicios no se miden con respecto a S, sino en cambio a todas las solicitudes previamente realizadas. 4.3 Mecanismos Alternativos de Oferta de Servicios Además de estos tres mecanismos de oferta de servicios (Oferta de Servicio con CEA, Oferta de Servicio con DCEA y Oferta de Servicio con ID3), incluimos otros dos mecanismos. 1304 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) • Oferta de Servicio Aleatoria (RO): El productor genera una contraoferta aleatoriamente de la lista de servicios disponibles, sin considerar las preferencias de los consumidores. • Oferta de Servicio considerando solo la solicitud actual (SCR): El productor selecciona una contraoferta de acuerdo con la similitud de la solicitud actual del consumidor pero no considera solicitudes anteriores. 5. ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una métrica de similitud que toma dos entradas y devuelve qué tan similares son. Existen varios métricos de similitud utilizados en sistemas de razonamiento basado en casos, como la suma ponderada de la distancia euclidiana, la distancia de Hamming, entre otros [12]. La métrica de similitud afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor. Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de similitud semántica llamada Similitud RP. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente. En la Ecuación (1), común representa la cantidad de atributos coincidentes, mientras que diferente representa la cantidad de atributos diferentes. Nuestra suposición actual es que α y β son iguales entre sí. SMpq = α(común) α(común) + β(diferente) (1) Aquí, al comparar dos características, asignamos cero para la disimilitud y uno para la similitud al omitir la cercanía semántica entre los valores de las características. La métrica de similitud de Tversky está diseñada para comparar dos vectores de características. En nuestro sistema, mientras que la lista de servicios que puede ofrecer el productor son cada uno un vector de características, el conjunto más específico S no es un vector de características. S consiste en hipótesis de vectores de características. Por lo tanto, estimamos la similitud de cada hipótesis dentro del conjunto más específico S y luego calculamos el promedio de las similitudes. EJEMPLO 3. Suponga que S contiene las siguientes dos hipótesis: { {Luz, Moderado, (Rojo, Blanco)} , {Completo, Fuerte, Rosa}}. Toma el servicio s como (Ligero, Resistente, Rosa). Entonces, la similitud del primero es igual a 1/3 y la del segundo es igual a 2/3 de acuerdo con la Ecuación (1). Normalmente, tomamos el promedio de ello y obtenemos (1/3 + 2/3)/2, que es igual a 1/2. Sin embargo, la primera hipótesis implica el efecto de dos solicitudes y la segunda hipótesis implica solo una solicitud. Por lo tanto, esperamos que el efecto de la primera hipótesis sea mayor que el de la segunda. Por lo tanto, calculamos la similitud promedio teniendo en cuenta la cantidad de muestras que las hipótesis cubren. Que ch denote el número de muestras que cubre la hipótesis h y (SM(h,servicio)) denote la similitud de la hipótesis h con el servicio dado. Calculamos la similitud de cada hipótesis con el servicio dado y las ponderamos con el número de muestras que cubren. Encontramos la similitud dividiendo la suma ponderada de las similitudes de todas las hipótesis en S con el servicio por el número de todas las muestras que están cubiertas en S. AV G−SM(servicio, S) = |S| |h| (ch ∗ SM(h, servicio)) |S| |h| ch (2) Figura 2: Taxonomía de muestra para estimación de similitud EJEMPLO 4. Para el ejemplo anterior, la similitud de (Luz, Fuerte, Rosa) con el conjunto específico es (2 ∗ 1/3 + 2/3)/3, igual a 4/9. El número posible de muestras que abarca una hipótesis se puede estimar multiplicando las cardinalidades de cada atributo. Por ejemplo, la cardinalidad del primer atributo es dos y la de los demás es igual a uno para la hipótesis dada, como {Luz, Moderado, (Rojo, Blanco)}. Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La métrica de similitud de Lins Un taxonomía puede ser utilizada al estimar la similitud semántica entre dos conceptos. Estimar la similitud semántica en una taxonomía de tipo Es-Un se puede hacer calculando la distancia entre los nodos relacionados con los conceptos comparados. Los enlaces entre los nodos pueden considerarse como distancias. Entonces, la longitud del camino entre los nodos indica qué tan similares son los conceptos. Una estimación alternativa para utilizar el contenido de información en la estimación de la similitud semántica en lugar del método de conteo de aristas, fue propuesta por Lin [8]. La ecuación (3) [8] muestra la similitud de Lin donde c1 y c2 son los conceptos comparados y c0 es el concepto más específico que subsume a ambos. Además, P(C) representa la probabilidad de que un objeto seleccionado arbitrariamente pertenezca al concepto C. La similitud(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Métrica de similitud de Wu y Palmers Diferente de Lin, Wu y Palmer utilizan la distancia entre los nodos en la taxonomía ES-UN [20]. La similitud semántica se representa con la Ecuación (4) [20]. Aquí, se estima la similitud entre c1 y c2 y c0 es el concepto más específico que subsume estas clases. N1 es el número de aristas entre c1 y c0. N2 es el número de aristas entre c2 y c0. N0 es el número de enlaces IS-A de c0 desde la raíz de la taxonomía. Proponemos estimar la distancia relativa en una taxonomía entre dos conceptos utilizando las siguientes intuiciones. Utilizamos la Figura 2 para ilustrar estas intuiciones. • Padre versus abuelo: El padre de un nodo es más similar al nodo que los abuelos de ese. Generalización del Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1305 es un concepto que razonablemente resulta en alejarse más de ese concepto. Cuanto más generales son los conceptos, menos similares son. Por ejemplo, AnyWineColor es el padre de ReddishColor y ReddishColor es el padre de Red. Entonces, esperamos que la similitud entre ReddishColor y Red sea mayor que la similitud entre AnyWineColor y Red. • Padre versus hermano: Un nodo tendría una similitud mayor con su padre que con su hermano. Por ejemplo, Rojo y Rosa son hijos de ColorRojo. En este caso, esperamos que la similitud entre Rojo y ColorRojizo sea mayor que la de Rojo y Rosa. • Hermano versus abuelo: Un nodo es más similar a su hermano que a su abuelo. Para ilustrar, AnyWineColor es el abuelo de Red, y Red y Rose son hermanos. Por lo tanto, posiblemente anticipamos que Rojo y Rosa son más similares que CualquierColorDeVino y Rojo. Como una taxonomía está representada en un árbol, ese árbol puede ser recorrido desde el primer concepto que se está comparando hasta el segundo concepto. En el nodo inicial relacionado con el primer concepto, el valor de similitud es constante y igual a uno. Este valor se reduce por una constante en cada nodo visitado a lo largo del camino que llegará al nodo que incluye el segundo concepto. Cuanto más corto sea el camino entre los conceptos, mayor será la similitud entre los nodos. Algoritmo 2 Estimar-Similitud-RP(c1,c2) Requerido: Las constantes deben ser m > n > m2 donde m, n ∈ R[0, 1] 1: Similitud ← 1 2: si c1 es igual a c2 entonces 3: Devolver Similitud 4: fin si 5: padreComun ← encontrarPadreComun(c1, c2) {padreComun es el concepto más específico que cubre tanto c1 como c2} 6: N1 ← encontrarDistancia(padreComun, c1) 7: N2 ← encontrarDistancia(padreComun, c2) {N1 y N2 son el número de enlaces entre el concepto y el concepto padre} 8: si (padreComun == c1) o (padreComun == c2) entonces 9: Similitud ← Similitud ∗ m(N1+N2) 10: sino 11: Similitud ← Similitud ∗ n ∗ m(N1+N2−2) 12: fin si 13: Devolver Similitud La distancia relativa entre los nodos c1 y c2 se estima de la siguiente manera. Comenzando desde c1, se recorre el árbol para llegar a c2. En cada salto, la similitud disminuye ya que los conceptos se están alejando cada vez más entre sí. Sin embargo, según nuestras intuiciones, no todos los saltos disminuyen la similitud de igual manera. Que m represente el factor para saltar de un hijo a un padre y que n represente el factor para saltar de un hermano a otro hermano. Dado que saltar de un nodo a su abuelo cuenta como dos saltos de padre, el factor de descuento al moverse de un nodo a su abuelo es m2. De acuerdo con las intuiciones anteriores, nuestras constantes deben estar en la forma m > n > m2 donde el valor de m y n debe estar entre cero y uno. El algoritmo 2 muestra el cálculo de la distancia. Según el algoritmo, en primer lugar la similitud se inicializa con el valor de uno (línea 1). Si los conceptos son iguales entre sí, entonces la similitud será uno (líneas 2-4). De lo contrario, calculamos el ancestro común de los dos nodos y la distancia de cada concepto al ancestro común sin considerar al hermano (líneas 5-7). Si uno de los conceptos es igual al padre común, entonces no hay relación de hermanos entre los conceptos. Para cada nivel, multiplicamos la similitud por m y no consideramos el factor de hermanos en la estimación de la similitud. Como resultado, disminuimos la similitud en cada nivel con la tasa de m (línea 9). De lo contrario, tiene que existir una relación de hermanos. Esto significa que debemos considerar el efecto de n al medir la similitud. Recuerde que hemos contado N1+N2 aristas entre los conceptos. Dado que existe una relación de hermanos, dos de estos bordes constituyen la relación de hermanos. Por lo tanto, al calcular el efecto de la relación parental, utilizamos N1+N2 −2 aristas (línea 11). Algunas estimaciones de similitud relacionadas con la taxonomía en la Figura 2 se presentan en la Tabla 2. En este ejemplo, se toma m como 2/3 y n como 4/7. Tabla 2: Estimación de similitud de muestra sobre la taxonomía de muestra. Similitud(ColorRojo, Rosa) = 1 ∗ (2/3) = 0.6666667 Similitud(Rojo, Rosa) = 1 ∗ (4/7) = 0.5714286 Similitud(CualquierColorVino, Rosa) = 1 ∗ (2/3)2 = 0.44444445 Similitud(Blanco, Rosa) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 Para todas las métricas de similitud semántica en nuestra arquitectura, la taxonomía de características se mantiene en la ontología compartida. Para evaluar la similitud del vector de características, primero estimamos la similitud para cada característica individualmente y luego calculamos la suma promedio de estas similitudes. Entonces, el resultado es igual a la similitud semántica promedio de todo el vector de características. 6. SISTEMA DESARROLLADO Hemos implementado nuestra arquitectura en Java. Para facilitar las pruebas del sistema, el agente del consumidor tiene una interfaz de usuario que nos permite ingresar varias solicitudes. El agente productor está completamente automatizado y las operaciones de aprendizaje y oferta de servicios funcionan como se explicó anteriormente. En esta sección, explicamos los detalles de implementación del sistema desarrollado. Utilizamos OWL [11] como nuestro lenguaje de ontología y JENA como nuestro razonador de ontología. La ontología compartida es la versión modificada de la Ontología del Vino [19]. Incluye la descripción del vino como concepto y diferentes tipos de vino. Todos los participantes de la negociación utilizan esta ontología para entenderse mutuamente. Según la ontología, siete propiedades conforman el concepto de vino. El agente consumidor y el agente productor obtienen los valores posibles para estas propiedades consultando la ontología. Por lo tanto, todos los valores posibles para los componentes del concepto del vino, como el color, cuerpo, azúcar, etc., pueden ser alcanzados por ambos agentes. También se describen en esta ontología una variedad de tipos de vino como Borgoña, Chardonnay, Chenin Blanc, entre otros. Intuitivamente, cualquier tipo de vino descrito en la ontología también representa un concepto de vino. Esto nos permite considerar las instancias de vino Chardonnay como instancias de la clase Vino. Además de la descripción del vino, la información jerárquica de algunas características se puede inferir de la ontología. Por ejemplo, podemos representar la información de que el continente europeo abarca países occidentales. El país occidental abarca la región francesa, que incluye algunos territorios como el Loira, Burdeos, entre otros. Esta información jerárquica se utiliza en la estimación de similitud semántica. En esta parte, se pueden hacer algunos razonamientos como si un concepto X abarca Y y Y abarca Z, entonces el concepto X abarca Z. Por ejemplo, el Continente Europeo abarca Burdeos. 1306 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Para algunas características como cuerpo, sabor y azúcar, no hay información jerárquica, pero sus valores están nivelados semánticamente. Cuando eso sucede, proporcionamos los valores de similitud razonables para estas características. Por ejemplo, el cuerpo puede ser ligero, medio o fuerte. En este caso, asumimos que la luz es 0.66 similar a media pero solo 0.33 a fuerte. La ontología de WineStock es el inventario de los productores y describe una clase de producto como WineProduct. Esta clase es necesaria para que el productor registre los vinos que vende. La ontología implica a los individuos de esta clase. Los individuos representan los servicios disponibles que posee el productor. Hemos preparado dos ontologías de WineStock separadas para realizar pruebas. En la primera ontología, hay 19 productos de vino disponibles y en la segunda ontología, hay 50 productos. EVALUACIÓN DEL RENDIMIENTO Evaluamos el rendimiento de los sistemas propuestos en relación con la técnica de aprendizaje que utilizaron, DCEA e ID3, comparándolos con CEA, RO (para oferta aleatoria) y SCR (oferta basada solo en la solicitud actual). Aplicamos una variedad de escenarios en este conjunto de datos para ver las diferencias de rendimiento. Cada escenario de prueba contiene una lista de preferencias para el usuario y el número de coincidencias de la lista de productos. La Tabla 3 muestra estas preferencias y la disponibilidad de esos productos en el inventario para los primeros cinco escenarios. Ten en cuenta que estas preferencias son internas al consumidor y el productor intenta aprenderlas durante la negociación. Tabla 3: Disponibilidad de vinos en diferentes escenarios de prueba ID Preferencia del consumidor Disponibilidad (de 19) 1 Vino seco 15 2 Vino tinto y seco 8 3 Vino tinto, seco y moderado 4 4 Vino tinto y fuerte 2 5 Vino tinto o rosado, y fuerte 3 7.1 Comparación de Algoritmos de Aprendizaje En la comparación de algoritmos de aprendizaje, utilizamos los cinco escenarios de la Tabla 3. Aquí, primero usamos la medida de similitud de Tversky. Con estos casos de prueba, estamos interesados en encontrar el número de iteraciones que se requieren para que el productor genere una oferta aceptable para el consumidor. Dado que el rendimiento también depende de la solicitud inicial, repetimos nuestros experimentos con diferentes solicitudes iniciales. Por consiguiente, para cada caso, ejecutamos los algoritmos cinco veces con varias variaciones de las solicitudes iniciales. En cada experimento, contamos el número de iteraciones necesarias para llegar a un acuerdo. Tomamos el promedio de estos números para evaluar estos sistemas de manera justa. Como es costumbre, probamos cada algoritmo con las mismas solicitudes iniciales. La Tabla 4 compara los enfoques utilizando diferentes algoritmos de aprendizaje. Cuando las partes grandes del inventario son compatibles con las preferencias de los clientes, como en el primer caso de prueba, el rendimiento de todas las técnicas es casi el mismo (por ejemplo, Escenario 1). A medida que el número de servicios compatibles disminuye, RO funciona mal como se esperaba. El segundo peor método es SCR ya que solo considera la solicitud más reciente de los clientes y no aprende de las solicitudes anteriores. CEA da los mejores resultados cuando puede generar una respuesta pero no puede manejar los casos que contienen preferencias disyuntivas, como el que se presenta en el Escenario 5. ID3 y DCEA logran los mejores resultados. Su rendimiento es comparable y pueden manejar todos los casos, incluido el Escenario 5. Tabla 4: Comparación de algoritmos de aprendizaje en términos del número promedio de interacciones. Ejecutar DCEA SCR RO CEA ID3 Escenario 1: 1.2 1.4 1.2 1.2 1.2 Escenario 2: 1.4 1.4 2.6 1.4 1.4 Escenario 3: 1.4 1.8 4.4 1.4 1.4 Escenario 4: 2.2 2.8 9.6 1.8 2 Escenario 5: 2 2.6 7.6 1.75+ Sin oferta 1.8 Promedio de todos los casos: 1.64 2 5.08 1.51+Sin oferta 1.56 7.2 Comparación de Métricas de Similitud Para comparar las métricas de similitud que se explicaron en la Sección 5, fijamos el algoritmo de aprendizaje en DCEA. Además de los escenarios mostrados en la Tabla 3, agregamos los siguientes cinco nuevos escenarios considerando la información jerárquica. • El cliente desea comprar vino cuya bodega esté ubicada en California y cuya uva sea de tipo blanco. Además, la bodega del vino no debería ser costosa. Solo hay cuatro productos que cumplen con estas condiciones. • El cliente quiere comprar vino de color rojo o rosado y de tipo de uva tinta. Además, la ubicación del vino debe ser en Europa. Se desea que el grado de dulzura sea seco o semiseco. El sabor debe ser delicado o moderado, mientras que el cuerpo debe ser medio o ligero. Además, la bodega del vino debería ser una bodega cara. Hay dos productos que cumplen con todos estos requisitos. El cliente quiere comprar vino rosado moderado, que se encuentra alrededor de la región francesa. La categoría de bodega debería ser Bodega Moderada. Solo hay un producto que cumple con estos requisitos. • El cliente quiere comprar vino tinto caro, que se encuentra alrededor de la Región de California o vino blanco barato, que se encuentra alrededor de la Región de Texas. Hay cinco productos disponibles. • El cliente quiere comprar un vino blanco delicado cuyo productor esté en la categoría de Bodega Costosa. Hay dos productos disponibles. Los primeros siete escenarios se prueban con el primer conjunto de datos que contiene un total de 19 servicios y los últimos tres escenarios se prueban con el segundo conjunto de datos que contiene 50 servicios. La Tabla 5 muestra la evaluación del rendimiento en términos del número de interacciones necesarias para llegar a un consenso. La métrica de Tversky da los peores resultados ya que no considera la similitud semántica. El rendimiento de Lins es mejor que el de Tversky pero peor que el de otros. La métrica de Wu-Palmer y la medida de similitud de RP casi ofrecen el mismo rendimiento y son mejores que otras. Cuando se examinan los resultados, considerar la cercanía semántica aumenta el rendimiento. 8. DISCUSIÓN Revisamos la literatura reciente en comparación con nuestro trabajo. Tama et al. [16] proponen un nuevo enfoque basado en ontología para la negociación. Según su enfoque, los protocolos de negociación utilizados en el comercio electrónico pueden ser modelados como ontologías. Por lo tanto, los agentes pueden llevar a cabo un protocolo de negociación utilizando esta ontología compartida sin necesidad de estar codificados con los detalles del protocolo de negociación. Mientras tanto, la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1307 Tabla 5: Comparación de métricas de similitud en términos de número de interacciones. Ejecutar Tversky Lin Wu Palmer RP Escenario 1: 1.2 1.2 1 1 Escenario 2: 1.4 1.4 1.6 1.6 Escenario 3: 1.4 1.8 2 2 Escenario 4: 2.2 1 1.2 1.2 Escenario 5: 2 1.6 1.6 1.6 Escenario 6: 5 3.8 2.4 2.6 Escenario 7: 3.2 1.2 1 1 Escenario 8: 5.6 2 2 2.2 Escenario 9: 2.6 2.2 2.2 2.6 Escenario 10: 4.4 2 2 1.8 Promedio de todos los casos: 2.9 1.82 1.7 1.76 Tama et al. modelan el protocolo de negociación utilizando ontologías, en cambio, nosotros hemos modelado el servicio a ser negociado. Además, hemos construido un sistema con el cual se pueden aprender las preferencias de negociación. El estudio de Sadri et al. analiza la negociación en el contexto de la asignación de recursos [14]. Los agentes tienen recursos limitados y necesitan solicitar recursos faltantes a otros agentes. Se propone un mecanismo basado en secuencias de diálogo entre agentes como solución. El mecanismo se basa en el ciclo de agente de observar-pensar-actuar. Estos diálogos incluyen ofrecer recursos, intercambios de recursos y ofrecer recursos alternativos. Cada agente en el sistema planea sus acciones para alcanzar un estado objetivo. A diferencia de nuestro enfoque, el estudio de Sadri et al. no se preocupa por las preferencias de aprendizaje mutuas. Brzostowski y Kowalczyk proponen un enfoque para seleccionar un socio de negociación adecuado investigando negociaciones previas de múltiples atributos [1]. Para lograr esto, utilizan el razonamiento basado en casos. Su enfoque es probabilístico ya que el comportamiento de los socios puede cambiar en cada iteración. En nuestro enfoque, estamos interesados en negociar el contenido del servicio. Después de que el consumidor y el productor acuerden el servicio, se pueden utilizar mecanismos de negociación orientados al precio para acordar el precio. Fatima et al. estudian los factores que afectan la negociación, como las preferencias, el plazo, el precio, entre otros, ya que el agente que desarrolla una estrategia contra su oponente debe considerar todos ellos [5]. En su enfoque, el objetivo del agente vendedor es vender el servicio al precio más alto posible, mientras que el objetivo del agente comprador es comprar el bien al precio más bajo posible. El intervalo de tiempo afecta a estos agentes de manera diferente. En comparación con Fatima et al., nuestro enfoque es diferente. Mientras ellos estudian el efecto del tiempo en la negociación, nuestro enfoque está en aprender las preferencias para una negociación exitosa. Faratin et al. proponen un mecanismo de negociación multi-tema, donde las variables de servicio para la negociación, como el precio, la calidad del servicio, entre otros, se consideran intercambios entre sí (es decir, un precio más alto por una entrega más temprana) [4]. Generan un modelo heurístico para compensaciones que incluye la estimación de similitud difusa y una exploración de escalada de colina para ofertas posiblemente aceptables. Aunque abordamos un problema similar, aprendemos las preferencias del cliente con la ayuda del aprendizaje inductivo y generamos contraofertas de acuerdo con estas preferencias aprendidas. Faratin et al. solo utilizan la última oferta realizada por el consumidor al calcular la similitud para elegir la contraoferta. A diferencia de ellos, también tenemos en cuenta las solicitudes previas del consumidor. En sus experimentos, Faratin et al. asumen que los pesos de las variables de servicio están fijos a priori. Por el contrario, aprendemos estas preferencias con el tiempo. En nuestro trabajo futuro, planeamos integrar el razonamiento ontológico en el algoritmo de aprendizaje para que la información jerárquica pueda ser aprendida a partir de la jerarquía de subsumpción de relaciones. Además, al utilizar las relaciones entre las características, el productor puede descubrir nuevos conocimientos a partir de los conocimientos existentes. Estas son direcciones interesantes que seguiremos en nuestro trabajo futuro. 9. REFERENCIAS [1] J. Brzostowski y R. Kowalczyk. En el razonamiento basado en casos posibilístico para la selección de socios para la negociación de agentes de múltiples atributos. En Actas del 4to Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 273-278, 2005. [2] L. Busch e I. Horstman. Un comentario sobre negociaciones tema por tema. Juegos y Comportamiento Económico, 19:144-148, 1997. [3] J. K. Debenham. Gestión de la negociación en el mercado electrónico en el contexto de un sistema multiagente. En Actas de la 21ª Conferencia Internacional sobre Sistemas Basados en el Conocimiento e Inteligencia Artificial Aplicada, ES2002:, 2002. [4] P. Faratin, C. Sierra y N. R. Jennings. Utilizando criterios de similitud para hacer compensaciones de problemas en negociaciones automatizadas. Inteligencia Artificial, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge y N. Jennings. Agentes óptimos para negociaciones de múltiples temas. En Actas del 2do Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 129-136, 2003. [6] C. Giraud-Carrier. Una nota sobre la utilidad del aprendizaje incremental. Comunicaciones de IA, 13(4):215-223, 2000. [7] T.-P. Hong y S.-S. Tseng. Dividiendo y fusionando espacios de versiones para aprender conceptos disyuntivos. IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.\n\nTraducción al español:\nIEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin. Una definición de similitud basada en teoría de la información. En Actas de la 15ª Conferencia Internacional sobre Aprendizaje Automático, páginas 296-304. Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, y A. G. Moukas. Agentes que compran y venden. Comunicaciones de la ACM, 42(3):81-91, 1999. [10] T. M. Mitchell. Aprendizaje automático. McGraw Hill, NY, 1997. [11] Búho. OWL: Guía del lenguaje de ontologías web, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal y S. C. K. Shiu. Fundamentos del Razonamiento Basado en Casos Blandos. John Wiley & Sons, Nueva Jersey, 2004. [13] J. R. Quinlan. Inducción de árboles de decisión. Aprendizaje automático, 1(1):81-106, 1986. [14] F. Sadri, F. Toni y P. Torroni. Diálogos para negociación: Variedades de agentes y secuencias de diálogo. En ATAL 2001, Artículos Revisados, volumen 2333 de LNAI, páginas 405-421. Springer-Verlag, 2002. [15] M. P. Singh. \n\nSpringer-Verlag, 2002. [15] M. P. Singh. Comercio electrónico orientado al valor. IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, y M. Wooldridge. Ontologías para apoyar la negociación en el comercio electrónico. Aplicaciones de la Inteligencia Artificial en Ingeniería, 18:223-236, 2005. [17] A. Tversky. Características de similitud. Revisión Psicológica, 84(4):327-352, 1977. [18] P. E. Utgoff. Inducción incremental de árboles de decisión. Aprendizaje automático, 4:161-186, 1989. [19] Vino, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu y M. Palmer. Semántica de verbos y selección léxica. En el 32. Reunión anual de la Asociación de Lingüística Computacional, páginas 133-138, 1994. 1308 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    "múltiples espacios de versión",
                    "múltiples espacios de versiones"
                ]
            ]
        },
        "disjunctive hypothesis": {
            "translated_key": "hipótesis disyuntivas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include <br>disjunctive hypothesis</br> in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include <br>disjunctive hypothesis</br> in addition to the conjunctives and negation."
            ],
            "translated_annotated_samples": [
                "Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir <br>hipótesis disyuntivas</br> además de las conjunciones y la negación."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor. La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor. Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios. Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las preferencias del consumidor y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta. Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor. Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles. Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor. Este desafío se puede estudiar en tres etapas: • Aprendizaje de preferencias: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5) APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica. Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio. Por ejemplo, un consumidor puede tener preferencia por el vino tinto. Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo. Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo). Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor. Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores. Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa. Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo. Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6]. Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones. Investigamos particularmente dos enfoques. El primero es el aprendizaje inductivo. Esta técnica se aplica para aprender las preferencias como conceptos. Desarrollamos el Algoritmo de Eliminación de Candidatos (CEA) para el Espacio de Versiones [10]. Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva. Curiosamente, la mayoría de las veces las preferencias del consumidor son disyuntivas. Estamos considerando un agente que está comprando vino. El consumidor puede preferir vino tinto o vino rosado pero no vino blanco. Para utilizar CEA con tales preferencias, es necesaria una modificación sólida. El segundo enfoque son los árboles de decisión. Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas. Un árbol de decisión incremental bien conocido es ID5R [18]. Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional. Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados. El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender. El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener. Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto. El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo. Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío. Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo. En cada interacción entre el productor y el consumidor, tanto G como S son modificados. Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas. Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento. Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa. Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí. Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 CEA Disyuntivo Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos. Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos. Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones. Algunos de estos enfoques utilizan múltiples espacios de versión. Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7]. Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir <br>hipótesis disyuntivas</br> además de las conjunciones y la negación. Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica. EJEMPLO 1. Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco). El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor. Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco. En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}. Solo cuando todos los valores existan en la lista, serán reemplazados por ?. En otras palabras, permitimos que el algoritmo generalice más lentamente que antes. Modificamos el algoritmo CEA para hacer frente a este cambio. El algoritmo modificado, DCEA, se presenta como Algoritmo 1. Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de múltiples espacios de versiones. La fase de inicialización es la misma que el algoritmo original (líneas 1, 2). Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4). Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí. Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran. Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7). Cuando llega una muestra negativa, no cambiamos S como antes. Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15). A diferencia del CEA original, intentamos especializar el G mínimamente. El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13). Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada. Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez. Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14). Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar. Por ejemplo, considera el atributo del color. Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2. La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA. El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra. Después de la primera muestra positiva, S se generaliza para cubrir también la instancia. La segunda muestra es negativa. Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada. En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general. La tercera muestra es positiva y generaliza S aún más. Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco). Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis. Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}. Por el Algoritmo 1, no perdemos ninguna información. 3.3 ID3 ID3 [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor. Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos. El algoritmo ID3 se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas. Hay dos clases: positiva y negativa. Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor. Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas. El árbol de decisión tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba. El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio. Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos. Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja. El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender. Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos. Después de cada solicitud entrante, el árbol de decisiones se reconstruye. Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso. Sin embargo, en la práctica hemos evaluado que el ID3 es rápido y el costo de reconstrucción es insignificante. 4. OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA. El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes. Cuando el productor recibe una solicitud del consumidor, el conjunto de aprendizaje del productor se entrena con esta solicitud como una muestra positiva. Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios. El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor. En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor. El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores. Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios. Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios. Esta lista constituye el inventario de servicios de los proveedores. Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace. Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor. Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación. Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S). Calculamos las similitudes de varias maneras, como se explicará en la Sección 5. Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud. El agente productor puede romper el empate de varias maneras. Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de ID3 Si el productor aprende las preferencias de los consumidores con ID3, se aplica un mecanismo similar con dos diferencias. Primero, dado que ID3 no mantiene G, se eliminan de la lista de servicios aquellos no aceptados que se clasifican como negativos. Segundo, las similitudes de los posibles servicios no se miden con respecto a S, sino en cambio a todas las solicitudes previamente realizadas. 4.3 Mecanismos Alternativos de Oferta de Servicios Además de estos tres mecanismos de oferta de servicios (Oferta de Servicio con CEA, Oferta de Servicio con DCEA y Oferta de Servicio con ID3), incluimos otros dos mecanismos. 1304 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) • Oferta de Servicio Aleatoria (RO): El productor genera una contraoferta aleatoriamente de la lista de servicios disponibles, sin considerar las preferencias de los consumidores. • Oferta de Servicio considerando solo la solicitud actual (SCR): El productor selecciona una contraoferta de acuerdo con la similitud de la solicitud actual del consumidor pero no considera solicitudes anteriores. 5. ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una métrica de similitud que toma dos entradas y devuelve qué tan similares son. Existen varios métricos de similitud utilizados en sistemas de razonamiento basado en casos, como la suma ponderada de la distancia euclidiana, la distancia de Hamming, entre otros [12]. La métrica de similitud afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor. Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de similitud semántica llamada Similitud RP. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente. En la Ecuación (1), común representa la cantidad de atributos coincidentes, mientras que diferente representa la cantidad de atributos diferentes. Nuestra suposición actual es que α y β son iguales entre sí. SMpq = α(común) α(común) + β(diferente) (1) Aquí, al comparar dos características, asignamos cero para la disimilitud y uno para la similitud al omitir la cercanía semántica entre los valores de las características. La métrica de similitud de Tversky está diseñada para comparar dos vectores de características. En nuestro sistema, mientras que la lista de servicios que puede ofrecer el productor son cada uno un vector de características, el conjunto más específico S no es un vector de características. S consiste en hipótesis de vectores de características. Por lo tanto, estimamos la similitud de cada hipótesis dentro del conjunto más específico S y luego calculamos el promedio de las similitudes. EJEMPLO 3. Suponga que S contiene las siguientes dos hipótesis: { {Luz, Moderado, (Rojo, Blanco)} , {Completo, Fuerte, Rosa}}. Toma el servicio s como (Ligero, Resistente, Rosa). Entonces, la similitud del primero es igual a 1/3 y la del segundo es igual a 2/3 de acuerdo con la Ecuación (1). Normalmente, tomamos el promedio de ello y obtenemos (1/3 + 2/3)/2, que es igual a 1/2. Sin embargo, la primera hipótesis implica el efecto de dos solicitudes y la segunda hipótesis implica solo una solicitud. Por lo tanto, esperamos que el efecto de la primera hipótesis sea mayor que el de la segunda. Por lo tanto, calculamos la similitud promedio teniendo en cuenta la cantidad de muestras que las hipótesis cubren. Que ch denote el número de muestras que cubre la hipótesis h y (SM(h,servicio)) denote la similitud de la hipótesis h con el servicio dado. Calculamos la similitud de cada hipótesis con el servicio dado y las ponderamos con el número de muestras que cubren. Encontramos la similitud dividiendo la suma ponderada de las similitudes de todas las hipótesis en S con el servicio por el número de todas las muestras que están cubiertas en S. AV G−SM(servicio, S) = |S| |h| (ch ∗ SM(h, servicio)) |S| |h| ch (2) Figura 2: Taxonomía de muestra para estimación de similitud EJEMPLO 4. Para el ejemplo anterior, la similitud de (Luz, Fuerte, Rosa) con el conjunto específico es (2 ∗ 1/3 + 2/3)/3, igual a 4/9. El número posible de muestras que abarca una hipótesis se puede estimar multiplicando las cardinalidades de cada atributo. Por ejemplo, la cardinalidad del primer atributo es dos y la de los demás es igual a uno para la hipótesis dada, como {Luz, Moderado, (Rojo, Blanco)}. Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La métrica de similitud de Lins Un taxonomía puede ser utilizada al estimar la similitud semántica entre dos conceptos. Estimar la similitud semántica en una taxonomía de tipo Es-Un se puede hacer calculando la distancia entre los nodos relacionados con los conceptos comparados. Los enlaces entre los nodos pueden considerarse como distancias. Entonces, la longitud del camino entre los nodos indica qué tan similares son los conceptos. Una estimación alternativa para utilizar el contenido de información en la estimación de la similitud semántica en lugar del método de conteo de aristas, fue propuesta por Lin [8]. La ecuación (3) [8] muestra la similitud de Lin donde c1 y c2 son los conceptos comparados y c0 es el concepto más específico que subsume a ambos. Además, P(C) representa la probabilidad de que un objeto seleccionado arbitrariamente pertenezca al concepto C. La similitud(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Métrica de similitud de Wu y Palmers Diferente de Lin, Wu y Palmer utilizan la distancia entre los nodos en la taxonomía ES-UN [20]. La similitud semántica se representa con la Ecuación (4) [20]. Aquí, se estima la similitud entre c1 y c2 y c0 es el concepto más específico que subsume estas clases. N1 es el número de aristas entre c1 y c0. N2 es el número de aristas entre c2 y c0. N0 es el número de enlaces IS-A de c0 desde la raíz de la taxonomía. Proponemos estimar la distancia relativa en una taxonomía entre dos conceptos utilizando las siguientes intuiciones. Utilizamos la Figura 2 para ilustrar estas intuiciones. • Padre versus abuelo: El padre de un nodo es más similar al nodo que los abuelos de ese. Generalización del Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1305 es un concepto que razonablemente resulta en alejarse más de ese concepto. Cuanto más generales son los conceptos, menos similares son. Por ejemplo, AnyWineColor es el padre de ReddishColor y ReddishColor es el padre de Red. Entonces, esperamos que la similitud entre ReddishColor y Red sea mayor que la similitud entre AnyWineColor y Red. • Padre versus hermano: Un nodo tendría una similitud mayor con su padre que con su hermano. Por ejemplo, Rojo y Rosa son hijos de ColorRojo. En este caso, esperamos que la similitud entre Rojo y ColorRojizo sea mayor que la de Rojo y Rosa. • Hermano versus abuelo: Un nodo es más similar a su hermano que a su abuelo. Para ilustrar, AnyWineColor es el abuelo de Red, y Red y Rose son hermanos. Por lo tanto, posiblemente anticipamos que Rojo y Rosa son más similares que CualquierColorDeVino y Rojo. Como una taxonomía está representada en un árbol, ese árbol puede ser recorrido desde el primer concepto que se está comparando hasta el segundo concepto. En el nodo inicial relacionado con el primer concepto, el valor de similitud es constante y igual a uno. Este valor se reduce por una constante en cada nodo visitado a lo largo del camino que llegará al nodo que incluye el segundo concepto. Cuanto más corto sea el camino entre los conceptos, mayor será la similitud entre los nodos. Algoritmo 2 Estimar-Similitud-RP(c1,c2) Requerido: Las constantes deben ser m > n > m2 donde m, n ∈ R[0, 1] 1: Similitud ← 1 2: si c1 es igual a c2 entonces 3: Devolver Similitud 4: fin si 5: padreComun ← encontrarPadreComun(c1, c2) {padreComun es el concepto más específico que cubre tanto c1 como c2} 6: N1 ← encontrarDistancia(padreComun, c1) 7: N2 ← encontrarDistancia(padreComun, c2) {N1 y N2 son el número de enlaces entre el concepto y el concepto padre} 8: si (padreComun == c1) o (padreComun == c2) entonces 9: Similitud ← Similitud ∗ m(N1+N2) 10: sino 11: Similitud ← Similitud ∗ n ∗ m(N1+N2−2) 12: fin si 13: Devolver Similitud La distancia relativa entre los nodos c1 y c2 se estima de la siguiente manera. Comenzando desde c1, se recorre el árbol para llegar a c2. En cada salto, la similitud disminuye ya que los conceptos se están alejando cada vez más entre sí. Sin embargo, según nuestras intuiciones, no todos los saltos disminuyen la similitud de igual manera. Que m represente el factor para saltar de un hijo a un padre y que n represente el factor para saltar de un hermano a otro hermano. Dado que saltar de un nodo a su abuelo cuenta como dos saltos de padre, el factor de descuento al moverse de un nodo a su abuelo es m2. De acuerdo con las intuiciones anteriores, nuestras constantes deben estar en la forma m > n > m2 donde el valor de m y n debe estar entre cero y uno. El algoritmo 2 muestra el cálculo de la distancia. Según el algoritmo, en primer lugar la similitud se inicializa con el valor de uno (línea 1). Si los conceptos son iguales entre sí, entonces la similitud será uno (líneas 2-4). De lo contrario, calculamos el ancestro común de los dos nodos y la distancia de cada concepto al ancestro común sin considerar al hermano (líneas 5-7). Si uno de los conceptos es igual al padre común, entonces no hay relación de hermanos entre los conceptos. Para cada nivel, multiplicamos la similitud por m y no consideramos el factor de hermanos en la estimación de la similitud. Como resultado, disminuimos la similitud en cada nivel con la tasa de m (línea 9). De lo contrario, tiene que existir una relación de hermanos. Esto significa que debemos considerar el efecto de n al medir la similitud. Recuerde que hemos contado N1+N2 aristas entre los conceptos. Dado que existe una relación de hermanos, dos de estos bordes constituyen la relación de hermanos. Por lo tanto, al calcular el efecto de la relación parental, utilizamos N1+N2 −2 aristas (línea 11). Algunas estimaciones de similitud relacionadas con la taxonomía en la Figura 2 se presentan en la Tabla 2. En este ejemplo, se toma m como 2/3 y n como 4/7. Tabla 2: Estimación de similitud de muestra sobre la taxonomía de muestra. Similitud(ColorRojo, Rosa) = 1 ∗ (2/3) = 0.6666667 Similitud(Rojo, Rosa) = 1 ∗ (4/7) = 0.5714286 Similitud(CualquierColorVino, Rosa) = 1 ∗ (2/3)2 = 0.44444445 Similitud(Blanco, Rosa) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 Para todas las métricas de similitud semántica en nuestra arquitectura, la taxonomía de características se mantiene en la ontología compartida. Para evaluar la similitud del vector de características, primero estimamos la similitud para cada característica individualmente y luego calculamos la suma promedio de estas similitudes. Entonces, el resultado es igual a la similitud semántica promedio de todo el vector de características. 6. SISTEMA DESARROLLADO Hemos implementado nuestra arquitectura en Java. Para facilitar las pruebas del sistema, el agente del consumidor tiene una interfaz de usuario que nos permite ingresar varias solicitudes. El agente productor está completamente automatizado y las operaciones de aprendizaje y oferta de servicios funcionan como se explicó anteriormente. En esta sección, explicamos los detalles de implementación del sistema desarrollado. Utilizamos OWL [11] como nuestro lenguaje de ontología y JENA como nuestro razonador de ontología. La ontología compartida es la versión modificada de la Ontología del Vino [19]. Incluye la descripción del vino como concepto y diferentes tipos de vino. Todos los participantes de la negociación utilizan esta ontología para entenderse mutuamente. Según la ontología, siete propiedades conforman el concepto de vino. El agente consumidor y el agente productor obtienen los valores posibles para estas propiedades consultando la ontología. Por lo tanto, todos los valores posibles para los componentes del concepto del vino, como el color, cuerpo, azúcar, etc., pueden ser alcanzados por ambos agentes. También se describen en esta ontología una variedad de tipos de vino como Borgoña, Chardonnay, Chenin Blanc, entre otros. Intuitivamente, cualquier tipo de vino descrito en la ontología también representa un concepto de vino. Esto nos permite considerar las instancias de vino Chardonnay como instancias de la clase Vino. Además de la descripción del vino, la información jerárquica de algunas características se puede inferir de la ontología. Por ejemplo, podemos representar la información de que el continente europeo abarca países occidentales. El país occidental abarca la región francesa, que incluye algunos territorios como el Loira, Burdeos, entre otros. Esta información jerárquica se utiliza en la estimación de similitud semántica. En esta parte, se pueden hacer algunos razonamientos como si un concepto X abarca Y y Y abarca Z, entonces el concepto X abarca Z. Por ejemplo, el Continente Europeo abarca Burdeos. 1306 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Para algunas características como cuerpo, sabor y azúcar, no hay información jerárquica, pero sus valores están nivelados semánticamente. Cuando eso sucede, proporcionamos los valores de similitud razonables para estas características. Por ejemplo, el cuerpo puede ser ligero, medio o fuerte. En este caso, asumimos que la luz es 0.66 similar a media pero solo 0.33 a fuerte. La ontología de WineStock es el inventario de los productores y describe una clase de producto como WineProduct. Esta clase es necesaria para que el productor registre los vinos que vende. La ontología implica a los individuos de esta clase. Los individuos representan los servicios disponibles que posee el productor. Hemos preparado dos ontologías de WineStock separadas para realizar pruebas. En la primera ontología, hay 19 productos de vino disponibles y en la segunda ontología, hay 50 productos. EVALUACIÓN DEL RENDIMIENTO Evaluamos el rendimiento de los sistemas propuestos en relación con la técnica de aprendizaje que utilizaron, DCEA e ID3, comparándolos con CEA, RO (para oferta aleatoria) y SCR (oferta basada solo en la solicitud actual). Aplicamos una variedad de escenarios en este conjunto de datos para ver las diferencias de rendimiento. Cada escenario de prueba contiene una lista de preferencias para el usuario y el número de coincidencias de la lista de productos. La Tabla 3 muestra estas preferencias y la disponibilidad de esos productos en el inventario para los primeros cinco escenarios. Ten en cuenta que estas preferencias son internas al consumidor y el productor intenta aprenderlas durante la negociación. Tabla 3: Disponibilidad de vinos en diferentes escenarios de prueba ID Preferencia del consumidor Disponibilidad (de 19) 1 Vino seco 15 2 Vino tinto y seco 8 3 Vino tinto, seco y moderado 4 4 Vino tinto y fuerte 2 5 Vino tinto o rosado, y fuerte 3 7.1 Comparación de Algoritmos de Aprendizaje En la comparación de algoritmos de aprendizaje, utilizamos los cinco escenarios de la Tabla 3. Aquí, primero usamos la medida de similitud de Tversky. Con estos casos de prueba, estamos interesados en encontrar el número de iteraciones que se requieren para que el productor genere una oferta aceptable para el consumidor. Dado que el rendimiento también depende de la solicitud inicial, repetimos nuestros experimentos con diferentes solicitudes iniciales. Por consiguiente, para cada caso, ejecutamos los algoritmos cinco veces con varias variaciones de las solicitudes iniciales. En cada experimento, contamos el número de iteraciones necesarias para llegar a un acuerdo. Tomamos el promedio de estos números para evaluar estos sistemas de manera justa. Como es costumbre, probamos cada algoritmo con las mismas solicitudes iniciales. La Tabla 4 compara los enfoques utilizando diferentes algoritmos de aprendizaje. Cuando las partes grandes del inventario son compatibles con las preferencias de los clientes, como en el primer caso de prueba, el rendimiento de todas las técnicas es casi el mismo (por ejemplo, Escenario 1). A medida que el número de servicios compatibles disminuye, RO funciona mal como se esperaba. El segundo peor método es SCR ya que solo considera la solicitud más reciente de los clientes y no aprende de las solicitudes anteriores. CEA da los mejores resultados cuando puede generar una respuesta pero no puede manejar los casos que contienen preferencias disyuntivas, como el que se presenta en el Escenario 5. ID3 y DCEA logran los mejores resultados. Su rendimiento es comparable y pueden manejar todos los casos, incluido el Escenario 5. Tabla 4: Comparación de algoritmos de aprendizaje en términos del número promedio de interacciones. Ejecutar DCEA SCR RO CEA ID3 Escenario 1: 1.2 1.4 1.2 1.2 1.2 Escenario 2: 1.4 1.4 2.6 1.4 1.4 Escenario 3: 1.4 1.8 4.4 1.4 1.4 Escenario 4: 2.2 2.8 9.6 1.8 2 Escenario 5: 2 2.6 7.6 1.75+ Sin oferta 1.8 Promedio de todos los casos: 1.64 2 5.08 1.51+Sin oferta 1.56 7.2 Comparación de Métricas de Similitud Para comparar las métricas de similitud que se explicaron en la Sección 5, fijamos el algoritmo de aprendizaje en DCEA. Además de los escenarios mostrados en la Tabla 3, agregamos los siguientes cinco nuevos escenarios considerando la información jerárquica. • El cliente desea comprar vino cuya bodega esté ubicada en California y cuya uva sea de tipo blanco. Además, la bodega del vino no debería ser costosa. Solo hay cuatro productos que cumplen con estas condiciones. • El cliente quiere comprar vino de color rojo o rosado y de tipo de uva tinta. Además, la ubicación del vino debe ser en Europa. Se desea que el grado de dulzura sea seco o semiseco. El sabor debe ser delicado o moderado, mientras que el cuerpo debe ser medio o ligero. Además, la bodega del vino debería ser una bodega cara. Hay dos productos que cumplen con todos estos requisitos. El cliente quiere comprar vino rosado moderado, que se encuentra alrededor de la región francesa. La categoría de bodega debería ser Bodega Moderada. Solo hay un producto que cumple con estos requisitos. • El cliente quiere comprar vino tinto caro, que se encuentra alrededor de la Región de California o vino blanco barato, que se encuentra alrededor de la Región de Texas. Hay cinco productos disponibles. • El cliente quiere comprar un vino blanco delicado cuyo productor esté en la categoría de Bodega Costosa. Hay dos productos disponibles. Los primeros siete escenarios se prueban con el primer conjunto de datos que contiene un total de 19 servicios y los últimos tres escenarios se prueban con el segundo conjunto de datos que contiene 50 servicios. La Tabla 5 muestra la evaluación del rendimiento en términos del número de interacciones necesarias para llegar a un consenso. La métrica de Tversky da los peores resultados ya que no considera la similitud semántica. El rendimiento de Lins es mejor que el de Tversky pero peor que el de otros. La métrica de Wu-Palmer y la medida de similitud de RP casi ofrecen el mismo rendimiento y son mejores que otras. Cuando se examinan los resultados, considerar la cercanía semántica aumenta el rendimiento. 8. DISCUSIÓN Revisamos la literatura reciente en comparación con nuestro trabajo. Tama et al. [16] proponen un nuevo enfoque basado en ontología para la negociación. Según su enfoque, los protocolos de negociación utilizados en el comercio electrónico pueden ser modelados como ontologías. Por lo tanto, los agentes pueden llevar a cabo un protocolo de negociación utilizando esta ontología compartida sin necesidad de estar codificados con los detalles del protocolo de negociación. Mientras tanto, la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1307 Tabla 5: Comparación de métricas de similitud en términos de número de interacciones. Ejecutar Tversky Lin Wu Palmer RP Escenario 1: 1.2 1.2 1 1 Escenario 2: 1.4 1.4 1.6 1.6 Escenario 3: 1.4 1.8 2 2 Escenario 4: 2.2 1 1.2 1.2 Escenario 5: 2 1.6 1.6 1.6 Escenario 6: 5 3.8 2.4 2.6 Escenario 7: 3.2 1.2 1 1 Escenario 8: 5.6 2 2 2.2 Escenario 9: 2.6 2.2 2.2 2.6 Escenario 10: 4.4 2 2 1.8 Promedio de todos los casos: 2.9 1.82 1.7 1.76 Tama et al. modelan el protocolo de negociación utilizando ontologías, en cambio, nosotros hemos modelado el servicio a ser negociado. Además, hemos construido un sistema con el cual se pueden aprender las preferencias de negociación. El estudio de Sadri et al. analiza la negociación en el contexto de la asignación de recursos [14]. Los agentes tienen recursos limitados y necesitan solicitar recursos faltantes a otros agentes. Se propone un mecanismo basado en secuencias de diálogo entre agentes como solución. El mecanismo se basa en el ciclo de agente de observar-pensar-actuar. Estos diálogos incluyen ofrecer recursos, intercambios de recursos y ofrecer recursos alternativos. Cada agente en el sistema planea sus acciones para alcanzar un estado objetivo. A diferencia de nuestro enfoque, el estudio de Sadri et al. no se preocupa por las preferencias de aprendizaje mutuas. Brzostowski y Kowalczyk proponen un enfoque para seleccionar un socio de negociación adecuado investigando negociaciones previas de múltiples atributos [1]. Para lograr esto, utilizan el razonamiento basado en casos. Su enfoque es probabilístico ya que el comportamiento de los socios puede cambiar en cada iteración. En nuestro enfoque, estamos interesados en negociar el contenido del servicio. Después de que el consumidor y el productor acuerden el servicio, se pueden utilizar mecanismos de negociación orientados al precio para acordar el precio. Fatima et al. estudian los factores que afectan la negociación, como las preferencias, el plazo, el precio, entre otros, ya que el agente que desarrolla una estrategia contra su oponente debe considerar todos ellos [5]. En su enfoque, el objetivo del agente vendedor es vender el servicio al precio más alto posible, mientras que el objetivo del agente comprador es comprar el bien al precio más bajo posible. El intervalo de tiempo afecta a estos agentes de manera diferente. En comparación con Fatima et al., nuestro enfoque es diferente. Mientras ellos estudian el efecto del tiempo en la negociación, nuestro enfoque está en aprender las preferencias para una negociación exitosa. Faratin et al. proponen un mecanismo de negociación multi-tema, donde las variables de servicio para la negociación, como el precio, la calidad del servicio, entre otros, se consideran intercambios entre sí (es decir, un precio más alto por una entrega más temprana) [4]. Generan un modelo heurístico para compensaciones que incluye la estimación de similitud difusa y una exploración de escalada de colina para ofertas posiblemente aceptables. Aunque abordamos un problema similar, aprendemos las preferencias del cliente con la ayuda del aprendizaje inductivo y generamos contraofertas de acuerdo con estas preferencias aprendidas. Faratin et al. solo utilizan la última oferta realizada por el consumidor al calcular la similitud para elegir la contraoferta. A diferencia de ellos, también tenemos en cuenta las solicitudes previas del consumidor. En sus experimentos, Faratin et al. asumen que los pesos de las variables de servicio están fijos a priori. Por el contrario, aprendemos estas preferencias con el tiempo. En nuestro trabajo futuro, planeamos integrar el razonamiento ontológico en el algoritmo de aprendizaje para que la información jerárquica pueda ser aprendida a partir de la jerarquía de subsumpción de relaciones. Además, al utilizar las relaciones entre las características, el productor puede descubrir nuevos conocimientos a partir de los conocimientos existentes. Estas son direcciones interesantes que seguiremos en nuestro trabajo futuro. 9. REFERENCIAS [1] J. Brzostowski y R. Kowalczyk. En el razonamiento basado en casos posibilístico para la selección de socios para la negociación de agentes de múltiples atributos. En Actas del 4to Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 273-278, 2005. [2] L. Busch e I. Horstman. Un comentario sobre negociaciones tema por tema. Juegos y Comportamiento Económico, 19:144-148, 1997. [3] J. K. Debenham. Gestión de la negociación en el mercado electrónico en el contexto de un sistema multiagente. En Actas de la 21ª Conferencia Internacional sobre Sistemas Basados en el Conocimiento e Inteligencia Artificial Aplicada, ES2002:, 2002. [4] P. Faratin, C. Sierra y N. R. Jennings. Utilizando criterios de similitud para hacer compensaciones de problemas en negociaciones automatizadas. Inteligencia Artificial, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge y N. Jennings. Agentes óptimos para negociaciones de múltiples temas. En Actas del 2do Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 129-136, 2003. [6] C. Giraud-Carrier. Una nota sobre la utilidad del aprendizaje incremental. Comunicaciones de IA, 13(4):215-223, 2000. [7] T.-P. Hong y S.-S. Tseng. Dividiendo y fusionando espacios de versiones para aprender conceptos disyuntivos. IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.\n\nTraducción al español:\nIEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin. Una definición de similitud basada en teoría de la información. En Actas de la 15ª Conferencia Internacional sobre Aprendizaje Automático, páginas 296-304. Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, y A. G. Moukas. Agentes que compran y venden. Comunicaciones de la ACM, 42(3):81-91, 1999. [10] T. M. Mitchell. Aprendizaje automático. McGraw Hill, NY, 1997. [11] Búho. OWL: Guía del lenguaje de ontologías web, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal y S. C. K. Shiu. Fundamentos del Razonamiento Basado en Casos Blandos. John Wiley & Sons, Nueva Jersey, 2004. [13] J. R. Quinlan. Inducción de árboles de decisión. Aprendizaje automático, 1(1):81-106, 1986. [14] F. Sadri, F. Toni y P. Torroni. Diálogos para negociación: Variedades de agentes y secuencias de diálogo. En ATAL 2001, Artículos Revisados, volumen 2333 de LNAI, páginas 405-421. Springer-Verlag, 2002. [15] M. P. Singh. \n\nSpringer-Verlag, 2002. [15] M. P. Singh. Comercio electrónico orientado al valor. IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, y M. Wooldridge. Ontologías para apoyar la negociación en el comercio electrónico. Aplicaciones de la Inteligencia Artificial en Ingeniería, 18:223-236, 2005. [17] A. Tversky. Características de similitud. Revisión Psicológica, 84(4):327-352, 1977. [18] P. E. Utgoff. Inducción incremental de árboles de decisión. Aprendizaje automático, 4:161-186, 1989. [19] Vino, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu y M. Palmer. Semántica de verbos y selección léxica. En el 32. Reunión anual de la Asociación de Lingüística Computacional, páginas 133-138, 1994. 1308 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "id3": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the <br>id3</br> algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 <br>id3</br> <br>id3</br> [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The <br>id3</br> algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated <br>id3</br> to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via <br>id3</br> If the producer learns the consumers preferences with <br>id3</br>, a similar mechanism is applied with two differences.",
                "First, since <br>id3</br> does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with <br>id3</br>), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and <br>id3</br>, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "<br>id3</br> and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA <br>id3</br> Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "For this reason, we instead use the <br>id3</br> algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "By Algorithm 1, we do not miss any information. 3.3 <br>id3</br> <br>id3</br> [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "The <br>id3</br> algorithm is used in the learning process with the purpose of classification of offers.",
                "However, in practice we have evaluated <br>id3</br> to be fast and the reconstruction cost to be negligible. 4.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via <br>id3</br> If the producer learns the consumers preferences with <br>id3</br>, a similar mechanism is applied with two differences."
            ],
            "translated_annotated_samples": [
                "Por esta razón, en su lugar utilizamos el <br>algoritmo ID3</br> [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados.",
                "Por el Algoritmo 1, no perdemos ninguna información. 3.3 <br>ID3</br> <br>ID3</br> [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor.",
                "El <br>algoritmo ID3</br> se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas.",
                "Sin embargo, en la práctica hemos evaluado que el <br>ID3</br> es rápido y el costo de reconstrucción es insignificante. 4.",
                "Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de <br>ID3</br> Si el productor aprende las preferencias de los consumidores con <br>ID3</br>, se aplica un mecanismo similar con dos diferencias."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor. La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor. Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios. Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las preferencias del consumidor y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta. Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor. Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles. Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor. Este desafío se puede estudiar en tres etapas: • Aprendizaje de preferencias: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5) APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica. Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio. Por ejemplo, un consumidor puede tener preferencia por el vino tinto. Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo. Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo). Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor. Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores. Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa. Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo. Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6]. Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones. Investigamos particularmente dos enfoques. El primero es el aprendizaje inductivo. Esta técnica se aplica para aprender las preferencias como conceptos. Desarrollamos el Algoritmo de Eliminación de Candidatos (CEA) para el Espacio de Versiones [10]. Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva. Curiosamente, la mayoría de las veces las preferencias del consumidor son disyuntivas. Estamos considerando un agente que está comprando vino. El consumidor puede preferir vino tinto o vino rosado pero no vino blanco. Para utilizar CEA con tales preferencias, es necesaria una modificación sólida. El segundo enfoque son los árboles de decisión. Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas. Un árbol de decisión incremental bien conocido es ID5R [18]. Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional. Por esta razón, en su lugar utilizamos el <br>algoritmo ID3</br> [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados. El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender. El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener. Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto. El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo. Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío. Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo. En cada interacción entre el productor y el consumidor, tanto G como S son modificados. Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas. Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento. Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa. Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí. Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 CEA Disyuntivo Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos. Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos. Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones. Algunos de estos enfoques utilizan múltiples espacios de versión. Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7]. Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir hipótesis disyuntivas además de las conjunciones y la negación. Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica. EJEMPLO 1. Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco). El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor. Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco. En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}. Solo cuando todos los valores existan en la lista, serán reemplazados por ?. En otras palabras, permitimos que el algoritmo generalice más lentamente que antes. Modificamos el algoritmo CEA para hacer frente a este cambio. El algoritmo modificado, DCEA, se presenta como Algoritmo 1. Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de múltiples espacios de versiones. La fase de inicialización es la misma que el algoritmo original (líneas 1, 2). Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4). Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí. Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran. Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7). Cuando llega una muestra negativa, no cambiamos S como antes. Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15). A diferencia del CEA original, intentamos especializar el G mínimamente. El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13). Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada. Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez. Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14). Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar. Por ejemplo, considera el atributo del color. Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2. La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA. El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra. Después de la primera muestra positiva, S se generaliza para cubrir también la instancia. La segunda muestra es negativa. Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada. En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general. La tercera muestra es positiva y generaliza S aún más. Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco). Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis. Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}. Por el Algoritmo 1, no perdemos ninguna información. 3.3 <br>ID3</br> <br>ID3</br> [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor. Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos. El <br>algoritmo ID3</br> se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas. Hay dos clases: positiva y negativa. Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor. Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas. El árbol de decisión tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba. El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio. Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos. Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja. El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender. Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos. Después de cada solicitud entrante, el árbol de decisiones se reconstruye. Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso. Sin embargo, en la práctica hemos evaluado que el <br>ID3</br> es rápido y el costo de reconstrucción es insignificante. 4. OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA. El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes. Cuando el productor recibe una solicitud del consumidor, el conjunto de aprendizaje del productor se entrena con esta solicitud como una muestra positiva. Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios. El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor. En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor. El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores. Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios. Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios. Esta lista constituye el inventario de servicios de los proveedores. Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace. Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor. Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación. Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S). Calculamos las similitudes de varias maneras, como se explicará en la Sección 5. Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud. El agente productor puede romper el empate de varias maneras. Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de <br>ID3</br> Si el productor aprende las preferencias de los consumidores con <br>ID3</br>, se aplica un mecanismo similar con dos diferencias. ",
            "candidates": [],
            "error": [
                [
                    "algoritmo ID3",
                    "ID3",
                    "ID3",
                    "algoritmo ID3",
                    "ID3",
                    "ID3",
                    "ID3"
                ]
            ]
        },
        "learning set": {
            "translated_key": "conjunto de aprendizaje",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the <br>learning set</br> of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "When producer receives a request from the consumer, the <br>learning set</br> of the producer is trained with this request as a positive sample."
            ],
            "translated_annotated_samples": [
                "Cuando el productor recibe una solicitud del consumidor, el <br>conjunto de aprendizaje</br> del productor se entrena con esta solicitud como una muestra positiva."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor. La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor. Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios. Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las preferencias del consumidor y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta. Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor. Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles. Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor. Este desafío se puede estudiar en tres etapas: • Aprendizaje de preferencias: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5) APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica. Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio. Por ejemplo, un consumidor puede tener preferencia por el vino tinto. Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo. Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo). Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor. Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores. Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa. Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo. Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6]. Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones. Investigamos particularmente dos enfoques. El primero es el aprendizaje inductivo. Esta técnica se aplica para aprender las preferencias como conceptos. Desarrollamos el Algoritmo de Eliminación de Candidatos (CEA) para el Espacio de Versiones [10]. Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva. Curiosamente, la mayoría de las veces las preferencias del consumidor son disyuntivas. Estamos considerando un agente que está comprando vino. El consumidor puede preferir vino tinto o vino rosado pero no vino blanco. Para utilizar CEA con tales preferencias, es necesaria una modificación sólida. El segundo enfoque son los árboles de decisión. Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas. Un árbol de decisión incremental bien conocido es ID5R [18]. Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional. Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados. El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender. El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener. Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto. El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo. Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío. Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo. En cada interacción entre el productor y el consumidor, tanto G como S son modificados. Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas. Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento. Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa. Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí. Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 CEA Disyuntivo Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos. Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos. Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones. Algunos de estos enfoques utilizan múltiples espacios de versión. Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7]. Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir hipótesis disyuntivas además de las conjunciones y la negación. Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica. EJEMPLO 1. Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco). El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor. Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco. En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}. Solo cuando todos los valores existan en la lista, serán reemplazados por ?. En otras palabras, permitimos que el algoritmo generalice más lentamente que antes. Modificamos el algoritmo CEA para hacer frente a este cambio. El algoritmo modificado, DCEA, se presenta como Algoritmo 1. Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de múltiples espacios de versiones. La fase de inicialización es la misma que el algoritmo original (líneas 1, 2). Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4). Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí. Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran. Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7). Cuando llega una muestra negativa, no cambiamos S como antes. Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15). A diferencia del CEA original, intentamos especializar el G mínimamente. El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13). Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada. Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez. Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14). Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar. Por ejemplo, considera el atributo del color. Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2. La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA. El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra. Después de la primera muestra positiva, S se generaliza para cubrir también la instancia. La segunda muestra es negativa. Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada. En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general. La tercera muestra es positiva y generaliza S aún más. Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco). Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis. Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}. Por el Algoritmo 1, no perdemos ninguna información. 3.3 ID3 ID3 [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor. Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos. El algoritmo ID3 se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas. Hay dos clases: positiva y negativa. Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor. Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas. El árbol de decisión tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba. El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio. Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos. Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja. El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender. Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos. Después de cada solicitud entrante, el árbol de decisiones se reconstruye. Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso. Sin embargo, en la práctica hemos evaluado que el ID3 es rápido y el costo de reconstrucción es insignificante. 4. OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA. El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes. Cuando el productor recibe una solicitud del consumidor, el <br>conjunto de aprendizaje</br> del productor se entrena con esta solicitud como una muestra positiva. Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios. El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor. En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor. El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores. Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios. Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios. Esta lista constituye el inventario de servicios de los proveedores. Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace. Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor. Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación. Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S). Calculamos las similitudes de varias maneras, como se explicará en la Sección 5. Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud. El agente productor puede romper el empate de varias maneras. Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de ID3 Si el productor aprende las preferencias de los consumidores con ID3, se aplica un mecanismo similar con dos diferencias. Primero, dado que ID3 no mantiene G, se eliminan de la lista de servicios aquellos no aceptados que se clasifican como negativos. Segundo, las similitudes de los posibles servicios no se miden con respecto a S, sino en cambio a todas las solicitudes previamente realizadas. 4.3 Mecanismos Alternativos de Oferta de Servicios Además de estos tres mecanismos de oferta de servicios (Oferta de Servicio con CEA, Oferta de Servicio con DCEA y Oferta de Servicio con ID3), incluimos otros dos mecanismos. 1304 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) • Oferta de Servicio Aleatoria (RO): El productor genera una contraoferta aleatoriamente de la lista de servicios disponibles, sin considerar las preferencias de los consumidores. • Oferta de Servicio considerando solo la solicitud actual (SCR): El productor selecciona una contraoferta de acuerdo con la similitud de la solicitud actual del consumidor pero no considera solicitudes anteriores. 5. ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una métrica de similitud que toma dos entradas y devuelve qué tan similares son. Existen varios métricos de similitud utilizados en sistemas de razonamiento basado en casos, como la suma ponderada de la distancia euclidiana, la distancia de Hamming, entre otros [12]. La métrica de similitud afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor. Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de similitud semántica llamada Similitud RP. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente. En la Ecuación (1), común representa la cantidad de atributos coincidentes, mientras que diferente representa la cantidad de atributos diferentes. Nuestra suposición actual es que α y β son iguales entre sí. SMpq = α(común) α(común) + β(diferente) (1) Aquí, al comparar dos características, asignamos cero para la disimilitud y uno para la similitud al omitir la cercanía semántica entre los valores de las características. La métrica de similitud de Tversky está diseñada para comparar dos vectores de características. En nuestro sistema, mientras que la lista de servicios que puede ofrecer el productor son cada uno un vector de características, el conjunto más específico S no es un vector de características. S consiste en hipótesis de vectores de características. Por lo tanto, estimamos la similitud de cada hipótesis dentro del conjunto más específico S y luego calculamos el promedio de las similitudes. EJEMPLO 3. Suponga que S contiene las siguientes dos hipótesis: { {Luz, Moderado, (Rojo, Blanco)} , {Completo, Fuerte, Rosa}}. Toma el servicio s como (Ligero, Resistente, Rosa). Entonces, la similitud del primero es igual a 1/3 y la del segundo es igual a 2/3 de acuerdo con la Ecuación (1). Normalmente, tomamos el promedio de ello y obtenemos (1/3 + 2/3)/2, que es igual a 1/2. Sin embargo, la primera hipótesis implica el efecto de dos solicitudes y la segunda hipótesis implica solo una solicitud. Por lo tanto, esperamos que el efecto de la primera hipótesis sea mayor que el de la segunda. Por lo tanto, calculamos la similitud promedio teniendo en cuenta la cantidad de muestras que las hipótesis cubren. Que ch denote el número de muestras que cubre la hipótesis h y (SM(h,servicio)) denote la similitud de la hipótesis h con el servicio dado. Calculamos la similitud de cada hipótesis con el servicio dado y las ponderamos con el número de muestras que cubren. Encontramos la similitud dividiendo la suma ponderada de las similitudes de todas las hipótesis en S con el servicio por el número de todas las muestras que están cubiertas en S. AV G−SM(servicio, S) = |S| |h| (ch ∗ SM(h, servicio)) |S| |h| ch (2) Figura 2: Taxonomía de muestra para estimación de similitud EJEMPLO 4. Para el ejemplo anterior, la similitud de (Luz, Fuerte, Rosa) con el conjunto específico es (2 ∗ 1/3 + 2/3)/3, igual a 4/9. El número posible de muestras que abarca una hipótesis se puede estimar multiplicando las cardinalidades de cada atributo. Por ejemplo, la cardinalidad del primer atributo es dos y la de los demás es igual a uno para la hipótesis dada, como {Luz, Moderado, (Rojo, Blanco)}. Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La métrica de similitud de Lins Un taxonomía puede ser utilizada al estimar la similitud semántica entre dos conceptos. Estimar la similitud semántica en una taxonomía de tipo Es-Un se puede hacer calculando la distancia entre los nodos relacionados con los conceptos comparados. Los enlaces entre los nodos pueden considerarse como distancias. Entonces, la longitud del camino entre los nodos indica qué tan similares son los conceptos. Una estimación alternativa para utilizar el contenido de información en la estimación de la similitud semántica en lugar del método de conteo de aristas, fue propuesta por Lin [8]. La ecuación (3) [8] muestra la similitud de Lin donde c1 y c2 son los conceptos comparados y c0 es el concepto más específico que subsume a ambos. Además, P(C) representa la probabilidad de que un objeto seleccionado arbitrariamente pertenezca al concepto C. La similitud(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Métrica de similitud de Wu y Palmers Diferente de Lin, Wu y Palmer utilizan la distancia entre los nodos en la taxonomía ES-UN [20]. La similitud semántica se representa con la Ecuación (4) [20]. Aquí, se estima la similitud entre c1 y c2 y c0 es el concepto más específico que subsume estas clases. N1 es el número de aristas entre c1 y c0. N2 es el número de aristas entre c2 y c0. N0 es el número de enlaces IS-A de c0 desde la raíz de la taxonomía. Proponemos estimar la distancia relativa en una taxonomía entre dos conceptos utilizando las siguientes intuiciones. Utilizamos la Figura 2 para ilustrar estas intuiciones. • Padre versus abuelo: El padre de un nodo es más similar al nodo que los abuelos de ese. Generalización del Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1305 es un concepto que razonablemente resulta en alejarse más de ese concepto. Cuanto más generales son los conceptos, menos similares son. Por ejemplo, AnyWineColor es el padre de ReddishColor y ReddishColor es el padre de Red. Entonces, esperamos que la similitud entre ReddishColor y Red sea mayor que la similitud entre AnyWineColor y Red. • Padre versus hermano: Un nodo tendría una similitud mayor con su padre que con su hermano. Por ejemplo, Rojo y Rosa son hijos de ColorRojo. En este caso, esperamos que la similitud entre Rojo y ColorRojizo sea mayor que la de Rojo y Rosa. • Hermano versus abuelo: Un nodo es más similar a su hermano que a su abuelo. Para ilustrar, AnyWineColor es el abuelo de Red, y Red y Rose son hermanos. Por lo tanto, posiblemente anticipamos que Rojo y Rosa son más similares que CualquierColorDeVino y Rojo. Como una taxonomía está representada en un árbol, ese árbol puede ser recorrido desde el primer concepto que se está comparando hasta el segundo concepto. En el nodo inicial relacionado con el primer concepto, el valor de similitud es constante y igual a uno. Este valor se reduce por una constante en cada nodo visitado a lo largo del camino que llegará al nodo que incluye el segundo concepto. Cuanto más corto sea el camino entre los conceptos, mayor será la similitud entre los nodos. Algoritmo 2 Estimar-Similitud-RP(c1,c2) Requerido: Las constantes deben ser m > n > m2 donde m, n ∈ R[0, 1] 1: Similitud ← 1 2: si c1 es igual a c2 entonces 3: Devolver Similitud 4: fin si 5: padreComun ← encontrarPadreComun(c1, c2) {padreComun es el concepto más específico que cubre tanto c1 como c2} 6: N1 ← encontrarDistancia(padreComun, c1) 7: N2 ← encontrarDistancia(padreComun, c2) {N1 y N2 son el número de enlaces entre el concepto y el concepto padre} 8: si (padreComun == c1) o (padreComun == c2) entonces 9: Similitud ← Similitud ∗ m(N1+N2) 10: sino 11: Similitud ← Similitud ∗ n ∗ m(N1+N2−2) 12: fin si 13: Devolver Similitud La distancia relativa entre los nodos c1 y c2 se estima de la siguiente manera. Comenzando desde c1, se recorre el árbol para llegar a c2. En cada salto, la similitud disminuye ya que los conceptos se están alejando cada vez más entre sí. Sin embargo, según nuestras intuiciones, no todos los saltos disminuyen la similitud de igual manera. Que m represente el factor para saltar de un hijo a un padre y que n represente el factor para saltar de un hermano a otro hermano. Dado que saltar de un nodo a su abuelo cuenta como dos saltos de padre, el factor de descuento al moverse de un nodo a su abuelo es m2. De acuerdo con las intuiciones anteriores, nuestras constantes deben estar en la forma m > n > m2 donde el valor de m y n debe estar entre cero y uno. El algoritmo 2 muestra el cálculo de la distancia. Según el algoritmo, en primer lugar la similitud se inicializa con el valor de uno (línea 1). Si los conceptos son iguales entre sí, entonces la similitud será uno (líneas 2-4). De lo contrario, calculamos el ancestro común de los dos nodos y la distancia de cada concepto al ancestro común sin considerar al hermano (líneas 5-7). Si uno de los conceptos es igual al padre común, entonces no hay relación de hermanos entre los conceptos. Para cada nivel, multiplicamos la similitud por m y no consideramos el factor de hermanos en la estimación de la similitud. Como resultado, disminuimos la similitud en cada nivel con la tasa de m (línea 9). De lo contrario, tiene que existir una relación de hermanos. Esto significa que debemos considerar el efecto de n al medir la similitud. Recuerde que hemos contado N1+N2 aristas entre los conceptos. Dado que existe una relación de hermanos, dos de estos bordes constituyen la relación de hermanos. Por lo tanto, al calcular el efecto de la relación parental, utilizamos N1+N2 −2 aristas (línea 11). Algunas estimaciones de similitud relacionadas con la taxonomía en la Figura 2 se presentan en la Tabla 2. En este ejemplo, se toma m como 2/3 y n como 4/7. Tabla 2: Estimación de similitud de muestra sobre la taxonomía de muestra. Similitud(ColorRojo, Rosa) = 1 ∗ (2/3) = 0.6666667 Similitud(Rojo, Rosa) = 1 ∗ (4/7) = 0.5714286 Similitud(CualquierColorVino, Rosa) = 1 ∗ (2/3)2 = 0.44444445 Similitud(Blanco, Rosa) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 Para todas las métricas de similitud semántica en nuestra arquitectura, la taxonomía de características se mantiene en la ontología compartida. Para evaluar la similitud del vector de características, primero estimamos la similitud para cada característica individualmente y luego calculamos la suma promedio de estas similitudes. Entonces, el resultado es igual a la similitud semántica promedio de todo el vector de características. 6. SISTEMA DESARROLLADO Hemos implementado nuestra arquitectura en Java. Para facilitar las pruebas del sistema, el agente del consumidor tiene una interfaz de usuario que nos permite ingresar varias solicitudes. El agente productor está completamente automatizado y las operaciones de aprendizaje y oferta de servicios funcionan como se explicó anteriormente. En esta sección, explicamos los detalles de implementación del sistema desarrollado. Utilizamos OWL [11] como nuestro lenguaje de ontología y JENA como nuestro razonador de ontología. La ontología compartida es la versión modificada de la Ontología del Vino [19]. Incluye la descripción del vino como concepto y diferentes tipos de vino. Todos los participantes de la negociación utilizan esta ontología para entenderse mutuamente. Según la ontología, siete propiedades conforman el concepto de vino. El agente consumidor y el agente productor obtienen los valores posibles para estas propiedades consultando la ontología. Por lo tanto, todos los valores posibles para los componentes del concepto del vino, como el color, cuerpo, azúcar, etc., pueden ser alcanzados por ambos agentes. También se describen en esta ontología una variedad de tipos de vino como Borgoña, Chardonnay, Chenin Blanc, entre otros. Intuitivamente, cualquier tipo de vino descrito en la ontología también representa un concepto de vino. Esto nos permite considerar las instancias de vino Chardonnay como instancias de la clase Vino. Además de la descripción del vino, la información jerárquica de algunas características se puede inferir de la ontología. Por ejemplo, podemos representar la información de que el continente europeo abarca países occidentales. El país occidental abarca la región francesa, que incluye algunos territorios como el Loira, Burdeos, entre otros. Esta información jerárquica se utiliza en la estimación de similitud semántica. En esta parte, se pueden hacer algunos razonamientos como si un concepto X abarca Y y Y abarca Z, entonces el concepto X abarca Z. Por ejemplo, el Continente Europeo abarca Burdeos. 1306 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Para algunas características como cuerpo, sabor y azúcar, no hay información jerárquica, pero sus valores están nivelados semánticamente. Cuando eso sucede, proporcionamos los valores de similitud razonables para estas características. Por ejemplo, el cuerpo puede ser ligero, medio o fuerte. En este caso, asumimos que la luz es 0.66 similar a media pero solo 0.33 a fuerte. La ontología de WineStock es el inventario de los productores y describe una clase de producto como WineProduct. Esta clase es necesaria para que el productor registre los vinos que vende. La ontología implica a los individuos de esta clase. Los individuos representan los servicios disponibles que posee el productor. Hemos preparado dos ontologías de WineStock separadas para realizar pruebas. En la primera ontología, hay 19 productos de vino disponibles y en la segunda ontología, hay 50 productos. EVALUACIÓN DEL RENDIMIENTO Evaluamos el rendimiento de los sistemas propuestos en relación con la técnica de aprendizaje que utilizaron, DCEA e ID3, comparándolos con CEA, RO (para oferta aleatoria) y SCR (oferta basada solo en la solicitud actual). Aplicamos una variedad de escenarios en este conjunto de datos para ver las diferencias de rendimiento. Cada escenario de prueba contiene una lista de preferencias para el usuario y el número de coincidencias de la lista de productos. La Tabla 3 muestra estas preferencias y la disponibilidad de esos productos en el inventario para los primeros cinco escenarios. Ten en cuenta que estas preferencias son internas al consumidor y el productor intenta aprenderlas durante la negociación. Tabla 3: Disponibilidad de vinos en diferentes escenarios de prueba ID Preferencia del consumidor Disponibilidad (de 19) 1 Vino seco 15 2 Vino tinto y seco 8 3 Vino tinto, seco y moderado 4 4 Vino tinto y fuerte 2 5 Vino tinto o rosado, y fuerte 3 7.1 Comparación de Algoritmos de Aprendizaje En la comparación de algoritmos de aprendizaje, utilizamos los cinco escenarios de la Tabla 3. Aquí, primero usamos la medida de similitud de Tversky. Con estos casos de prueba, estamos interesados en encontrar el número de iteraciones que se requieren para que el productor genere una oferta aceptable para el consumidor. Dado que el rendimiento también depende de la solicitud inicial, repetimos nuestros experimentos con diferentes solicitudes iniciales. Por consiguiente, para cada caso, ejecutamos los algoritmos cinco veces con varias variaciones de las solicitudes iniciales. En cada experimento, contamos el número de iteraciones necesarias para llegar a un acuerdo. Tomamos el promedio de estos números para evaluar estos sistemas de manera justa. Como es costumbre, probamos cada algoritmo con las mismas solicitudes iniciales. La Tabla 4 compara los enfoques utilizando diferentes algoritmos de aprendizaje. Cuando las partes grandes del inventario son compatibles con las preferencias de los clientes, como en el primer caso de prueba, el rendimiento de todas las técnicas es casi el mismo (por ejemplo, Escenario 1). A medida que el número de servicios compatibles disminuye, RO funciona mal como se esperaba. El segundo peor método es SCR ya que solo considera la solicitud más reciente de los clientes y no aprende de las solicitudes anteriores. CEA da los mejores resultados cuando puede generar una respuesta pero no puede manejar los casos que contienen preferencias disyuntivas, como el que se presenta en el Escenario 5. ID3 y DCEA logran los mejores resultados. Su rendimiento es comparable y pueden manejar todos los casos, incluido el Escenario 5. Tabla 4: Comparación de algoritmos de aprendizaje en términos del número promedio de interacciones. Ejecutar DCEA SCR RO CEA ID3 Escenario 1: 1.2 1.4 1.2 1.2 1.2 Escenario 2: 1.4 1.4 2.6 1.4 1.4 Escenario 3: 1.4 1.8 4.4 1.4 1.4 Escenario 4: 2.2 2.8 9.6 1.8 2 Escenario 5: 2 2.6 7.6 1.75+ Sin oferta 1.8 Promedio de todos los casos: 1.64 2 5.08 1.51+Sin oferta 1.56 7.2 Comparación de Métricas de Similitud Para comparar las métricas de similitud que se explicaron en la Sección 5, fijamos el algoritmo de aprendizaje en DCEA. Además de los escenarios mostrados en la Tabla 3, agregamos los siguientes cinco nuevos escenarios considerando la información jerárquica. • El cliente desea comprar vino cuya bodega esté ubicada en California y cuya uva sea de tipo blanco. Además, la bodega del vino no debería ser costosa. Solo hay cuatro productos que cumplen con estas condiciones. • El cliente quiere comprar vino de color rojo o rosado y de tipo de uva tinta. Además, la ubicación del vino debe ser en Europa. Se desea que el grado de dulzura sea seco o semiseco. El sabor debe ser delicado o moderado, mientras que el cuerpo debe ser medio o ligero. Además, la bodega del vino debería ser una bodega cara. Hay dos productos que cumplen con todos estos requisitos. El cliente quiere comprar vino rosado moderado, que se encuentra alrededor de la región francesa. La categoría de bodega debería ser Bodega Moderada. Solo hay un producto que cumple con estos requisitos. • El cliente quiere comprar vino tinto caro, que se encuentra alrededor de la Región de California o vino blanco barato, que se encuentra alrededor de la Región de Texas. Hay cinco productos disponibles. • El cliente quiere comprar un vino blanco delicado cuyo productor esté en la categoría de Bodega Costosa. Hay dos productos disponibles. Los primeros siete escenarios se prueban con el primer conjunto de datos que contiene un total de 19 servicios y los últimos tres escenarios se prueban con el segundo conjunto de datos que contiene 50 servicios. La Tabla 5 muestra la evaluación del rendimiento en términos del número de interacciones necesarias para llegar a un consenso. La métrica de Tversky da los peores resultados ya que no considera la similitud semántica. El rendimiento de Lins es mejor que el de Tversky pero peor que el de otros. La métrica de Wu-Palmer y la medida de similitud de RP casi ofrecen el mismo rendimiento y son mejores que otras. Cuando se examinan los resultados, considerar la cercanía semántica aumenta el rendimiento. 8. DISCUSIÓN Revisamos la literatura reciente en comparación con nuestro trabajo. Tama et al. [16] proponen un nuevo enfoque basado en ontología para la negociación. Según su enfoque, los protocolos de negociación utilizados en el comercio electrónico pueden ser modelados como ontologías. Por lo tanto, los agentes pueden llevar a cabo un protocolo de negociación utilizando esta ontología compartida sin necesidad de estar codificados con los detalles del protocolo de negociación. Mientras tanto, la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1307 Tabla 5: Comparación de métricas de similitud en términos de número de interacciones. Ejecutar Tversky Lin Wu Palmer RP Escenario 1: 1.2 1.2 1 1 Escenario 2: 1.4 1.4 1.6 1.6 Escenario 3: 1.4 1.8 2 2 Escenario 4: 2.2 1 1.2 1.2 Escenario 5: 2 1.6 1.6 1.6 Escenario 6: 5 3.8 2.4 2.6 Escenario 7: 3.2 1.2 1 1 Escenario 8: 5.6 2 2 2.2 Escenario 9: 2.6 2.2 2.2 2.6 Escenario 10: 4.4 2 2 1.8 Promedio de todos los casos: 2.9 1.82 1.7 1.76 Tama et al. modelan el protocolo de negociación utilizando ontologías, en cambio, nosotros hemos modelado el servicio a ser negociado. Además, hemos construido un sistema con el cual se pueden aprender las preferencias de negociación. El estudio de Sadri et al. analiza la negociación en el contexto de la asignación de recursos [14]. Los agentes tienen recursos limitados y necesitan solicitar recursos faltantes a otros agentes. Se propone un mecanismo basado en secuencias de diálogo entre agentes como solución. El mecanismo se basa en el ciclo de agente de observar-pensar-actuar. Estos diálogos incluyen ofrecer recursos, intercambios de recursos y ofrecer recursos alternativos. Cada agente en el sistema planea sus acciones para alcanzar un estado objetivo. A diferencia de nuestro enfoque, el estudio de Sadri et al. no se preocupa por las preferencias de aprendizaje mutuas. Brzostowski y Kowalczyk proponen un enfoque para seleccionar un socio de negociación adecuado investigando negociaciones previas de múltiples atributos [1]. Para lograr esto, utilizan el razonamiento basado en casos. Su enfoque es probabilístico ya que el comportamiento de los socios puede cambiar en cada iteración. En nuestro enfoque, estamos interesados en negociar el contenido del servicio. Después de que el consumidor y el productor acuerden el servicio, se pueden utilizar mecanismos de negociación orientados al precio para acordar el precio. Fatima et al. estudian los factores que afectan la negociación, como las preferencias, el plazo, el precio, entre otros, ya que el agente que desarrolla una estrategia contra su oponente debe considerar todos ellos [5]. En su enfoque, el objetivo del agente vendedor es vender el servicio al precio más alto posible, mientras que el objetivo del agente comprador es comprar el bien al precio más bajo posible. El intervalo de tiempo afecta a estos agentes de manera diferente. En comparación con Fatima et al., nuestro enfoque es diferente. Mientras ellos estudian el efecto del tiempo en la negociación, nuestro enfoque está en aprender las preferencias para una negociación exitosa. Faratin et al. proponen un mecanismo de negociación multi-tema, donde las variables de servicio para la negociación, como el precio, la calidad del servicio, entre otros, se consideran intercambios entre sí (es decir, un precio más alto por una entrega más temprana) [4]. Generan un modelo heurístico para compensaciones que incluye la estimación de similitud difusa y una exploración de escalada de colina para ofertas posiblemente aceptables. Aunque abordamos un problema similar, aprendemos las preferencias del cliente con la ayuda del aprendizaje inductivo y generamos contraofertas de acuerdo con estas preferencias aprendidas. Faratin et al. solo utilizan la última oferta realizada por el consumidor al calcular la similitud para elegir la contraoferta. A diferencia de ellos, también tenemos en cuenta las solicitudes previas del consumidor. En sus experimentos, Faratin et al. asumen que los pesos de las variables de servicio están fijos a priori. Por el contrario, aprendemos estas preferencias con el tiempo. En nuestro trabajo futuro, planeamos integrar el razonamiento ontológico en el algoritmo de aprendizaje para que la información jerárquica pueda ser aprendida a partir de la jerarquía de subsumpción de relaciones. Además, al utilizar las relaciones entre las características, el productor puede descubrir nuevos conocimientos a partir de los conocimientos existentes. Estas son direcciones interesantes que seguiremos en nuestro trabajo futuro. 9. REFERENCIAS [1] J. Brzostowski y R. Kowalczyk. En el razonamiento basado en casos posibilístico para la selección de socios para la negociación de agentes de múltiples atributos. En Actas del 4to Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 273-278, 2005. [2] L. Busch e I. Horstman. Un comentario sobre negociaciones tema por tema. Juegos y Comportamiento Económico, 19:144-148, 1997. [3] J. K. Debenham. Gestión de la negociación en el mercado electrónico en el contexto de un sistema multiagente. En Actas de la 21ª Conferencia Internacional sobre Sistemas Basados en el Conocimiento e Inteligencia Artificial Aplicada, ES2002:, 2002. [4] P. Faratin, C. Sierra y N. R. Jennings. Utilizando criterios de similitud para hacer compensaciones de problemas en negociaciones automatizadas. Inteligencia Artificial, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge y N. Jennings. Agentes óptimos para negociaciones de múltiples temas. En Actas del 2do Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 129-136, 2003. [6] C. Giraud-Carrier. Una nota sobre la utilidad del aprendizaje incremental. Comunicaciones de IA, 13(4):215-223, 2000. [7] T.-P. Hong y S.-S. Tseng. Dividiendo y fusionando espacios de versiones para aprender conceptos disyuntivos. IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.\n\nTraducción al español:\nIEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin. Una definición de similitud basada en teoría de la información. En Actas de la 15ª Conferencia Internacional sobre Aprendizaje Automático, páginas 296-304. Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, y A. G. Moukas. Agentes que compran y venden. Comunicaciones de la ACM, 42(3):81-91, 1999. [10] T. M. Mitchell. Aprendizaje automático. McGraw Hill, NY, 1997. [11] Búho. OWL: Guía del lenguaje de ontologías web, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal y S. C. K. Shiu. Fundamentos del Razonamiento Basado en Casos Blandos. John Wiley & Sons, Nueva Jersey, 2004. [13] J. R. Quinlan. Inducción de árboles de decisión. Aprendizaje automático, 1(1):81-106, 1986. [14] F. Sadri, F. Toni y P. Torroni. Diálogos para negociación: Variedades de agentes y secuencias de diálogo. En ATAL 2001, Artículos Revisados, volumen 2333 de LNAI, páginas 405-421. Springer-Verlag, 2002. [15] M. P. Singh. \n\nSpringer-Verlag, 2002. [15] M. P. Singh. Comercio electrónico orientado al valor. IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, y M. Wooldridge. Ontologías para apoyar la negociación en el comercio electrónico. Aplicaciones de la Inteligencia Artificial en Ingeniería, 18:223-236, 2005. [17] A. Tversky. Características de similitud. Revisión Psicológica, 84(4):327-352, 1977. [18] P. E. Utgoff. Inducción incremental de árboles de decisión. Aprendizaje automático, 4:161-186, 1989. [19] Vino, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu y M. Palmer. Semántica de verbos y selección léxica. En el 32. Reunión anual de la Asociación de Lingüística Computacional, páginas 133-138, 1994. 1308 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "similarity metric": {
            "translated_key": "métrica de similitud",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a <br>similarity metric</br> that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The <br>similarity metric</br> affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic <br>similarity metric</br> named RP Similarity. 5.1 Tverskys <br>similarity metric</br> Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys <br>similarity metric</br> is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins <br>similarity metric</br> A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers <br>similarity metric</br> Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "SIMILARITY ESTIMATION Similarity can be estimated with a <br>similarity metric</br> that takes two entries and returns how similar they are.",
                "The <br>similarity metric</br> affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic <br>similarity metric</br> named RP Similarity. 5.1 Tverskys <br>similarity metric</br> Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "Tverskys <br>similarity metric</br> is designed to compare two feature vectors.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins <br>similarity metric</br> A taxonomy can be used while estimating semantic similarity between two concepts."
            ],
            "translated_annotated_samples": [
                "ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una <br>métrica de similitud</br> que toma dos entradas y devuelve qué tan similares son.",
                "La <br>métrica de similitud</br> afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor.",
                "Primero analizamos algunas métricas existentes y luego proponemos una nueva <br>métrica de similitud</br> semántica llamada Similitud RP. La <br>métrica de similitud</br> de Tversky compara dos vectores en términos del número de características que coinciden exactamente.",
                "La <br>métrica de similitud</br> de Tversky está diseñada para comparar dos vectores de características.",
                "Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La <br>métrica de similitud</br> de Lins Un taxonomía puede ser utilizada al estimar la similitud semántica entre dos conceptos."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor. La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor. Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios. Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las preferencias del consumidor y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta. Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor. Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles. Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor. Este desafío se puede estudiar en tres etapas: • Aprendizaje de preferencias: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5) APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica. Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio. Por ejemplo, un consumidor puede tener preferencia por el vino tinto. Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo. Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo). Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor. Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores. Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa. Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo. Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6]. Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones. Investigamos particularmente dos enfoques. El primero es el aprendizaje inductivo. Esta técnica se aplica para aprender las preferencias como conceptos. Desarrollamos el Algoritmo de Eliminación de Candidatos (CEA) para el Espacio de Versiones [10]. Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva. Curiosamente, la mayoría de las veces las preferencias del consumidor son disyuntivas. Estamos considerando un agente que está comprando vino. El consumidor puede preferir vino tinto o vino rosado pero no vino blanco. Para utilizar CEA con tales preferencias, es necesaria una modificación sólida. El segundo enfoque son los árboles de decisión. Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas. Un árbol de decisión incremental bien conocido es ID5R [18]. Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional. Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados. El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender. El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener. Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto. El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo. Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío. Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo. En cada interacción entre el productor y el consumidor, tanto G como S son modificados. Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas. Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento. Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa. Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí. Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 CEA Disyuntivo Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos. Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos. Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones. Algunos de estos enfoques utilizan múltiples espacios de versión. Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7]. Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir hipótesis disyuntivas además de las conjunciones y la negación. Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica. EJEMPLO 1. Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco). El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor. Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco. En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}. Solo cuando todos los valores existan en la lista, serán reemplazados por ?. En otras palabras, permitimos que el algoritmo generalice más lentamente que antes. Modificamos el algoritmo CEA para hacer frente a este cambio. El algoritmo modificado, DCEA, se presenta como Algoritmo 1. Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de múltiples espacios de versiones. La fase de inicialización es la misma que el algoritmo original (líneas 1, 2). Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4). Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí. Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran. Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7). Cuando llega una muestra negativa, no cambiamos S como antes. Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15). A diferencia del CEA original, intentamos especializar el G mínimamente. El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13). Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada. Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez. Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14). Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar. Por ejemplo, considera el atributo del color. Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2. La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA. El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra. Después de la primera muestra positiva, S se generaliza para cubrir también la instancia. La segunda muestra es negativa. Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada. En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general. La tercera muestra es positiva y generaliza S aún más. Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco). Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis. Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}. Por el Algoritmo 1, no perdemos ninguna información. 3.3 ID3 ID3 [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor. Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos. El algoritmo ID3 se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas. Hay dos clases: positiva y negativa. Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor. Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas. El árbol de decisión tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba. El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio. Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos. Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja. El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender. Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos. Después de cada solicitud entrante, el árbol de decisiones se reconstruye. Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso. Sin embargo, en la práctica hemos evaluado que el ID3 es rápido y el costo de reconstrucción es insignificante. 4. OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA. El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes. Cuando el productor recibe una solicitud del consumidor, el conjunto de aprendizaje del productor se entrena con esta solicitud como una muestra positiva. Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios. El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor. En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor. El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores. Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios. Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios. Esta lista constituye el inventario de servicios de los proveedores. Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace. Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor. Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación. Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S). Calculamos las similitudes de varias maneras, como se explicará en la Sección 5. Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud. El agente productor puede romper el empate de varias maneras. Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de ID3 Si el productor aprende las preferencias de los consumidores con ID3, se aplica un mecanismo similar con dos diferencias. Primero, dado que ID3 no mantiene G, se eliminan de la lista de servicios aquellos no aceptados que se clasifican como negativos. Segundo, las similitudes de los posibles servicios no se miden con respecto a S, sino en cambio a todas las solicitudes previamente realizadas. 4.3 Mecanismos Alternativos de Oferta de Servicios Además de estos tres mecanismos de oferta de servicios (Oferta de Servicio con CEA, Oferta de Servicio con DCEA y Oferta de Servicio con ID3), incluimos otros dos mecanismos. 1304 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) • Oferta de Servicio Aleatoria (RO): El productor genera una contraoferta aleatoriamente de la lista de servicios disponibles, sin considerar las preferencias de los consumidores. • Oferta de Servicio considerando solo la solicitud actual (SCR): El productor selecciona una contraoferta de acuerdo con la similitud de la solicitud actual del consumidor pero no considera solicitudes anteriores. 5. ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una <br>métrica de similitud</br> que toma dos entradas y devuelve qué tan similares son. Existen varios métricos de similitud utilizados en sistemas de razonamiento basado en casos, como la suma ponderada de la distancia euclidiana, la distancia de Hamming, entre otros [12]. La <br>métrica de similitud</br> afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor. Primero analizamos algunas métricas existentes y luego proponemos una nueva <br>métrica de similitud</br> semántica llamada Similitud RP. La <br>métrica de similitud</br> de Tversky compara dos vectores en términos del número de características que coinciden exactamente. En la Ecuación (1), común representa la cantidad de atributos coincidentes, mientras que diferente representa la cantidad de atributos diferentes. Nuestra suposición actual es que α y β son iguales entre sí. SMpq = α(común) α(común) + β(diferente) (1) Aquí, al comparar dos características, asignamos cero para la disimilitud y uno para la similitud al omitir la cercanía semántica entre los valores de las características. La <br>métrica de similitud</br> de Tversky está diseñada para comparar dos vectores de características. En nuestro sistema, mientras que la lista de servicios que puede ofrecer el productor son cada uno un vector de características, el conjunto más específico S no es un vector de características. S consiste en hipótesis de vectores de características. Por lo tanto, estimamos la similitud de cada hipótesis dentro del conjunto más específico S y luego calculamos el promedio de las similitudes. EJEMPLO 3. Suponga que S contiene las siguientes dos hipótesis: { {Luz, Moderado, (Rojo, Blanco)} , {Completo, Fuerte, Rosa}}. Toma el servicio s como (Ligero, Resistente, Rosa). Entonces, la similitud del primero es igual a 1/3 y la del segundo es igual a 2/3 de acuerdo con la Ecuación (1). Normalmente, tomamos el promedio de ello y obtenemos (1/3 + 2/3)/2, que es igual a 1/2. Sin embargo, la primera hipótesis implica el efecto de dos solicitudes y la segunda hipótesis implica solo una solicitud. Por lo tanto, esperamos que el efecto de la primera hipótesis sea mayor que el de la segunda. Por lo tanto, calculamos la similitud promedio teniendo en cuenta la cantidad de muestras que las hipótesis cubren. Que ch denote el número de muestras que cubre la hipótesis h y (SM(h,servicio)) denote la similitud de la hipótesis h con el servicio dado. Calculamos la similitud de cada hipótesis con el servicio dado y las ponderamos con el número de muestras que cubren. Encontramos la similitud dividiendo la suma ponderada de las similitudes de todas las hipótesis en S con el servicio por el número de todas las muestras que están cubiertas en S. AV G−SM(servicio, S) = |S| |h| (ch ∗ SM(h, servicio)) |S| |h| ch (2) Figura 2: Taxonomía de muestra para estimación de similitud EJEMPLO 4. Para el ejemplo anterior, la similitud de (Luz, Fuerte, Rosa) con el conjunto específico es (2 ∗ 1/3 + 2/3)/3, igual a 4/9. El número posible de muestras que abarca una hipótesis se puede estimar multiplicando las cardinalidades de cada atributo. Por ejemplo, la cardinalidad del primer atributo es dos y la de los demás es igual a uno para la hipótesis dada, como {Luz, Moderado, (Rojo, Blanco)}. Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La <br>métrica de similitud</br> de Lins Un taxonomía puede ser utilizada al estimar la similitud semántica entre dos conceptos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "rp similarity": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named <br>rp similarity</br>. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and <br>rp similarity</br> measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "We first analyze some existing metrics and then propose a new semantic similarity metric named <br>rp similarity</br>. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "Wu Palmers metric and <br>rp similarity</br> measure nearly give the same performance and better than others."
            ],
            "translated_annotated_samples": [
                "Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de similitud semántica llamada <br>Similitud RP</br>. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente.",
                "La métrica de Wu-Palmer y la medida de <br>similitud de RP</br> casi ofrecen el mismo rendimiento y son mejores que otras."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor. La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor. Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios. Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las preferencias del consumidor y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta. Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor. Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles. Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor. Este desafío se puede estudiar en tres etapas: • Aprendizaje de preferencias: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5) APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica. Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio. Por ejemplo, un consumidor puede tener preferencia por el vino tinto. Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo. Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo). Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor. Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores. Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa. Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo. Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6]. Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones. Investigamos particularmente dos enfoques. El primero es el aprendizaje inductivo. Esta técnica se aplica para aprender las preferencias como conceptos. Desarrollamos el Algoritmo de Eliminación de Candidatos (CEA) para el Espacio de Versiones [10]. Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva. Curiosamente, la mayoría de las veces las preferencias del consumidor son disyuntivas. Estamos considerando un agente que está comprando vino. El consumidor puede preferir vino tinto o vino rosado pero no vino blanco. Para utilizar CEA con tales preferencias, es necesaria una modificación sólida. El segundo enfoque son los árboles de decisión. Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas. Un árbol de decisión incremental bien conocido es ID5R [18]. Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional. Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados. El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender. El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener. Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto. El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo. Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío. Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo. En cada interacción entre el productor y el consumidor, tanto G como S son modificados. Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas. Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento. Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa. Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí. Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 CEA Disyuntivo Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos. Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos. Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones. Algunos de estos enfoques utilizan múltiples espacios de versión. Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7]. Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir hipótesis disyuntivas además de las conjunciones y la negación. Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica. EJEMPLO 1. Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco). El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor. Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco. En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}. Solo cuando todos los valores existan en la lista, serán reemplazados por ?. En otras palabras, permitimos que el algoritmo generalice más lentamente que antes. Modificamos el algoritmo CEA para hacer frente a este cambio. El algoritmo modificado, DCEA, se presenta como Algoritmo 1. Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de múltiples espacios de versiones. La fase de inicialización es la misma que el algoritmo original (líneas 1, 2). Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4). Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí. Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran. Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7). Cuando llega una muestra negativa, no cambiamos S como antes. Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15). A diferencia del CEA original, intentamos especializar el G mínimamente. El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13). Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada. Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez. Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14). Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar. Por ejemplo, considera el atributo del color. Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2. La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA. El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra. Después de la primera muestra positiva, S se generaliza para cubrir también la instancia. La segunda muestra es negativa. Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada. En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general. La tercera muestra es positiva y generaliza S aún más. Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco). Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis. Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}. Por el Algoritmo 1, no perdemos ninguna información. 3.3 ID3 ID3 [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor. Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos. El algoritmo ID3 se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas. Hay dos clases: positiva y negativa. Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor. Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas. El árbol de decisión tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba. El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio. Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos. Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja. El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender. Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos. Después de cada solicitud entrante, el árbol de decisiones se reconstruye. Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso. Sin embargo, en la práctica hemos evaluado que el ID3 es rápido y el costo de reconstrucción es insignificante. 4. OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA. El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes. Cuando el productor recibe una solicitud del consumidor, el conjunto de aprendizaje del productor se entrena con esta solicitud como una muestra positiva. Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios. El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor. En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor. El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores. Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios. Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios. Esta lista constituye el inventario de servicios de los proveedores. Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace. Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor. Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación. Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S). Calculamos las similitudes de varias maneras, como se explicará en la Sección 5. Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud. El agente productor puede romper el empate de varias maneras. Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de ID3 Si el productor aprende las preferencias de los consumidores con ID3, se aplica un mecanismo similar con dos diferencias. Primero, dado que ID3 no mantiene G, se eliminan de la lista de servicios aquellos no aceptados que se clasifican como negativos. Segundo, las similitudes de los posibles servicios no se miden con respecto a S, sino en cambio a todas las solicitudes previamente realizadas. 4.3 Mecanismos Alternativos de Oferta de Servicios Además de estos tres mecanismos de oferta de servicios (Oferta de Servicio con CEA, Oferta de Servicio con DCEA y Oferta de Servicio con ID3), incluimos otros dos mecanismos. 1304 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) • Oferta de Servicio Aleatoria (RO): El productor genera una contraoferta aleatoriamente de la lista de servicios disponibles, sin considerar las preferencias de los consumidores. • Oferta de Servicio considerando solo la solicitud actual (SCR): El productor selecciona una contraoferta de acuerdo con la similitud de la solicitud actual del consumidor pero no considera solicitudes anteriores. 5. ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una métrica de similitud que toma dos entradas y devuelve qué tan similares son. Existen varios métricos de similitud utilizados en sistemas de razonamiento basado en casos, como la suma ponderada de la distancia euclidiana, la distancia de Hamming, entre otros [12]. La métrica de similitud afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor. Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de similitud semántica llamada <br>Similitud RP</br>. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente. En la Ecuación (1), común representa la cantidad de atributos coincidentes, mientras que diferente representa la cantidad de atributos diferentes. Nuestra suposición actual es que α y β son iguales entre sí. SMpq = α(común) α(común) + β(diferente) (1) Aquí, al comparar dos características, asignamos cero para la disimilitud y uno para la similitud al omitir la cercanía semántica entre los valores de las características. La métrica de similitud de Tversky está diseñada para comparar dos vectores de características. En nuestro sistema, mientras que la lista de servicios que puede ofrecer el productor son cada uno un vector de características, el conjunto más específico S no es un vector de características. S consiste en hipótesis de vectores de características. Por lo tanto, estimamos la similitud de cada hipótesis dentro del conjunto más específico S y luego calculamos el promedio de las similitudes. EJEMPLO 3. Suponga que S contiene las siguientes dos hipótesis: { {Luz, Moderado, (Rojo, Blanco)} , {Completo, Fuerte, Rosa}}. Toma el servicio s como (Ligero, Resistente, Rosa). Entonces, la similitud del primero es igual a 1/3 y la del segundo es igual a 2/3 de acuerdo con la Ecuación (1). Normalmente, tomamos el promedio de ello y obtenemos (1/3 + 2/3)/2, que es igual a 1/2. Sin embargo, la primera hipótesis implica el efecto de dos solicitudes y la segunda hipótesis implica solo una solicitud. Por lo tanto, esperamos que el efecto de la primera hipótesis sea mayor que el de la segunda. Por lo tanto, calculamos la similitud promedio teniendo en cuenta la cantidad de muestras que las hipótesis cubren. Que ch denote el número de muestras que cubre la hipótesis h y (SM(h,servicio)) denote la similitud de la hipótesis h con el servicio dado. Calculamos la similitud de cada hipótesis con el servicio dado y las ponderamos con el número de muestras que cubren. Encontramos la similitud dividiendo la suma ponderada de las similitudes de todas las hipótesis en S con el servicio por el número de todas las muestras que están cubiertas en S. AV G−SM(servicio, S) = |S| |h| (ch ∗ SM(h, servicio)) |S| |h| ch (2) Figura 2: Taxonomía de muestra para estimación de similitud EJEMPLO 4. Para el ejemplo anterior, la similitud de (Luz, Fuerte, Rosa) con el conjunto específico es (2 ∗ 1/3 + 2/3)/3, igual a 4/9. El número posible de muestras que abarca una hipótesis se puede estimar multiplicando las cardinalidades de cada atributo. Por ejemplo, la cardinalidad del primer atributo es dos y la de los demás es igual a uno para la hipótesis dada, como {Luz, Moderado, (Rojo, Blanco)}. Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La métrica de similitud de Lins Un taxonomía puede ser utilizada al estimar la similitud semántica entre dos conceptos. Estimar la similitud semántica en una taxonomía de tipo Es-Un se puede hacer calculando la distancia entre los nodos relacionados con los conceptos comparados. Los enlaces entre los nodos pueden considerarse como distancias. Entonces, la longitud del camino entre los nodos indica qué tan similares son los conceptos. Una estimación alternativa para utilizar el contenido de información en la estimación de la similitud semántica en lugar del método de conteo de aristas, fue propuesta por Lin [8]. La ecuación (3) [8] muestra la similitud de Lin donde c1 y c2 son los conceptos comparados y c0 es el concepto más específico que subsume a ambos. Además, P(C) representa la probabilidad de que un objeto seleccionado arbitrariamente pertenezca al concepto C. La similitud(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Métrica de similitud de Wu y Palmers Diferente de Lin, Wu y Palmer utilizan la distancia entre los nodos en la taxonomía ES-UN [20]. La similitud semántica se representa con la Ecuación (4) [20]. Aquí, se estima la similitud entre c1 y c2 y c0 es el concepto más específico que subsume estas clases. N1 es el número de aristas entre c1 y c0. N2 es el número de aristas entre c2 y c0. N0 es el número de enlaces IS-A de c0 desde la raíz de la taxonomía. Proponemos estimar la distancia relativa en una taxonomía entre dos conceptos utilizando las siguientes intuiciones. Utilizamos la Figura 2 para ilustrar estas intuiciones. • Padre versus abuelo: El padre de un nodo es más similar al nodo que los abuelos de ese. Generalización del Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1305 es un concepto que razonablemente resulta en alejarse más de ese concepto. Cuanto más generales son los conceptos, menos similares son. Por ejemplo, AnyWineColor es el padre de ReddishColor y ReddishColor es el padre de Red. Entonces, esperamos que la similitud entre ReddishColor y Red sea mayor que la similitud entre AnyWineColor y Red. • Padre versus hermano: Un nodo tendría una similitud mayor con su padre que con su hermano. Por ejemplo, Rojo y Rosa son hijos de ColorRojo. En este caso, esperamos que la similitud entre Rojo y ColorRojizo sea mayor que la de Rojo y Rosa. • Hermano versus abuelo: Un nodo es más similar a su hermano que a su abuelo. Para ilustrar, AnyWineColor es el abuelo de Red, y Red y Rose son hermanos. Por lo tanto, posiblemente anticipamos que Rojo y Rosa son más similares que CualquierColorDeVino y Rojo. Como una taxonomía está representada en un árbol, ese árbol puede ser recorrido desde el primer concepto que se está comparando hasta el segundo concepto. En el nodo inicial relacionado con el primer concepto, el valor de similitud es constante y igual a uno. Este valor se reduce por una constante en cada nodo visitado a lo largo del camino que llegará al nodo que incluye el segundo concepto. Cuanto más corto sea el camino entre los conceptos, mayor será la similitud entre los nodos. Algoritmo 2 Estimar-Similitud-RP(c1,c2) Requerido: Las constantes deben ser m > n > m2 donde m, n ∈ R[0, 1] 1: Similitud ← 1 2: si c1 es igual a c2 entonces 3: Devolver Similitud 4: fin si 5: padreComun ← encontrarPadreComun(c1, c2) {padreComun es el concepto más específico que cubre tanto c1 como c2} 6: N1 ← encontrarDistancia(padreComun, c1) 7: N2 ← encontrarDistancia(padreComun, c2) {N1 y N2 son el número de enlaces entre el concepto y el concepto padre} 8: si (padreComun == c1) o (padreComun == c2) entonces 9: Similitud ← Similitud ∗ m(N1+N2) 10: sino 11: Similitud ← Similitud ∗ n ∗ m(N1+N2−2) 12: fin si 13: Devolver Similitud La distancia relativa entre los nodos c1 y c2 se estima de la siguiente manera. Comenzando desde c1, se recorre el árbol para llegar a c2. En cada salto, la similitud disminuye ya que los conceptos se están alejando cada vez más entre sí. Sin embargo, según nuestras intuiciones, no todos los saltos disminuyen la similitud de igual manera. Que m represente el factor para saltar de un hijo a un padre y que n represente el factor para saltar de un hermano a otro hermano. Dado que saltar de un nodo a su abuelo cuenta como dos saltos de padre, el factor de descuento al moverse de un nodo a su abuelo es m2. De acuerdo con las intuiciones anteriores, nuestras constantes deben estar en la forma m > n > m2 donde el valor de m y n debe estar entre cero y uno. El algoritmo 2 muestra el cálculo de la distancia. Según el algoritmo, en primer lugar la similitud se inicializa con el valor de uno (línea 1). Si los conceptos son iguales entre sí, entonces la similitud será uno (líneas 2-4). De lo contrario, calculamos el ancestro común de los dos nodos y la distancia de cada concepto al ancestro común sin considerar al hermano (líneas 5-7). Si uno de los conceptos es igual al padre común, entonces no hay relación de hermanos entre los conceptos. Para cada nivel, multiplicamos la similitud por m y no consideramos el factor de hermanos en la estimación de la similitud. Como resultado, disminuimos la similitud en cada nivel con la tasa de m (línea 9). De lo contrario, tiene que existir una relación de hermanos. Esto significa que debemos considerar el efecto de n al medir la similitud. Recuerde que hemos contado N1+N2 aristas entre los conceptos. Dado que existe una relación de hermanos, dos de estos bordes constituyen la relación de hermanos. Por lo tanto, al calcular el efecto de la relación parental, utilizamos N1+N2 −2 aristas (línea 11). Algunas estimaciones de similitud relacionadas con la taxonomía en la Figura 2 se presentan en la Tabla 2. En este ejemplo, se toma m como 2/3 y n como 4/7. Tabla 2: Estimación de similitud de muestra sobre la taxonomía de muestra. Similitud(ColorRojo, Rosa) = 1 ∗ (2/3) = 0.6666667 Similitud(Rojo, Rosa) = 1 ∗ (4/7) = 0.5714286 Similitud(CualquierColorVino, Rosa) = 1 ∗ (2/3)2 = 0.44444445 Similitud(Blanco, Rosa) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 Para todas las métricas de similitud semántica en nuestra arquitectura, la taxonomía de características se mantiene en la ontología compartida. Para evaluar la similitud del vector de características, primero estimamos la similitud para cada característica individualmente y luego calculamos la suma promedio de estas similitudes. Entonces, el resultado es igual a la similitud semántica promedio de todo el vector de características. 6. SISTEMA DESARROLLADO Hemos implementado nuestra arquitectura en Java. Para facilitar las pruebas del sistema, el agente del consumidor tiene una interfaz de usuario que nos permite ingresar varias solicitudes. El agente productor está completamente automatizado y las operaciones de aprendizaje y oferta de servicios funcionan como se explicó anteriormente. En esta sección, explicamos los detalles de implementación del sistema desarrollado. Utilizamos OWL [11] como nuestro lenguaje de ontología y JENA como nuestro razonador de ontología. La ontología compartida es la versión modificada de la Ontología del Vino [19]. Incluye la descripción del vino como concepto y diferentes tipos de vino. Todos los participantes de la negociación utilizan esta ontología para entenderse mutuamente. Según la ontología, siete propiedades conforman el concepto de vino. El agente consumidor y el agente productor obtienen los valores posibles para estas propiedades consultando la ontología. Por lo tanto, todos los valores posibles para los componentes del concepto del vino, como el color, cuerpo, azúcar, etc., pueden ser alcanzados por ambos agentes. También se describen en esta ontología una variedad de tipos de vino como Borgoña, Chardonnay, Chenin Blanc, entre otros. Intuitivamente, cualquier tipo de vino descrito en la ontología también representa un concepto de vino. Esto nos permite considerar las instancias de vino Chardonnay como instancias de la clase Vino. Además de la descripción del vino, la información jerárquica de algunas características se puede inferir de la ontología. Por ejemplo, podemos representar la información de que el continente europeo abarca países occidentales. El país occidental abarca la región francesa, que incluye algunos territorios como el Loira, Burdeos, entre otros. Esta información jerárquica se utiliza en la estimación de similitud semántica. En esta parte, se pueden hacer algunos razonamientos como si un concepto X abarca Y y Y abarca Z, entonces el concepto X abarca Z. Por ejemplo, el Continente Europeo abarca Burdeos. 1306 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Para algunas características como cuerpo, sabor y azúcar, no hay información jerárquica, pero sus valores están nivelados semánticamente. Cuando eso sucede, proporcionamos los valores de similitud razonables para estas características. Por ejemplo, el cuerpo puede ser ligero, medio o fuerte. En este caso, asumimos que la luz es 0.66 similar a media pero solo 0.33 a fuerte. La ontología de WineStock es el inventario de los productores y describe una clase de producto como WineProduct. Esta clase es necesaria para que el productor registre los vinos que vende. La ontología implica a los individuos de esta clase. Los individuos representan los servicios disponibles que posee el productor. Hemos preparado dos ontologías de WineStock separadas para realizar pruebas. En la primera ontología, hay 19 productos de vino disponibles y en la segunda ontología, hay 50 productos. EVALUACIÓN DEL RENDIMIENTO Evaluamos el rendimiento de los sistemas propuestos en relación con la técnica de aprendizaje que utilizaron, DCEA e ID3, comparándolos con CEA, RO (para oferta aleatoria) y SCR (oferta basada solo en la solicitud actual). Aplicamos una variedad de escenarios en este conjunto de datos para ver las diferencias de rendimiento. Cada escenario de prueba contiene una lista de preferencias para el usuario y el número de coincidencias de la lista de productos. La Tabla 3 muestra estas preferencias y la disponibilidad de esos productos en el inventario para los primeros cinco escenarios. Ten en cuenta que estas preferencias son internas al consumidor y el productor intenta aprenderlas durante la negociación. Tabla 3: Disponibilidad de vinos en diferentes escenarios de prueba ID Preferencia del consumidor Disponibilidad (de 19) 1 Vino seco 15 2 Vino tinto y seco 8 3 Vino tinto, seco y moderado 4 4 Vino tinto y fuerte 2 5 Vino tinto o rosado, y fuerte 3 7.1 Comparación de Algoritmos de Aprendizaje En la comparación de algoritmos de aprendizaje, utilizamos los cinco escenarios de la Tabla 3. Aquí, primero usamos la medida de similitud de Tversky. Con estos casos de prueba, estamos interesados en encontrar el número de iteraciones que se requieren para que el productor genere una oferta aceptable para el consumidor. Dado que el rendimiento también depende de la solicitud inicial, repetimos nuestros experimentos con diferentes solicitudes iniciales. Por consiguiente, para cada caso, ejecutamos los algoritmos cinco veces con varias variaciones de las solicitudes iniciales. En cada experimento, contamos el número de iteraciones necesarias para llegar a un acuerdo. Tomamos el promedio de estos números para evaluar estos sistemas de manera justa. Como es costumbre, probamos cada algoritmo con las mismas solicitudes iniciales. La Tabla 4 compara los enfoques utilizando diferentes algoritmos de aprendizaje. Cuando las partes grandes del inventario son compatibles con las preferencias de los clientes, como en el primer caso de prueba, el rendimiento de todas las técnicas es casi el mismo (por ejemplo, Escenario 1). A medida que el número de servicios compatibles disminuye, RO funciona mal como se esperaba. El segundo peor método es SCR ya que solo considera la solicitud más reciente de los clientes y no aprende de las solicitudes anteriores. CEA da los mejores resultados cuando puede generar una respuesta pero no puede manejar los casos que contienen preferencias disyuntivas, como el que se presenta en el Escenario 5. ID3 y DCEA logran los mejores resultados. Su rendimiento es comparable y pueden manejar todos los casos, incluido el Escenario 5. Tabla 4: Comparación de algoritmos de aprendizaje en términos del número promedio de interacciones. Ejecutar DCEA SCR RO CEA ID3 Escenario 1: 1.2 1.4 1.2 1.2 1.2 Escenario 2: 1.4 1.4 2.6 1.4 1.4 Escenario 3: 1.4 1.8 4.4 1.4 1.4 Escenario 4: 2.2 2.8 9.6 1.8 2 Escenario 5: 2 2.6 7.6 1.75+ Sin oferta 1.8 Promedio de todos los casos: 1.64 2 5.08 1.51+Sin oferta 1.56 7.2 Comparación de Métricas de Similitud Para comparar las métricas de similitud que se explicaron en la Sección 5, fijamos el algoritmo de aprendizaje en DCEA. Además de los escenarios mostrados en la Tabla 3, agregamos los siguientes cinco nuevos escenarios considerando la información jerárquica. • El cliente desea comprar vino cuya bodega esté ubicada en California y cuya uva sea de tipo blanco. Además, la bodega del vino no debería ser costosa. Solo hay cuatro productos que cumplen con estas condiciones. • El cliente quiere comprar vino de color rojo o rosado y de tipo de uva tinta. Además, la ubicación del vino debe ser en Europa. Se desea que el grado de dulzura sea seco o semiseco. El sabor debe ser delicado o moderado, mientras que el cuerpo debe ser medio o ligero. Además, la bodega del vino debería ser una bodega cara. Hay dos productos que cumplen con todos estos requisitos. El cliente quiere comprar vino rosado moderado, que se encuentra alrededor de la región francesa. La categoría de bodega debería ser Bodega Moderada. Solo hay un producto que cumple con estos requisitos. • El cliente quiere comprar vino tinto caro, que se encuentra alrededor de la Región de California o vino blanco barato, que se encuentra alrededor de la Región de Texas. Hay cinco productos disponibles. • El cliente quiere comprar un vino blanco delicado cuyo productor esté en la categoría de Bodega Costosa. Hay dos productos disponibles. Los primeros siete escenarios se prueban con el primer conjunto de datos que contiene un total de 19 servicios y los últimos tres escenarios se prueban con el segundo conjunto de datos que contiene 50 servicios. La Tabla 5 muestra la evaluación del rendimiento en términos del número de interacciones necesarias para llegar a un consenso. La métrica de Tversky da los peores resultados ya que no considera la similitud semántica. El rendimiento de Lins es mejor que el de Tversky pero peor que el de otros. La métrica de Wu-Palmer y la medida de <br>similitud de RP</br> casi ofrecen el mismo rendimiento y son mejores que otras. Cuando se examinan los resultados, considerar la cercanía semántica aumenta el rendimiento. 8. DISCUSIÓN Revisamos la literatura reciente en comparación con nuestro trabajo. Tama et al. [16] proponen un nuevo enfoque basado en ontología para la negociación. Según su enfoque, los protocolos de negociación utilizados en el comercio electrónico pueden ser modelados como ontologías. Por lo tanto, los agentes pueden llevar a cabo un protocolo de negociación utilizando esta ontología compartida sin necesidad de estar codificados con los detalles del protocolo de negociación. Mientras tanto, la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1307 Tabla 5: Comparación de métricas de similitud en términos de número de interacciones. Ejecutar Tversky Lin Wu Palmer RP Escenario 1: 1.2 1.2 1 1 Escenario 2: 1.4 1.4 1.6 1.6 Escenario 3: 1.4 1.8 2 2 Escenario 4: 2.2 1 1.2 1.2 Escenario 5: 2 1.6 1.6 1.6 Escenario 6: 5 3.8 2.4 2.6 Escenario 7: 3.2 1.2 1 1 Escenario 8: 5.6 2 2 2.2 Escenario 9: 2.6 2.2 2.2 2.6 Escenario 10: 4.4 2 2 1.8 Promedio de todos los casos: 2.9 1.82 1.7 1.76 Tama et al. modelan el protocolo de negociación utilizando ontologías, en cambio, nosotros hemos modelado el servicio a ser negociado. Además, hemos construido un sistema con el cual se pueden aprender las preferencias de negociación. El estudio de Sadri et al. analiza la negociación en el contexto de la asignación de recursos [14]. Los agentes tienen recursos limitados y necesitan solicitar recursos faltantes a otros agentes. Se propone un mecanismo basado en secuencias de diálogo entre agentes como solución. El mecanismo se basa en el ciclo de agente de observar-pensar-actuar. Estos diálogos incluyen ofrecer recursos, intercambios de recursos y ofrecer recursos alternativos. Cada agente en el sistema planea sus acciones para alcanzar un estado objetivo. A diferencia de nuestro enfoque, el estudio de Sadri et al. no se preocupa por las preferencias de aprendizaje mutuas. Brzostowski y Kowalczyk proponen un enfoque para seleccionar un socio de negociación adecuado investigando negociaciones previas de múltiples atributos [1]. Para lograr esto, utilizan el razonamiento basado en casos. Su enfoque es probabilístico ya que el comportamiento de los socios puede cambiar en cada iteración. En nuestro enfoque, estamos interesados en negociar el contenido del servicio. Después de que el consumidor y el productor acuerden el servicio, se pueden utilizar mecanismos de negociación orientados al precio para acordar el precio. Fatima et al. estudian los factores que afectan la negociación, como las preferencias, el plazo, el precio, entre otros, ya que el agente que desarrolla una estrategia contra su oponente debe considerar todos ellos [5]. En su enfoque, el objetivo del agente vendedor es vender el servicio al precio más alto posible, mientras que el objetivo del agente comprador es comprar el bien al precio más bajo posible. El intervalo de tiempo afecta a estos agentes de manera diferente. En comparación con Fatima et al., nuestro enfoque es diferente. Mientras ellos estudian el efecto del tiempo en la negociación, nuestro enfoque está en aprender las preferencias para una negociación exitosa. Faratin et al. proponen un mecanismo de negociación multi-tema, donde las variables de servicio para la negociación, como el precio, la calidad del servicio, entre otros, se consideran intercambios entre sí (es decir, un precio más alto por una entrega más temprana) [4]. Generan un modelo heurístico para compensaciones que incluye la estimación de similitud difusa y una exploración de escalada de colina para ofertas posiblemente aceptables. Aunque abordamos un problema similar, aprendemos las preferencias del cliente con la ayuda del aprendizaje inductivo y generamos contraofertas de acuerdo con estas preferencias aprendidas. Faratin et al. solo utilizan la última oferta realizada por el consumidor al calcular la similitud para elegir la contraoferta. A diferencia de ellos, también tenemos en cuenta las solicitudes previas del consumidor. En sus experimentos, Faratin et al. asumen que los pesos de las variables de servicio están fijos a priori. Por el contrario, aprendemos estas preferencias con el tiempo. En nuestro trabajo futuro, planeamos integrar el razonamiento ontológico en el algoritmo de aprendizaje para que la información jerárquica pueda ser aprendida a partir de la jerarquía de subsumpción de relaciones. Además, al utilizar las relaciones entre las características, el productor puede descubrir nuevos conocimientos a partir de los conocimientos existentes. Estas son direcciones interesantes que seguiremos en nuestro trabajo futuro. 9. REFERENCIAS [1] J. Brzostowski y R. Kowalczyk. En el razonamiento basado en casos posibilístico para la selección de socios para la negociación de agentes de múltiples atributos. En Actas del 4to Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 273-278, 2005. [2] L. Busch e I. Horstman. Un comentario sobre negociaciones tema por tema. Juegos y Comportamiento Económico, 19:144-148, 1997. [3] J. K. Debenham. Gestión de la negociación en el mercado electrónico en el contexto de un sistema multiagente. En Actas de la 21ª Conferencia Internacional sobre Sistemas Basados en el Conocimiento e Inteligencia Artificial Aplicada, ES2002:, 2002. [4] P. Faratin, C. Sierra y N. R. Jennings. Utilizando criterios de similitud para hacer compensaciones de problemas en negociaciones automatizadas. Inteligencia Artificial, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge y N. Jennings. Agentes óptimos para negociaciones de múltiples temas. En Actas del 2do Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 129-136, 2003. [6] C. Giraud-Carrier. Una nota sobre la utilidad del aprendizaje incremental. Comunicaciones de IA, 13(4):215-223, 2000. [7] T.-P. Hong y S.-S. Tseng. Dividiendo y fusionando espacios de versiones para aprender conceptos disyuntivos. IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.\n\nTraducción al español:\nIEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin. Una definición de similitud basada en teoría de la información. En Actas de la 15ª Conferencia Internacional sobre Aprendizaje Automático, páginas 296-304. Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, y A. G. Moukas. Agentes que compran y venden. Comunicaciones de la ACM, 42(3):81-91, 1999. [10] T. M. Mitchell. Aprendizaje automático. McGraw Hill, NY, 1997. [11] Búho. OWL: Guía del lenguaje de ontologías web, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal y S. C. K. Shiu. Fundamentos del Razonamiento Basado en Casos Blandos. John Wiley & Sons, Nueva Jersey, 2004. [13] J. R. Quinlan. Inducción de árboles de decisión. Aprendizaje automático, 1(1):81-106, 1986. [14] F. Sadri, F. Toni y P. Torroni. Diálogos para negociación: Variedades de agentes y secuencias de diálogo. En ATAL 2001, Artículos Revisados, volumen 2333 de LNAI, páginas 405-421. Springer-Verlag, 2002. [15] M. P. Singh. \n\nSpringer-Verlag, 2002. [15] M. P. Singh. Comercio electrónico orientado al valor. IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, y M. Wooldridge. Ontologías para apoyar la negociación en el comercio electrónico. Aplicaciones de la Inteligencia Artificial en Ingeniería, 18:223-236, 2005. [17] A. Tversky. Características de similitud. Revisión Psicológica, 84(4):327-352, 1977. [18] P. E. Utgoff. Inducción incremental de árboles de decisión. Aprendizaje automático, 4:161-186, 1989. [19] Vino, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu y M. Palmer. Semántica de verbos y selección léxica. En el 32. Reunión anual de la Asociación de Lingüística Computacional, páginas 133-138, 1994. 1308 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    "Similitud RP",
                    "similitud de RP"
                ]
            ]
        },
        "negotiation": {
            "translated_key": "negociación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent <br>negotiation</br> approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a <br>negotiation</br> approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for <br>negotiation</br> by assuming that the service content is fixed [9].",
                "However, <br>negotiation</br> on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing <br>negotiation</br> approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the <br>negotiation</br> process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the <br>negotiation</br>.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for <br>negotiation</br>.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented <br>negotiation</br>.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The <br>negotiation</br> takes place in a turn-taking fashion, where the consumer agent starts the <br>negotiation</br> with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the <br>negotiation</br> ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the <br>negotiation</br> will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented <br>negotiation</br> is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed <br>negotiation</br> Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of <br>negotiation</br>, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the <br>negotiation</br> of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the <br>negotiation</br> interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the <br>negotiation</br>.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the <br>negotiation</br> use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during <br>negotiation</br>.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for <br>negotiation</br>.",
                "According to their approach, the <br>negotiation</br> protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform <br>negotiation</br> protocol by using this shared ontology without the need of being hard coded of <br>negotiation</br> protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the <br>negotiation</br> protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which <br>negotiation</br> preferences can be learned.",
                "Sadri et al. study <br>negotiation</br> in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate <br>negotiation</br> partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in <br>negotiation</br> the content of the service.",
                "After the consumer and producer agree on the service, price-oriented <br>negotiation</br> mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the <br>negotiation</br> such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on <br>negotiation</br>, our focus is on learning preferences for a successful <br>negotiation</br>.",
                "Faratin et al. propose a multi-issue <br>negotiation</br> mechanism, where the service variables for the <br>negotiation</br> such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent <br>negotiation</br>.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market <br>negotiation</br> in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue <br>negotiation</br>.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for <br>negotiation</br>: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting <br>negotiation</br> in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Multiagent <br>negotiation</br> approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "This calls for a <br>negotiation</br> approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for <br>negotiation</br> by assuming that the service content is fixed [9].",
                "However, <br>negotiation</br> on price presupposes that other properties of the service have already been agreed upon.",
                "However, most existing <br>negotiation</br> approaches assume that all features of a service are equally important and concentrate on the price [5, 2]."
            ],
            "translated_annotated_samples": [
                "Los enfoques de <br>negociación</br> multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio.",
                "Esto requiere un enfoque de <br>negociación</br> en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo.",
                "INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la <br>negociación</br> al asumir que el contenido del servicio está fijo [9].",
                "Sin embargo, la <br>negociación</br> sobre el precio presupone que otras propiedades del servicio ya han sido acordadas.",
                "Sin embargo, la mayoría de los enfoques de <br>negociación</br> existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de <br>negociación</br> multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de <br>negociación</br> en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la <br>negociación</br> al asumir que el contenido del servicio está fijo [9]. Sin embargo, la <br>negociación</br> sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de <br>negociación</br> existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "inductive learn": {
            "translated_key": "aprendizaje inductivo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using Semantic Similarity ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is <br>inductive learn</br>ing.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the <br>inductive learn</br>ing algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts.",
                "Estimating semantic similarity in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The semantic similarity is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average semantic similarity of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of semantic similarity.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the semantic similarity.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of <br>inductive learn</br>ing and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "The first one is <br>inductive learn</br>ing.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the <br>inductive learn</br>ing algorithms that learns concepts from observed examples.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of <br>inductive learn</br>ing and generate counter-offers in accordance with these learned preferences."
            ],
            "translated_annotated_samples": [
                "El primero es el <br>aprendizaje inductivo</br>.",
                "Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de <br>aprendizaje inductivo</br> que aprende conceptos a partir de ejemplos observados.",
                "Aunque abordamos un problema similar, aprendemos las preferencias del cliente con la ayuda del <br>aprendizaje inductivo</br> y generamos contraofertas de acuerdo con estas preferencias aprendidas."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando similitud semántica ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la similitud semántica entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la similitud semántica (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor. La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor. Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios. Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las preferencias del consumidor y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta. Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor. Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles. Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor. Este desafío se puede estudiar en tres etapas: • Aprendizaje de preferencias: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5) APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica. Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio. Por ejemplo, un consumidor puede tener preferencia por el vino tinto. Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo. Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo). Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor. Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores. Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa. Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo. Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6]. Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones. Investigamos particularmente dos enfoques. El primero es el <br>aprendizaje inductivo</br>. Esta técnica se aplica para aprender las preferencias como conceptos. Desarrollamos el Algoritmo de Eliminación de Candidatos (CEA) para el Espacio de Versiones [10]. Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva. Curiosamente, la mayoría de las veces las preferencias del consumidor son disyuntivas. Estamos considerando un agente que está comprando vino. El consumidor puede preferir vino tinto o vino rosado pero no vino blanco. Para utilizar CEA con tales preferencias, es necesaria una modificación sólida. El segundo enfoque son los árboles de decisión. Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas. Un árbol de decisión incremental bien conocido es ID5R [18]. Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional. Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de <br>aprendizaje inductivo</br> que aprende conceptos a partir de ejemplos observados. El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender. El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener. Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto. El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo. Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío. Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo. En cada interacción entre el productor y el consumidor, tanto G como S son modificados. Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas. Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento. Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa. Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí. Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 CEA Disyuntivo Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos. Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos. Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones. Algunos de estos enfoques utilizan múltiples espacios de versión. Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7]. Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir hipótesis disyuntivas además de las conjunciones y la negación. Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica. EJEMPLO 1. Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco). El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor. Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco. En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}. Solo cuando todos los valores existan en la lista, serán reemplazados por ?. En otras palabras, permitimos que el algoritmo generalice más lentamente que antes. Modificamos el algoritmo CEA para hacer frente a este cambio. El algoritmo modificado, DCEA, se presenta como Algoritmo 1. Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de múltiples espacios de versiones. La fase de inicialización es la misma que el algoritmo original (líneas 1, 2). Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4). Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí. Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran. Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7). Cuando llega una muestra negativa, no cambiamos S como antes. Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15). A diferencia del CEA original, intentamos especializar el G mínimamente. El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13). Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada. Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez. Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14). Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar. Por ejemplo, considera el atributo del color. Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2. La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA. El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra. Después de la primera muestra positiva, S se generaliza para cubrir también la instancia. La segunda muestra es negativa. Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada. En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general. La tercera muestra es positiva y generaliza S aún más. Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco). Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis. Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}. Por el Algoritmo 1, no perdemos ninguna información. 3.3 ID3 ID3 [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor. Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos. El algoritmo ID3 se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas. Hay dos clases: positiva y negativa. Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor. Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas. El árbol de decisión tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba. El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio. Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos. Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja. El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender. Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos. Después de cada solicitud entrante, el árbol de decisiones se reconstruye. Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso. Sin embargo, en la práctica hemos evaluado que el ID3 es rápido y el costo de reconstrucción es insignificante. 4. OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA. El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes. Cuando el productor recibe una solicitud del consumidor, el conjunto de aprendizaje del productor se entrena con esta solicitud como una muestra positiva. Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios. El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor. En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor. El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores. Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios. Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios. Esta lista constituye el inventario de servicios de los proveedores. Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace. Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor. Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación. Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S). Calculamos las similitudes de varias maneras, como se explicará en la Sección 5. Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud. El agente productor puede romper el empate de varias maneras. Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de ID3 Si el productor aprende las preferencias de los consumidores con ID3, se aplica un mecanismo similar con dos diferencias. Primero, dado que ID3 no mantiene G, se eliminan de la lista de servicios aquellos no aceptados que se clasifican como negativos. Segundo, las similitudes de los posibles servicios no se miden con respecto a S, sino en cambio a todas las solicitudes previamente realizadas. 4.3 Mecanismos Alternativos de Oferta de Servicios Además de estos tres mecanismos de oferta de servicios (Oferta de Servicio con CEA, Oferta de Servicio con DCEA y Oferta de Servicio con ID3), incluimos otros dos mecanismos. 1304 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) • Oferta de Servicio Aleatoria (RO): El productor genera una contraoferta aleatoriamente de la lista de servicios disponibles, sin considerar las preferencias de los consumidores. • Oferta de Servicio considerando solo la solicitud actual (SCR): El productor selecciona una contraoferta de acuerdo con la similitud de la solicitud actual del consumidor pero no considera solicitudes anteriores. 5. ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una métrica de similitud que toma dos entradas y devuelve qué tan similares son. Existen varios métricos de similitud utilizados en sistemas de razonamiento basado en casos, como la suma ponderada de la distancia euclidiana, la distancia de Hamming, entre otros [12]. La métrica de similitud afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor. Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de similitud semántica llamada Similitud RP. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente. En la Ecuación (1), común representa la cantidad de atributos coincidentes, mientras que diferente representa la cantidad de atributos diferentes. Nuestra suposición actual es que α y β son iguales entre sí. SMpq = α(común) α(común) + β(diferente) (1) Aquí, al comparar dos características, asignamos cero para la disimilitud y uno para la similitud al omitir la cercanía semántica entre los valores de las características. La métrica de similitud de Tversky está diseñada para comparar dos vectores de características. En nuestro sistema, mientras que la lista de servicios que puede ofrecer el productor son cada uno un vector de características, el conjunto más específico S no es un vector de características. S consiste en hipótesis de vectores de características. Por lo tanto, estimamos la similitud de cada hipótesis dentro del conjunto más específico S y luego calculamos el promedio de las similitudes. EJEMPLO 3. Suponga que S contiene las siguientes dos hipótesis: { {Luz, Moderado, (Rojo, Blanco)} , {Completo, Fuerte, Rosa}}. Toma el servicio s como (Ligero, Resistente, Rosa). Entonces, la similitud del primero es igual a 1/3 y la del segundo es igual a 2/3 de acuerdo con la Ecuación (1). Normalmente, tomamos el promedio de ello y obtenemos (1/3 + 2/3)/2, que es igual a 1/2. Sin embargo, la primera hipótesis implica el efecto de dos solicitudes y la segunda hipótesis implica solo una solicitud. Por lo tanto, esperamos que el efecto de la primera hipótesis sea mayor que el de la segunda. Por lo tanto, calculamos la similitud promedio teniendo en cuenta la cantidad de muestras que las hipótesis cubren. Que ch denote el número de muestras que cubre la hipótesis h y (SM(h,servicio)) denote la similitud de la hipótesis h con el servicio dado. Calculamos la similitud de cada hipótesis con el servicio dado y las ponderamos con el número de muestras que cubren. Encontramos la similitud dividiendo la suma ponderada de las similitudes de todas las hipótesis en S con el servicio por el número de todas las muestras que están cubiertas en S. AV G−SM(servicio, S) = |S| |h| (ch ∗ SM(h, servicio)) |S| |h| ch (2) Figura 2: Taxonomía de muestra para estimación de similitud EJEMPLO 4. Para el ejemplo anterior, la similitud de (Luz, Fuerte, Rosa) con el conjunto específico es (2 ∗ 1/3 + 2/3)/3, igual a 4/9. El número posible de muestras que abarca una hipótesis se puede estimar multiplicando las cardinalidades de cada atributo. Por ejemplo, la cardinalidad del primer atributo es dos y la de los demás es igual a uno para la hipótesis dada, como {Luz, Moderado, (Rojo, Blanco)}. Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La métrica de similitud de Lins Un taxonomía puede ser utilizada al estimar la similitud semántica entre dos conceptos. Estimar la similitud semántica en una taxonomía de tipo Es-Un se puede hacer calculando la distancia entre los nodos relacionados con los conceptos comparados. Los enlaces entre los nodos pueden considerarse como distancias. Entonces, la longitud del camino entre los nodos indica qué tan similares son los conceptos. Una estimación alternativa para utilizar el contenido de información en la estimación de la similitud semántica en lugar del método de conteo de aristas, fue propuesta por Lin [8]. La ecuación (3) [8] muestra la similitud de Lin donde c1 y c2 son los conceptos comparados y c0 es el concepto más específico que subsume a ambos. Además, P(C) representa la probabilidad de que un objeto seleccionado arbitrariamente pertenezca al concepto C. La similitud(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Métrica de similitud de Wu y Palmers Diferente de Lin, Wu y Palmer utilizan la distancia entre los nodos en la taxonomía ES-UN [20]. La similitud semántica se representa con la Ecuación (4) [20]. Aquí, se estima la similitud entre c1 y c2 y c0 es el concepto más específico que subsume estas clases. N1 es el número de aristas entre c1 y c0. N2 es el número de aristas entre c2 y c0. N0 es el número de enlaces IS-A de c0 desde la raíz de la taxonomía. Proponemos estimar la distancia relativa en una taxonomía entre dos conceptos utilizando las siguientes intuiciones. Utilizamos la Figura 2 para ilustrar estas intuiciones. • Padre versus abuelo: El padre de un nodo es más similar al nodo que los abuelos de ese. Generalización del Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1305 es un concepto que razonablemente resulta en alejarse más de ese concepto. Cuanto más generales son los conceptos, menos similares son. Por ejemplo, AnyWineColor es el padre de ReddishColor y ReddishColor es el padre de Red. Entonces, esperamos que la similitud entre ReddishColor y Red sea mayor que la similitud entre AnyWineColor y Red. • Padre versus hermano: Un nodo tendría una similitud mayor con su padre que con su hermano. Por ejemplo, Rojo y Rosa son hijos de ColorRojo. En este caso, esperamos que la similitud entre Rojo y ColorRojizo sea mayor que la de Rojo y Rosa. • Hermano versus abuelo: Un nodo es más similar a su hermano que a su abuelo. Para ilustrar, AnyWineColor es el abuelo de Red, y Red y Rose son hermanos. Por lo tanto, posiblemente anticipamos que Rojo y Rosa son más similares que CualquierColorDeVino y Rojo. Como una taxonomía está representada en un árbol, ese árbol puede ser recorrido desde el primer concepto que se está comparando hasta el segundo concepto. En el nodo inicial relacionado con el primer concepto, el valor de similitud es constante y igual a uno. Este valor se reduce por una constante en cada nodo visitado a lo largo del camino que llegará al nodo que incluye el segundo concepto. Cuanto más corto sea el camino entre los conceptos, mayor será la similitud entre los nodos. Algoritmo 2 Estimar-Similitud-RP(c1,c2) Requerido: Las constantes deben ser m > n > m2 donde m, n ∈ R[0, 1] 1: Similitud ← 1 2: si c1 es igual a c2 entonces 3: Devolver Similitud 4: fin si 5: padreComun ← encontrarPadreComun(c1, c2) {padreComun es el concepto más específico que cubre tanto c1 como c2} 6: N1 ← encontrarDistancia(padreComun, c1) 7: N2 ← encontrarDistancia(padreComun, c2) {N1 y N2 son el número de enlaces entre el concepto y el concepto padre} 8: si (padreComun == c1) o (padreComun == c2) entonces 9: Similitud ← Similitud ∗ m(N1+N2) 10: sino 11: Similitud ← Similitud ∗ n ∗ m(N1+N2−2) 12: fin si 13: Devolver Similitud La distancia relativa entre los nodos c1 y c2 se estima de la siguiente manera. Comenzando desde c1, se recorre el árbol para llegar a c2. En cada salto, la similitud disminuye ya que los conceptos se están alejando cada vez más entre sí. Sin embargo, según nuestras intuiciones, no todos los saltos disminuyen la similitud de igual manera. Que m represente el factor para saltar de un hijo a un padre y que n represente el factor para saltar de un hermano a otro hermano. Dado que saltar de un nodo a su abuelo cuenta como dos saltos de padre, el factor de descuento al moverse de un nodo a su abuelo es m2. De acuerdo con las intuiciones anteriores, nuestras constantes deben estar en la forma m > n > m2 donde el valor de m y n debe estar entre cero y uno. El algoritmo 2 muestra el cálculo de la distancia. Según el algoritmo, en primer lugar la similitud se inicializa con el valor de uno (línea 1). Si los conceptos son iguales entre sí, entonces la similitud será uno (líneas 2-4). De lo contrario, calculamos el ancestro común de los dos nodos y la distancia de cada concepto al ancestro común sin considerar al hermano (líneas 5-7). Si uno de los conceptos es igual al padre común, entonces no hay relación de hermanos entre los conceptos. Para cada nivel, multiplicamos la similitud por m y no consideramos el factor de hermanos en la estimación de la similitud. Como resultado, disminuimos la similitud en cada nivel con la tasa de m (línea 9). De lo contrario, tiene que existir una relación de hermanos. Esto significa que debemos considerar el efecto de n al medir la similitud. Recuerde que hemos contado N1+N2 aristas entre los conceptos. Dado que existe una relación de hermanos, dos de estos bordes constituyen la relación de hermanos. Por lo tanto, al calcular el efecto de la relación parental, utilizamos N1+N2 −2 aristas (línea 11). Algunas estimaciones de similitud relacionadas con la taxonomía en la Figura 2 se presentan en la Tabla 2. En este ejemplo, se toma m como 2/3 y n como 4/7. Tabla 2: Estimación de similitud de muestra sobre la taxonomía de muestra. Similitud(ColorRojo, Rosa) = 1 ∗ (2/3) = 0.6666667 Similitud(Rojo, Rosa) = 1 ∗ (4/7) = 0.5714286 Similitud(CualquierColorVino, Rosa) = 1 ∗ (2/3)2 = 0.44444445 Similitud(Blanco, Rosa) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 Para todas las métricas de similitud semántica en nuestra arquitectura, la taxonomía de características se mantiene en la ontología compartida. Para evaluar la similitud del vector de características, primero estimamos la similitud para cada característica individualmente y luego calculamos la suma promedio de estas similitudes. Entonces, el resultado es igual a la similitud semántica promedio de todo el vector de características. 6. SISTEMA DESARROLLADO Hemos implementado nuestra arquitectura en Java. Para facilitar las pruebas del sistema, el agente del consumidor tiene una interfaz de usuario que nos permite ingresar varias solicitudes. El agente productor está completamente automatizado y las operaciones de aprendizaje y oferta de servicios funcionan como se explicó anteriormente. En esta sección, explicamos los detalles de implementación del sistema desarrollado. Utilizamos OWL [11] como nuestro lenguaje de ontología y JENA como nuestro razonador de ontología. La ontología compartida es la versión modificada de la Ontología del Vino [19]. Incluye la descripción del vino como concepto y diferentes tipos de vino. Todos los participantes de la negociación utilizan esta ontología para entenderse mutuamente. Según la ontología, siete propiedades conforman el concepto de vino. El agente consumidor y el agente productor obtienen los valores posibles para estas propiedades consultando la ontología. Por lo tanto, todos los valores posibles para los componentes del concepto del vino, como el color, cuerpo, azúcar, etc., pueden ser alcanzados por ambos agentes. También se describen en esta ontología una variedad de tipos de vino como Borgoña, Chardonnay, Chenin Blanc, entre otros. Intuitivamente, cualquier tipo de vino descrito en la ontología también representa un concepto de vino. Esto nos permite considerar las instancias de vino Chardonnay como instancias de la clase Vino. Además de la descripción del vino, la información jerárquica de algunas características se puede inferir de la ontología. Por ejemplo, podemos representar la información de que el continente europeo abarca países occidentales. El país occidental abarca la región francesa, que incluye algunos territorios como el Loira, Burdeos, entre otros. Esta información jerárquica se utiliza en la estimación de similitud semántica. En esta parte, se pueden hacer algunos razonamientos como si un concepto X abarca Y y Y abarca Z, entonces el concepto X abarca Z. Por ejemplo, el Continente Europeo abarca Burdeos. 1306 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Para algunas características como cuerpo, sabor y azúcar, no hay información jerárquica, pero sus valores están nivelados semánticamente. Cuando eso sucede, proporcionamos los valores de similitud razonables para estas características. Por ejemplo, el cuerpo puede ser ligero, medio o fuerte. En este caso, asumimos que la luz es 0.66 similar a media pero solo 0.33 a fuerte. La ontología de WineStock es el inventario de los productores y describe una clase de producto como WineProduct. Esta clase es necesaria para que el productor registre los vinos que vende. La ontología implica a los individuos de esta clase. Los individuos representan los servicios disponibles que posee el productor. Hemos preparado dos ontologías de WineStock separadas para realizar pruebas. En la primera ontología, hay 19 productos de vino disponibles y en la segunda ontología, hay 50 productos. EVALUACIÓN DEL RENDIMIENTO Evaluamos el rendimiento de los sistemas propuestos en relación con la técnica de aprendizaje que utilizaron, DCEA e ID3, comparándolos con CEA, RO (para oferta aleatoria) y SCR (oferta basada solo en la solicitud actual). Aplicamos una variedad de escenarios en este conjunto de datos para ver las diferencias de rendimiento. Cada escenario de prueba contiene una lista de preferencias para el usuario y el número de coincidencias de la lista de productos. La Tabla 3 muestra estas preferencias y la disponibilidad de esos productos en el inventario para los primeros cinco escenarios. Ten en cuenta que estas preferencias son internas al consumidor y el productor intenta aprenderlas durante la negociación. Tabla 3: Disponibilidad de vinos en diferentes escenarios de prueba ID Preferencia del consumidor Disponibilidad (de 19) 1 Vino seco 15 2 Vino tinto y seco 8 3 Vino tinto, seco y moderado 4 4 Vino tinto y fuerte 2 5 Vino tinto o rosado, y fuerte 3 7.1 Comparación de Algoritmos de Aprendizaje En la comparación de algoritmos de aprendizaje, utilizamos los cinco escenarios de la Tabla 3. Aquí, primero usamos la medida de similitud de Tversky. Con estos casos de prueba, estamos interesados en encontrar el número de iteraciones que se requieren para que el productor genere una oferta aceptable para el consumidor. Dado que el rendimiento también depende de la solicitud inicial, repetimos nuestros experimentos con diferentes solicitudes iniciales. Por consiguiente, para cada caso, ejecutamos los algoritmos cinco veces con varias variaciones de las solicitudes iniciales. En cada experimento, contamos el número de iteraciones necesarias para llegar a un acuerdo. Tomamos el promedio de estos números para evaluar estos sistemas de manera justa. Como es costumbre, probamos cada algoritmo con las mismas solicitudes iniciales. La Tabla 4 compara los enfoques utilizando diferentes algoritmos de aprendizaje. Cuando las partes grandes del inventario son compatibles con las preferencias de los clientes, como en el primer caso de prueba, el rendimiento de todas las técnicas es casi el mismo (por ejemplo, Escenario 1). A medida que el número de servicios compatibles disminuye, RO funciona mal como se esperaba. El segundo peor método es SCR ya que solo considera la solicitud más reciente de los clientes y no aprende de las solicitudes anteriores. CEA da los mejores resultados cuando puede generar una respuesta pero no puede manejar los casos que contienen preferencias disyuntivas, como el que se presenta en el Escenario 5. ID3 y DCEA logran los mejores resultados. Su rendimiento es comparable y pueden manejar todos los casos, incluido el Escenario 5. Tabla 4: Comparación de algoritmos de aprendizaje en términos del número promedio de interacciones. Ejecutar DCEA SCR RO CEA ID3 Escenario 1: 1.2 1.4 1.2 1.2 1.2 Escenario 2: 1.4 1.4 2.6 1.4 1.4 Escenario 3: 1.4 1.8 4.4 1.4 1.4 Escenario 4: 2.2 2.8 9.6 1.8 2 Escenario 5: 2 2.6 7.6 1.75+ Sin oferta 1.8 Promedio de todos los casos: 1.64 2 5.08 1.51+Sin oferta 1.56 7.2 Comparación de Métricas de Similitud Para comparar las métricas de similitud que se explicaron en la Sección 5, fijamos el algoritmo de aprendizaje en DCEA. Además de los escenarios mostrados en la Tabla 3, agregamos los siguientes cinco nuevos escenarios considerando la información jerárquica. • El cliente desea comprar vino cuya bodega esté ubicada en California y cuya uva sea de tipo blanco. Además, la bodega del vino no debería ser costosa. Solo hay cuatro productos que cumplen con estas condiciones. • El cliente quiere comprar vino de color rojo o rosado y de tipo de uva tinta. Además, la ubicación del vino debe ser en Europa. Se desea que el grado de dulzura sea seco o semiseco. El sabor debe ser delicado o moderado, mientras que el cuerpo debe ser medio o ligero. Además, la bodega del vino debería ser una bodega cara. Hay dos productos que cumplen con todos estos requisitos. El cliente quiere comprar vino rosado moderado, que se encuentra alrededor de la región francesa. La categoría de bodega debería ser Bodega Moderada. Solo hay un producto que cumple con estos requisitos. • El cliente quiere comprar vino tinto caro, que se encuentra alrededor de la Región de California o vino blanco barato, que se encuentra alrededor de la Región de Texas. Hay cinco productos disponibles. • El cliente quiere comprar un vino blanco delicado cuyo productor esté en la categoría de Bodega Costosa. Hay dos productos disponibles. Los primeros siete escenarios se prueban con el primer conjunto de datos que contiene un total de 19 servicios y los últimos tres escenarios se prueban con el segundo conjunto de datos que contiene 50 servicios. La Tabla 5 muestra la evaluación del rendimiento en términos del número de interacciones necesarias para llegar a un consenso. La métrica de Tversky da los peores resultados ya que no considera la similitud semántica. El rendimiento de Lins es mejor que el de Tversky pero peor que el de otros. La métrica de Wu-Palmer y la medida de similitud de RP casi ofrecen el mismo rendimiento y son mejores que otras. Cuando se examinan los resultados, considerar la cercanía semántica aumenta el rendimiento. 8. DISCUSIÓN Revisamos la literatura reciente en comparación con nuestro trabajo. Tama et al. [16] proponen un nuevo enfoque basado en ontología para la negociación. Según su enfoque, los protocolos de negociación utilizados en el comercio electrónico pueden ser modelados como ontologías. Por lo tanto, los agentes pueden llevar a cabo un protocolo de negociación utilizando esta ontología compartida sin necesidad de estar codificados con los detalles del protocolo de negociación. Mientras tanto, la Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1307 Tabla 5: Comparación de métricas de similitud en términos de número de interacciones. Ejecutar Tversky Lin Wu Palmer RP Escenario 1: 1.2 1.2 1 1 Escenario 2: 1.4 1.4 1.6 1.6 Escenario 3: 1.4 1.8 2 2 Escenario 4: 2.2 1 1.2 1.2 Escenario 5: 2 1.6 1.6 1.6 Escenario 6: 5 3.8 2.4 2.6 Escenario 7: 3.2 1.2 1 1 Escenario 8: 5.6 2 2 2.2 Escenario 9: 2.6 2.2 2.2 2.6 Escenario 10: 4.4 2 2 1.8 Promedio de todos los casos: 2.9 1.82 1.7 1.76 Tama et al. modelan el protocolo de negociación utilizando ontologías, en cambio, nosotros hemos modelado el servicio a ser negociado. Además, hemos construido un sistema con el cual se pueden aprender las preferencias de negociación. El estudio de Sadri et al. analiza la negociación en el contexto de la asignación de recursos [14]. Los agentes tienen recursos limitados y necesitan solicitar recursos faltantes a otros agentes. Se propone un mecanismo basado en secuencias de diálogo entre agentes como solución. El mecanismo se basa en el ciclo de agente de observar-pensar-actuar. Estos diálogos incluyen ofrecer recursos, intercambios de recursos y ofrecer recursos alternativos. Cada agente en el sistema planea sus acciones para alcanzar un estado objetivo. A diferencia de nuestro enfoque, el estudio de Sadri et al. no se preocupa por las preferencias de aprendizaje mutuas. Brzostowski y Kowalczyk proponen un enfoque para seleccionar un socio de negociación adecuado investigando negociaciones previas de múltiples atributos [1]. Para lograr esto, utilizan el razonamiento basado en casos. Su enfoque es probabilístico ya que el comportamiento de los socios puede cambiar en cada iteración. En nuestro enfoque, estamos interesados en negociar el contenido del servicio. Después de que el consumidor y el productor acuerden el servicio, se pueden utilizar mecanismos de negociación orientados al precio para acordar el precio. Fatima et al. estudian los factores que afectan la negociación, como las preferencias, el plazo, el precio, entre otros, ya que el agente que desarrolla una estrategia contra su oponente debe considerar todos ellos [5]. En su enfoque, el objetivo del agente vendedor es vender el servicio al precio más alto posible, mientras que el objetivo del agente comprador es comprar el bien al precio más bajo posible. El intervalo de tiempo afecta a estos agentes de manera diferente. En comparación con Fatima et al., nuestro enfoque es diferente. Mientras ellos estudian el efecto del tiempo en la negociación, nuestro enfoque está en aprender las preferencias para una negociación exitosa. Faratin et al. proponen un mecanismo de negociación multi-tema, donde las variables de servicio para la negociación, como el precio, la calidad del servicio, entre otros, se consideran intercambios entre sí (es decir, un precio más alto por una entrega más temprana) [4]. Generan un modelo heurístico para compensaciones que incluye la estimación de similitud difusa y una exploración de escalada de colina para ofertas posiblemente aceptables. Aunque abordamos un problema similar, aprendemos las preferencias del cliente con la ayuda del <br>aprendizaje inductivo</br> y generamos contraofertas de acuerdo con estas preferencias aprendidas. Faratin et al. solo utilizan la última oferta realizada por el consumidor al calcular la similitud para elegir la contraoferta. A diferencia de ellos, también tenemos en cuenta las solicitudes previas del consumidor. En sus experimentos, Faratin et al. asumen que los pesos de las variables de servicio están fijos a priori. Por el contrario, aprendemos estas preferencias con el tiempo. En nuestro trabajo futuro, planeamos integrar el razonamiento ontológico en el algoritmo de aprendizaje para que la información jerárquica pueda ser aprendida a partir de la jerarquía de subsumpción de relaciones. Además, al utilizar las relaciones entre las características, el productor puede descubrir nuevos conocimientos a partir de los conocimientos existentes. Estas son direcciones interesantes que seguiremos en nuestro trabajo futuro. 9. REFERENCIAS [1] J. Brzostowski y R. Kowalczyk. En el razonamiento basado en casos posibilístico para la selección de socios para la negociación de agentes de múltiples atributos. En Actas del 4to Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 273-278, 2005. [2] L. Busch e I. Horstman. Un comentario sobre negociaciones tema por tema. Juegos y Comportamiento Económico, 19:144-148, 1997. [3] J. K. Debenham. Gestión de la negociación en el mercado electrónico en el contexto de un sistema multiagente. En Actas de la 21ª Conferencia Internacional sobre Sistemas Basados en el Conocimiento e Inteligencia Artificial Aplicada, ES2002:, 2002. [4] P. Faratin, C. Sierra y N. R. Jennings. Utilizando criterios de similitud para hacer compensaciones de problemas en negociaciones automatizadas. Inteligencia Artificial, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge y N. Jennings. Agentes óptimos para negociaciones de múltiples temas. En Actas del 2do Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS), páginas 129-136, 2003. [6] C. Giraud-Carrier. Una nota sobre la utilidad del aprendizaje incremental. Comunicaciones de IA, 13(4):215-223, 2000. [7] T.-P. Hong y S.-S. Tseng. Dividiendo y fusionando espacios de versiones para aprender conceptos disyuntivos. IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.\n\nTraducción al español:\nIEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin. Una definición de similitud basada en teoría de la información. En Actas de la 15ª Conferencia Internacional sobre Aprendizaje Automático, páginas 296-304. Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, y A. G. Moukas. Agentes que compran y venden. Comunicaciones de la ACM, 42(3):81-91, 1999. [10] T. M. Mitchell. Aprendizaje automático. McGraw Hill, NY, 1997. [11] Búho. OWL: Guía del lenguaje de ontologías web, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal y S. C. K. Shiu. Fundamentos del Razonamiento Basado en Casos Blandos. John Wiley & Sons, Nueva Jersey, 2004. [13] J. R. Quinlan. Inducción de árboles de decisión. Aprendizaje automático, 1(1):81-106, 1986. [14] F. Sadri, F. Toni y P. Torroni. Diálogos para negociación: Variedades de agentes y secuencias de diálogo. En ATAL 2001, Artículos Revisados, volumen 2333 de LNAI, páginas 405-421. Springer-Verlag, 2002. [15] M. P. Singh. \n\nSpringer-Verlag, 2002. [15] M. P. Singh. Comercio electrónico orientado al valor. IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, y M. Wooldridge. Ontologías para apoyar la negociación en el comercio electrónico. Aplicaciones de la Inteligencia Artificial en Ingeniería, 18:223-236, 2005. [17] A. Tversky. Características de similitud. Revisión Psicológica, 84(4):327-352, 1977. [18] P. E. Utgoff. Inducción incremental de árboles de decisión. Aprendizaje automático, 4:161-186, 1989. [19] Vino, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu y M. Palmer. Semántica de verbos y selección léxica. En el 32. Reunión anual de la Asociación de Lingüística Computacional, páginas 133-138, 1994. 1308 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "semantic similarity": {
            "translated_key": "similitud semántica",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning Consumer Preferences Using <br>semantic similarity</br> ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "This requires the service consumers and providers to negotiate their service needs and offers.",
                "Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price.",
                "In contrast, this work develops an approach through which the parties can negotiate the content of a service.",
                "This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each others preferences incrementally over time.",
                "Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service.",
                "Through repetitive interactions, the provider learns consumers needs accurately and can make better targeted offers.",
                "To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques.",
                "We further develop a metric for measuring <br>semantic similarity</br> between services and compare the performance of our approach using different similarity metrics.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Current approaches to e-commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9].",
                "However, negotiation on price presupposes that other properties of the service have already been agreed upon.",
                "Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3].",
                "When this is the case, the producer and the consumer need to negotiate the content of the requested service [15].",
                "However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2].",
                "However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer.",
                "For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer.",
                "Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process.",
                "For this purpose, evaluation of the service components with different weights can be useful.",
                "Some studies take these weights as a priori and uses the fixed weights [4].",
                "On the other hand, mostly the producer does not know the consumers preferences before the negotiation.",
                "Hence, it is more appropriate for the producer to learn these preferences for each consumer.",
                "Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time.",
                "We represent service requests as a vector of service features.",
                "We use an ontology in order to capture the relations between services and to construct the features for a given service.",
                "By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation.",
                "The particular service we have used is a wine selling service.",
                "The wine seller learns the wine preferences of the customer to sell better targeted wines.",
                "The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer.",
                "Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction.",
                "Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services.",
                "In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer.",
                "For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine.",
                "What should the producers offer 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS contain; white wine or rose wine?",
                "If the producer has some domain knowledge about <br>semantic similarity</br> (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences.",
                "The rest of this paper is organized as follows: Section 2 explains our proposed architecture.",
                "Section 3 explains the learning algorithms that were studied to learn consumer preferences.",
                "Section 4 studies the different service offering mechanisms.",
                "Section 5 contains the similarity metrics used in the experiments.",
                "The details of the developed system is analyzed in Section 6.",
                "Section 7 provides our experimental setup, test cases, and results.",
                "Finally, Section 8 discusses and compares our work with other related work. 2.",
                "ARCHITECTURE Our main components are consumer and producer agents, which communicate with each other to perform content-oriented negotiation.",
                "Figure 1 depicts our architecture.",
                "The consumer agent represents the customer and hence has access to the preferences of the customer.",
                "The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences.",
                "Similarly, the producer agent has access to the producers inventory and knows which wines are available or not.",
                "A shared ontology provides the necessary vocabulary and hence enables a common language for agents.",
                "This ontology describes the content of the service.",
                "Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated.",
                "Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used.",
                "However, to make our discussion concrete, we use the well-known Wine ontology [19] with some modification to illustrate our ideas and to test our system.",
                "The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on.",
                "With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine.",
                "The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer.",
                "The data repository includes information on the products the producer owns, the number of the products and ratings of those products.",
                "Ratings indicate the popularity of the products among customers.",
                "Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent.",
                "The negotiation takes place in a turn-taking fashion, where the consumer agent starts the negotiation with a particular service request.",
                "The request is composed of significant features of the service.",
                "In the wine example, these features include color, winery and so on.",
                "This is the particular wine that the customer is interested in purchasing.",
                "If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends.",
                "Otherwise, the producer offers an alternative wine from the inventory.",
                "When the consumer receives a counter offer from the producer, it will evaluate it.",
                "If it is acceptable, then the negotiation will end.",
                "Otherwise, the customer will generate a new request or stick to the previous request.",
                "This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer.",
                "One of the crucial challenges of the content-oriented negotiation is the automatic generation of counter offers by the service producer.",
                "When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producers available services.",
                "Both the consumers current request and the producers own available services are accessible by the producer.",
                "However, the consumers preferences in most cases will not be available.",
                "Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer.",
                "This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customers preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumers preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) 3.",
                "PREFERENCE LEARNING The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature.",
                "The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.",
                "For example, a consumer may have preference for red wine.",
                "This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.",
                "Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g. (Medium, Strong, Red).",
                "At the beginning of negotiation, the producer agent does not know the consumers preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer.",
                "The preferences denote the relative importance of the features of the services demanded by the consumer agents.",
                "For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(?-Full), ?, ? }, - (Full,Delicate,Rose) {?, (?-Delicate), ? }, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ? }, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ? }, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.",
                "On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.",
                "To tackle this problem, we propose to use incremental learning algorithms [6].",
                "This is necessary since no training data is available before the interactions start.",
                "We particularly investigate two approaches.",
                "The first one is inductive learning.",
                "This technique is applied to learn the preferences as concepts.",
                "We elaborate on Candidate Elimination Algorithm (CEA) for Version Space [10].",
                "CEA is known to perform poorly if the information to be learned is disjunctive.",
                "Interestingly, most of the time consumer preferences are disjunctive.",
                "Say, we are considering an agent that is buying wine.",
                "The consumer may prefer red wine or rose wine but not white wine.",
                "To use CEA with such preferences, a solid modification is necessary.",
                "The second approach is decision trees.",
                "Decision trees can learn from examples easily and classify new instances as positive or negative.",
                "A well-known incremental decision tree is ID5R [18].",
                "However, ID5R is known to suffer from high computational complexity.",
                "For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning. 3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.",
                "The algorithm maintains two sets to model the concept to be learned.",
                "The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.",
                "As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept.",
                "The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.",
                "At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.",
                "During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.",
                "At each interaction between the producer and the consumer, both G and S are modified.",
                "The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive.",
                "When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.",
                "As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.",
                "Incrementally, G specializes and S generalizes until G and S are equal to each other.",
                "When these sets are equal, the algorithm converges by means of reaching the target concept. 3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts.",
                "On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes.",
                "There are several studies on learning disjunctive concepts via Version Space.",
                "Some of these approaches use multiple version space.",
                "For instance, Hong et al. maintain several version spaces by split and merge operation [7].",
                "To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation.",
                "Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.",
                "EXAMPLE 1.",
                "Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes.",
                "The original CEA will generalize this as (Light, Delicate, ? ), meaning the color can take any value.",
                "However, in fact, we only know that the color can be red or white.",
                "In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.",
                "Only when all the values exist in the list, they will be replaced by ?.",
                "In other words, we let the algorithm generalize more slowly than before.",
                "We modify the CEA algorithm to deal with this change.",
                "The modified algorithm, DCEA, is given as Algorithm 1.",
                "Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space.",
                "The initialization phase is the same as the original algorithm (lines 1, 2).",
                "If any positive sample comes, we add the sample to the special set as before (line 4).",
                "However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.",
                "Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.",
                "After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).",
                "When a negative sample comes, we do not change S as before.",
                "We only modify the most general hypotheses not to cover this negative sample (lines 11-15).",
                "Different from the original CEA, we try to specialize the G minimally.",
                "The algorithm removes the hypothesis covering the negative sample (line 13).",
                "Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.",
                "For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.",
                "Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).",
                "Note that, exclusive list contains the values that the attribute cannot take.",
                "For example, consider the color attribute.",
                "If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.",
                "Table 1 illustrates the first three interactions and the workings of DCEA.",
                "The most general set and the most specific set show the contents of G and S after the sample comes in.",
                "After the first positive sample, S is generalized to also cover the instance.",
                "The second sample is negative.",
                "Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized.",
                "In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.",
                "The third sample is positive and generalizes S even more.",
                "Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White).",
                "This stems from the possibility of using this rule in the generation of other hypotheses.",
                "For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.",
                "By Algorithm 1, we do not miss any information. 3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute-value pairs.",
                "Applying this algorithm to our system with the intention of learning the consumers preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts.",
                "The ID3 algorithm is used in the learning process with the purpose of classification of offers.",
                "There are two classes: positive and negative.",
                "Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.",
                "Consumers requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.",
                "The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.",
                "The test attribute in a non-leaf node is one of the attributes making up the service description.",
                "For instance, body, flavor, color and so on are potential test attributes for wine service.",
                "When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.",
                "The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning.",
                "To overcome this problem, the system keeps consumers requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as negative examples.",
                "After each coming request, the decision tree is rebuilt.",
                "Without doubt, there is a drawback of reconstruction such as additional process load.",
                "However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 4.",
                "SERVICE OFFERING After learning the consumers preferences, the producer needs to make a counter offer that is compatible with the consumers preferences. 4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm.",
                "The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different.",
                "When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample.",
                "The learning components, the most specific set S and the most general set G are actively used in offering service.",
                "The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent.",
                "In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer.",
                "The most specific set, S is used in order to find best offer, which is similar to the consumers preferences.",
                "Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list.",
                "When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object.",
                "This list constitutes the providers inventory of services.",
                "Upon receiving a request, if the producer can offer an exactly matching service, then it does so.",
                "For example, for a wine this corresponds to selling a wine that matches the specified features of the consumers request identically.",
                "When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation.",
                "To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S).",
                "We compute the similarities in various ways as will be explained in Section 5.",
                "After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity.",
                "The producer agent can break the tie in a number of ways.",
                "Here, we have associated a rating value with each service and the producer prefers the higher rated service to others. 4.2 Service Offering via ID3 If the producer learns the consumers preferences with ID3, a similar mechanism is applied with two differences.",
                "First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list.",
                "Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests. 4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms.. 1304 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumers preferences. • Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumers current request but does not consider previous requests. 5.",
                "SIMILARITY ESTIMATION Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are.",
                "There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12].",
                "The similarity metric affects the performance of the system while deciding which service is the closest to the consumers request.",
                "We first analyze some existing metrics and then propose a new <br>semantic similarity</br> metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes.",
                "Our current assumption is that α and β is equal to each other.",
                "SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values.",
                "Tverskys similarity metric is designed to compare two feature vectors.",
                "In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector.",
                "S consists of hypotheses of feature vectors.",
                "Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities.",
                "EXAMPLE 3.",
                "Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.",
                "Take service s as (Light, Strong, Rose).",
                "Then the similarity of the first one is equal to 1/3 and the second one is equal to 2/3 in accordance with Equation (1).",
                "Normally, we take the average of it and obtain (1/3 + 2/3)/2, equally 1/2.",
                "However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request.",
                "As a result, we expect the effect of the first hypothesis to be greater than that of the second.",
                "Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover.",
                "Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service.",
                "We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover.",
                "We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S. AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4.",
                "For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally 4/9.",
                "The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute.",
                "For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)}.",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating <br>semantic similarity</br> between two concepts.",
                "Estimating <br>semantic similarity</br> in a Is-A taxonomy can be done by calculating the distance between the nodes related to the compared concepts.",
                "The links among the nodes can be considered as distances.",
                "Then, the length of the path between the nodes indicates how closely similar the concepts are.",
                "An alternative estimation to use information content in estimation of <br>semantic similarity</br> rather than edge counting method, was proposed by Lin [8].",
                "The equation (3) [8] shows Lins similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them.",
                "Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C. Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmers Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS-A taxonomy [20].",
                "The <br>semantic similarity</br> is represented with Equation (4) [20].",
                "Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes.",
                "N1 is the number of edges between c1 and c0.",
                "N2 is the number of edges between c2 and c0.",
                "N0 is the number of IS-A links of c0 from the root of the taxonomy.",
                "SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions.",
                "We use Figure 2 to illustrate these intuitions. • Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that.",
                "Generalization of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept.",
                "The more general concepts are, the less similar they are.",
                "For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red.",
                "Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red. • Parent versus sibling: A node would have higher similarity to its parent than to its sibling.",
                "For instance, Red and Rose are children of ReddishColor.",
                "In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose. • Sibling versus grandparent: A node is more similar to its sibling then to its grandparent.",
                "To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings.",
                "Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red.",
                "As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept.",
                "At starting node related to the first concept, the similarity value is constant and equal to one.",
                "This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept.",
                "The shorter the path between the concepts, the higher the similarity between nodes.",
                "Algorithm 2 Estimate-RP-Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way.",
                "Starting from c1, the tree is traversed to reach c2.",
                "At each hop, the similarity decreases since the concepts are getting farther away from each other.",
                "However, based on our intuitions, not all hops decrease the similarity equally.",
                "Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling.",
                "Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .",
                "According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one.",
                "Algorithm 2 shows the distance calculation.",
                "According to the algorithm, firstly the similarity is initialized with the value of one (line 1).",
                "If the concepts are equal to each other then, similarity will be one (lines 2-4).",
                "Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5-7).",
                "If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts.",
                "For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation.",
                "As a result, we decrease the similarity at each level with the rate of m (line9).",
                "Otherwise, there has to be a sibling relation.",
                "This means that we have to consider the effect of n when measuring similarity.",
                "Recall that we have counted N1+N2 edges between the concepts.",
                "Since there is a sibling relation, two of these edges constitute the sibling relation.",
                "Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11).",
                "Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2.",
                "In this example, m is taken as 2/3 and n is taken as 4/7.",
                "Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524 For all <br>semantic similarity</br> metrics in our architecture, the taxonomy for features is held in the shared ontology.",
                "In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities.",
                "Then the result is equal to the average <br>semantic similarity</br> of the entire feature vector. 6.",
                "DEVELOPED SYSTEM We have implemented our architecture in Java.",
                "To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests.",
                "The producer agent is fully automated and the learning and service offering operations work as explained before.",
                "In this section, we explain the implementation details of the developed system.",
                "We use OWL [11] as our ontology language and JENA as our ontology reasoner.",
                "The shared ontology is the modified version of the Wine Ontology [19].",
                "It includes the description of wine as a concept and different types of wine.",
                "All participants of the negotiation use this ontology for understanding each other.",
                "According to the ontology, seven properties make up the wine concept.",
                "The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology.",
                "Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents.",
                "Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on.",
                "Intuitively, any wine type described in the ontology also represents a wine concept.",
                "This allows us to consider instances of Chardonnay wine as instances of Wine class.",
                "In addition to wine description, the hierarchical information of some features can be inferred from the ontology.",
                "For instance, we can represent the information Europe Continent covers Western Country.",
                "Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on.",
                "This hierarchical information is used in estimation of <br>semantic similarity</br>.",
                "In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z.",
                "For example, Europe Continent covers Bordeaux. 1306 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled.",
                "When that is the case, we give the reasonable similarity values for these features.",
                "For example, the body can be light, medium, or strong.",
                "In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong.",
                "WineStock Ontology is the producers inventory and describes a product class as WineProduct.",
                "This class is necessary for the producer to record the wines that it sells.",
                "Ontology involves the individuals of this class.",
                "The individuals represent available services that the producer owns.",
                "We have prepared two separate WineStock ontologies for testing.",
                "In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. 7.",
                "PERFORMANCE EVALUATION We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).",
                "We apply a variety of scenarios on this dataset in order to see the performance differences.",
                "Each test scenario contains a list of preferences for the user and number of matches from the product list.",
                "Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.",
                "Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.",
                "Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.",
                "Here, first we use Tverskys similarity measure.",
                "With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer.",
                "Since the performance also depends on the initial request, we repeat our experiments with different initial requests.",
                "Consequently, for each case, we run the algorithms five times with several variations of the initial requests.",
                "In each experiment, we count the number of iterations that were needed to reach an agreement.",
                "We take the average of these numbers in order to evaluate these systems fairly.",
                "As is customary, we test each algorithm with the same initial requests.",
                "Table 4 compares the approaches using different learning algorithm.",
                "When the large parts of inventory is compatible with the customers preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).",
                "As the number of compatible services drops, RO performs poorly as expected.",
                "The second worst method is SCR since it only considers the customers most recent request and does not learn from previous requests.",
                "CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.",
                "ID3 and DCEA achieve the best results.",
                "Their performance is comparable and they can handle all cases including Scenario 5.",
                "Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA.",
                "In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information. • The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.",
                "Moreover, the winery of the wine should not be expensive.",
                "There are only four products meeting these conditions. • The customer wants to buy wine whose color is red or rose and grape type is red grape.",
                "In addition, the location of wine should be in Europe.",
                "The sweetness degree is wished to be dry or off dry.",
                "The flavor should be delicate or moderate where the body should be medium or light.",
                "Furthermore, the winery of the wine should be an expensive winery.",
                "There are two products meeting all these requirements. • The customer wants to buy moderate rose wine, which is located around French Region.",
                "The category of winery should be Moderate Winery.",
                "There is only one product meeting these requirements. • The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region.",
                "There are five available products. • The customer wants to buy delicate white wine whose producer in the category of Expensive Winery.",
                "There are two available products.",
                "The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.",
                "Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.",
                "Tverskys metric gives the worst results since it does not consider the <br>semantic similarity</br>.",
                "Lins performance are better than Tversky but worse than others.",
                "Wu Palmers metric and RP similarity measure nearly give the same performance and better than others.",
                "When the results are examined, considering semantic closeness increases the performance. 8.",
                "DISCUSSION We review the recent literature in comparison to our work.",
                "Tama et al. [16] propose a new approach based on ontology for negotiation.",
                "According to their approach, the negotiation protocols used in e-commerce can be modeled as ontologies.",
                "Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details.",
                "While The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al. model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated.",
                "Further, we have built a system with which negotiation preferences can be learned.",
                "Sadri et al. study negotiation in the context of resource allocation [14].",
                "Agents have limited resources and need to require missing resources from other agents.",
                "A mechanism which is based on dialogue sequences among agents is proposed as a solution.",
                "The mechanism relies on observe-think-action agent cycle.",
                "These dialogues include offering resources, resource exchanges and offering alternative resource.",
                "Each agent in the system plans its actions to reach a goal state.",
                "Contrary to our approach, Sadri et al.s study is not concerned with learning preferences of each other.",
                "Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi-attribute negotiations [1].",
                "For achieving this, they use case-based reasoning.",
                "Their approach is probabilistic since the behavior of the partners can change at each iteration.",
                "In our approach, we are interested in negotiation the content of the service.",
                "After the consumer and producer agree on the service, price-oriented negotiation mechanisms can be used to agree on the price.",
                "Fatima et al. study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5].",
                "In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price.",
                "Time interval affects these agents differently.",
                "Compared to Fatima et al. our focus is different.",
                "While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation.",
                "Faratin et al. propose a multi-issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded-offs against each other (i.e., higher price for earlier delivery) [4].",
                "They generate a heuristic model for trade-offs including fuzzy similarity estimation and a hill-climbing exploration for possibly acceptable offers.",
                "Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter-offers in accordance with these learned preferences.",
                "Faratin et al. only use the last offer made by the consumer in calculating the similarity for choosing counter offer.",
                "Unlike them, we also take into account the previous requests of the consumer.",
                "In their experiments, Faratin et al. assume that the weights for service variables are fixed a priori.",
                "On the contrary, we learn these preferences over time.",
                "In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations.",
                "Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge.",
                "These are interesting directions that we will pursue in our future work. 9.",
                "REFERENCES [1] J. Brzostowski and R. Kowalczyk.",
                "On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation.",
                "In Proceedings of the 4th Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273-278, 2005. [2] L. Busch and I. Horstman.",
                "A comment on issue-by-issue negotiations.",
                "Games and Economic Behavior, 19:144-148, 1997. [3] J. K. Debenham.",
                "Managing e-market negotiation in context with a multiagent system.",
                "In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES2002:, 2002. [4] P. Faratin, C. Sierra, and N. R. Jennings.",
                "Using similarity criteria to make issue trade-offs in automated negotiations.",
                "Artificial Intelligence, 142:205-237, 2002. [5] S. Fatima, M. Wooldridge, and N. Jennings.",
                "Optimal agents for multi-issue negotiation.",
                "In Proceeding of the 2nd Intl.",
                "Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129-136, 2003. [6] C. Giraud-Carrier.",
                "A note on the utility of incremental learning.",
                "AI Communications, 13(4):215-223, 2000. [7] T.-P. Hong and S.-S. Tseng.",
                "Splitting and merging version spaces to learn disjunctive concepts.",
                "IEEE Transactions on Knowledge and Data Engineering, 11(5):813-815, 1999. [8] D. Lin.",
                "An information-theoretic definition of similarity.",
                "In Proc. 15th International Conf. on Machine Learning, pages 296-304.",
                "Morgan Kaufmann, San Francisco, CA, 1998. [9] P. Maes, R. H. Guttman, and A. G. Moukas.",
                "Agents that buy and sell.",
                "Communications of the ACM, 42(3):81-91, 1999. [10] T. M. Mitchell.",
                "Machine Learning.",
                "McGraw Hill, NY, 1997. [11] OWL.",
                "OWL: Web ontology language guide, 2003. http://www.w3.org/TR/2003/CR-owl-guide-20030818/. [12] S. K. Pal and S. C. K. Shiu.",
                "Foundations of Soft Case-Based Reasoning.",
                "John Wiley & Sons, New Jersey, 2004. [13] J. R. Quinlan.",
                "Induction of decision trees.",
                "Machine Learning, 1(1):81-106, 1986. [14] F. Sadri, F. Toni, and P. Torroni.",
                "Dialogues for negotiation: Agent varieties and dialogue sequences.",
                "In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405-421.",
                "Springer-Verlag, 2002. [15] M. P. Singh.",
                "Value-oriented electronic commerce.",
                "IEEE Internet Computing, 3(3):6-7, 1999. [16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.",
                "Ontologies for supporting negotiation in e-commerce.",
                "Engineering Applications of Artificial Intelligence, 18:223-236, 2005. [17] A. Tversky.",
                "Features of similarity.",
                "Psychological Review, 84(4):327-352, 1977. [18] P. E. Utgoff.",
                "Incremental induction of decision trees.",
                "Machine Learning, 4:161-186, 1989. [19] Wine, 2003. http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf. [20] Z. Wu and M. Palmer.",
                "Verb semantics and lexical selection.",
                "In 32nd.",
                "Annual Meeting of the Association for Computational Linguistics, pages 133 -138, 1994. 1308 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [
                "Learning Consumer Preferences Using <br>semantic similarity</br> ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Department of Computer Engineering Bo˘gaziçi University Bebek, 34342, Istanbul,Turkey ABSTRACT In online, dynamic environments, the services requested by consumers may not be readily served by the providers.",
                "We further develop a metric for measuring <br>semantic similarity</br> between services and compare the performance of our approach using different similarity metrics.",
                "If the producer has some domain knowledge about <br>semantic similarity</br> (e.g., knows that the red and rose wines are taste-wise more similar than white wine), then it can generate better offers.",
                "We first analyze some existing metrics and then propose a new <br>semantic similarity</br> metric named RP Similarity. 5.1 Tverskys Similarity Metric Tverskys similarity metric compares two vectors in terms of the number of exactly matching features [17].",
                "When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2). 5.2 Lins Similarity Metric A taxonomy can be used while estimating <br>semantic similarity</br> between two concepts."
            ],
            "translated_annotated_samples": [
                "Aprendiendo las preferencias del consumidor utilizando <br>similitud semántica</br> ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores.",
                "Desarrollamos aún más una métrica para medir la <br>similitud semántica</br> entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud.",
                "Si el productor tiene cierto conocimiento del dominio sobre la <br>similitud semántica</br> (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas.",
                "Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de <br>similitud semántica</br> llamada Similitud RP. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente.",
                "Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La métrica de similitud de Lins Un taxonomía puede ser utilizada al estimar la <br>similitud semántica</br> entre dos conceptos."
            ],
            "translated_text": "Aprendiendo las preferencias del consumidor utilizando <br>similitud semántica</br> ∗ Reyhan Aydo˘gan reyhan.aydogan@gmail.com Pınar Yolum pinar.yolum@boun.edu.tr Departamento de Ingeniería Informática Universidad Bo˘gaziçi Bebek, 34342, Estambul, Turquía RESUMEN En entornos en línea y dinámicos, los servicios solicitados por los consumidores pueden no ser atendidos de inmediato por los proveedores. Esto requiere que los consumidores y proveedores de servicios negocien sus necesidades y ofertas de servicio. Los enfoques de negociación multiagente suelen asumir que las partes están de acuerdo en el contenido del servicio y se centran en encontrar un consenso sobre el precio del servicio. Por el contrario, este trabajo desarrolla un enfoque a través del cual las partes pueden negociar el contenido de un servicio. Esto requiere un enfoque de negociación en el que las partes puedan entender la semántica de sus solicitudes y ofertas, y aprender gradualmente las preferencias de los demás con el tiempo. En consecuencia, proponemos una arquitectura en la que tanto los consumidores como los productores utilicen una ontología compartida para negociar un servicio. A través de interacciones repetitivas, el proveedor aprende con precisión las necesidades de los consumidores y puede hacer ofertas más dirigidas. Para permitir un aprendizaje rápido y preciso de las preferencias, desarrollamos una extensión al Espacio de Versiones y lo comparamos con técnicas de aprendizaje existentes. Desarrollamos aún más una métrica para medir la <br>similitud semántica</br> entre servicios y comparamos el rendimiento de nuestro enfoque utilizando diferentes métricas de similitud. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagente Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Los enfoques actuales del comercio electrónico tratan el precio del servicio como el principal elemento para la negociación al asumir que el contenido del servicio está fijo [9]. Sin embargo, la negociación sobre el precio presupone que otras propiedades del servicio ya han sido acordadas. Sin embargo, muchas veces el proveedor de servicios puede no estar ofreciendo el servicio exactamente solicitado debido a la falta de recursos, limitaciones en su política empresarial, y así sucesivamente [3]. Cuando esto sucede, el productor y el consumidor necesitan negociar el contenido del servicio solicitado [15]. Sin embargo, la mayoría de los enfoques de negociación existentes asumen que todas las características de un servicio son igualmente importantes y se centran en el precio [5, 2]. Sin embargo, en realidad no todas las características pueden ser relevantes y la relevancia de una característica puede variar de un consumidor a otro. Por ejemplo, el tiempo de finalización de un servicio puede ser importante para un consumidor, mientras que la calidad del servicio puede ser más importante para otro consumidor. Sin duda, tener en cuenta las preferencias del consumidor tiene un impacto positivo en el proceso de negociación. Para este propósito, la evaluación de los componentes del servicio con diferentes pesos puede ser útil. Algunos estudios toman estos pesos como a priori y utilizan los pesos fijos [4]. Por otro lado, en su mayoría el productor no conoce las preferencias de los consumidores antes de la negociación. Por lo tanto, es más apropiado que el productor conozca estas preferencias de cada consumidor. Aprendizaje de preferencias: Como alternativa, proponemos una arquitectura en la que los proveedores de servicios aprenden las características relevantes de un servicio para un cliente en particular con el tiempo. Representamos las solicitudes de servicio como un vector de características del servicio. Utilizamos una ontología para capturar las relaciones entre servicios y construir las características para un servicio dado. Al utilizar una ontología común, permitimos a los consumidores y productores compartir un vocabulario común para la negociación. El servicio en particular que hemos utilizado es un servicio de venta de vinos. El vendedor de vinos aprende las preferencias de vino del cliente para vender vinos más dirigidos. El productor modela las solicitudes del consumidor y sus contraofertas para aprender qué características son más importantes para el consumidor. Dado que no hay información presente antes de que comiencen las interacciones, el algoritmo de aprendizaje debe ser incremental para que pueda ser entrenado en tiempo de ejecución y pueda revisarse a sí mismo con cada nueva interacción. Generación de servicios: Incluso después de que el productor aprende las características importantes para un consumidor, necesita un método para generar ofertas que sean las más relevantes para el consumidor entre su conjunto de posibles servicios. En otras palabras, la pregunta es cómo el productor utiliza la información que se obtuvo de los diálogos para hacer la mejor oferta al consumidor. Por ejemplo, supongamos que el productor ha descubierto que el consumidor quiere comprar un vino tinto pero el productor solo puede ofrecer vino rosado o blanco. ¿Qué deberían ofrecer los productores 1301 978-81-904262-7-5 (RPS) c 2007 IFAAMAS; vino blanco o vino rosado? Si el productor tiene cierto conocimiento del dominio sobre la <br>similitud semántica</br> (por ejemplo, sabe que los vinos tinto y rosado son más similares en sabor que el vino blanco), entonces puede generar mejores ofertas. Sin embargo, además del conocimiento del dominio, esta derivación requiere métricas apropiadas para medir la similitud entre los servicios disponibles y las preferencias aprendidas. El resto de este documento está organizado de la siguiente manera: la Sección 2 explica nuestra arquitectura propuesta. La sección 3 explica los algoritmos de aprendizaje que se estudiaron para aprender las preferencias del consumidor. La sección 4 estudia los diferentes mecanismos de oferta de servicios. La sección 5 contiene las métricas de similitud utilizadas en los experimentos. Los detalles del sistema desarrollado se analizan en la Sección 6. La sección 7 proporciona nuestra configuración experimental, casos de prueba y resultados. Finalmente, la Sección 8 discute y compara nuestro trabajo con otros trabajos relacionados. 2. Nuestra arquitectura principal está compuesta por agentes consumidores y productores, los cuales se comunican entre sí para llevar a cabo negociaciones orientadas al contenido. La Figura 1 representa nuestra arquitectura. El agente del consumidor representa al cliente y, por lo tanto, tiene acceso a las preferencias del cliente. El agente del consumidor genera solicitudes de acuerdo con estas preferencias y negocia con el productor basándose en estas preferencias. De igual manera, el agente productor tiene acceso al inventario de los productores y sabe qué vinos están disponibles o no. Una ontología compartida proporciona el vocabulario necesario y, por lo tanto, permite un lenguaje común para los agentes. Esta ontología describe el contenido del servicio. Además, dado que una ontología puede representar conceptos, sus propiedades y sus relaciones semánticamente, los agentes pueden razonar los detalles del servicio que se está negociando. Dado que un servicio puede ser cualquier cosa, como vender un coche, reservar una habitación de hotel, etc., la arquitectura es independiente de la ontología utilizada. Sin embargo, para hacer nuestra discusión concreta, utilizamos la conocida ontología del Vino [19] con algunas modificaciones para ilustrar nuestras ideas y probar nuestro sistema. La ontología del vino describe diferentes tipos de vino e incluye características como color, cuerpo, bodega del vino, entre otros. Con esta ontología, el servicio que se está negociando entre el consumidor y el productor es el de vender vino. El repositorio de datos en la Figura 1 es utilizado únicamente por el agente productor y contiene la información del inventario del productor. El repositorio de datos incluye información sobre los productos que posee el productor, el número de productos y las calificaciones de esos productos. Las calificaciones indican la popularidad de los productos entre los clientes. Esos se utilizan para decidir qué producto se ofrecerá cuando existen más de un producto con la misma similitud a la solicitud del agente del consumidor. La negociación se lleva a cabo de manera secuencial, donde el agente consumidor inicia la negociación con una solicitud de servicio particular. La solicitud está compuesta por características significativas del servicio. En el ejemplo del vino, estas características incluyen el color, la bodega y demás. Este es el vino en particular que el cliente está interesado en comprar. Si el productor tiene el vino solicitado en su inventario, el productor ofrece el vino y la negociación termina. De lo contrario, el productor ofrece un vino alternativo del inventario. Cuando el consumidor recibe una contraoferta del productor, la evaluará. Si es aceptable, entonces la negociación terminará. De lo contrario, el cliente generará una nueva solicitud o se mantendrá en la solicitud anterior. Este proceso continuará hasta que algún servicio sea aceptado por el agente del consumidor o todas las ofertas posibles sean presentadas al consumidor por el productor. Uno de los desafíos cruciales de la negociación orientada al contenido es la generación automática de contraofertas por parte del productor de servicios. Cuando el productor construye su oferta, debe considerar tres cosas importantes: la solicitud actual, las preferencias del consumidor y los servicios disponibles del productor, tal como se muestra en la Figura 1: Arquitectura de Negociación Propuesta. Tanto la solicitud actual del consumidor como los servicios disponibles del productor son accesibles para el productor. Sin embargo, las preferencias de los consumidores en la mayoría de los casos no estarán disponibles. Por lo tanto, el productor tendrá que entender las necesidades del consumidor a partir de sus interacciones y generar una contraoferta que probablemente sea aceptada por el consumidor. Este desafío se puede estudiar en tres etapas: • Aprendizaje de preferencias: ¿Cómo pueden los productores aprender sobre las preferencias de cada cliente basándose en solicitudes y contraofertas? (Sección 3) • Oferta de servicios: ¿Cómo pueden los productores revisar sus ofertas basándose en las preferencias de los consumidores que han aprendido hasta ahora? (Sección 4) • Estimación de similitud: ¿Cómo puede el agente productor estimar la similitud entre la solicitud y los servicios disponibles? (Sección 5) APRENDIZAJE DE PREFERENCIAS Las solicitudes del consumidor y las contraofertas del productor se representan como vectores, donde cada elemento en el vector corresponde al valor de una característica. Las solicitudes de los consumidores representan productos de vino individuales, mientras que sus preferencias son restricciones sobre las características del servicio. Por ejemplo, un consumidor puede tener preferencia por el vino tinto. Esto significa que el consumidor está dispuesto a aceptar cualquier vino ofrecido por los productores siempre y cuando el color sea rojo. Por lo tanto, el consumidor genera una solicitud donde la característica de color se establece en rojo y otras características se establecen en valores arbitrarios, por ejemplo (Medio, Fuerte, Rojo). Al principio de la negociación, el agente del productor no conoce las preferencias del consumidor, pero necesitará aprenderlas utilizando la información obtenida de los diálogos entre el productor y el consumidor. Las preferencias denotan la importancia relativa de las características de los servicios demandados por los agentes consumidores. Por ejemplo, el color del vino puede ser importante, por lo que el consumidor insiste en comprar el vino cuyo color es rojo y rechaza todos los 1302 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Cómo funciona DCEA Tipo Muestra El conjunto más general El conjunto más específico + (Completo,Fuerte,Blanco) {(?, ?, ?)} {(Completo,Fuerte,Blanco)} {{(?-Completo), ?, ? }, - (Completo,Delicado,Rosa) {?, (?-Delicado), ? }, {(Completo,Fuerte,Blanco)} {?, ?, (?-Rosa)}} {{(?-Completo), ?, ? }, {{(Completo,Fuerte,Blanco)}, + (Medio,Moderado,Rojo) {?,(?-Delicado), ? }, {(Medio,Moderado,Rojo)}} {?, ?, (?-Rosa)}} las ofertas que involucran el vino cuyo color es blanco o rosa. Por el contrario, la bodega puede que no sea tan importante como el color para este cliente, por lo que el consumidor puede tener tendencia a aceptar vinos de cualquier bodega siempre y cuando el color sea rojo. Para abordar este problema, proponemos utilizar algoritmos de aprendizaje incremental [6]. Esto es necesario ya que no hay datos de entrenamiento disponibles antes de que comiencen las interacciones. Investigamos particularmente dos enfoques. El primero es el aprendizaje inductivo. Esta técnica se aplica para aprender las preferencias como conceptos. Desarrollamos el Algoritmo de Eliminación de Candidatos (CEA) para el Espacio de Versiones [10]. Se sabe que CEA tiene un rendimiento deficiente si la información que se va a aprender es disyuntiva. Curiosamente, la mayoría de las veces las preferencias del consumidor son disyuntivas. Estamos considerando un agente que está comprando vino. El consumidor puede preferir vino tinto o vino rosado pero no vino blanco. Para utilizar CEA con tales preferencias, es necesaria una modificación sólida. El segundo enfoque son los árboles de decisión. Los árboles de decisión pueden aprender fácilmente a partir de ejemplos y clasificar nuevas instancias como positivas o negativas. Un árbol de decisión incremental bien conocido es ID5R [18]. Sin embargo, se sabe que ID5R sufre de una alta complejidad computacional. Por esta razón, en su lugar utilizamos el algoritmo ID3 [13] y construimos de forma iterativa árboles de decisión para simular el aprendizaje incremental. CEA [10] es uno de los algoritmos de aprendizaje inductivo que aprende conceptos a partir de ejemplos observados. El algoritmo mantiene dos conjuntos para modelar el concepto que se va a aprender. El primer conjunto es el conjunto más general G. G contiene hipótesis sobre todos los posibles valores que el concepto puede obtener. Como su nombre indica, es una generalización y contiene todos los valores posibles a menos que se haya identificado que los valores no representan el concepto. El segundo conjunto es el conjunto S más específico. S solo contiene hipótesis que se sabe que identifican el concepto que se está aprendiendo. Al comienzo del algoritmo, G se inicializa para cubrir todos los conceptos posibles mientras que S se inicializa como vacío. Durante las interacciones, cada solicitud del consumidor puede considerarse como un ejemplo positivo y cada contraoferta generada por el productor y rechazada por el agente del consumidor puede ser considerada como un ejemplo negativo. En cada interacción entre el productor y el consumidor, tanto G como S son modificados. Las muestras negativas refuerzan la especialización de algunas hipótesis para que G no cubra ninguna hipótesis que acepte las muestras negativas como positivas. Cuando llega una muestra positiva, el conjunto S más específico debe generalizarse para cubrir la nueva instancia de entrenamiento. Como resultado, las hipótesis más generales y las hipótesis más específicas cubren todas las muestras de entrenamiento positivas pero no cubren ninguna negativa. Incrementalmente, G se especializa y S se generaliza hasta que G y S sean iguales entre sí. Cuando estos conjuntos son iguales, el algoritmo converge al alcanzar el concepto objetivo. 3.2 CEA Disyuntivo Desafortunadamente, CEA está principalmente dirigido a conceptos conjuntivos. Por otro lado, necesitamos aprender conceptos disyuntivos en la negociación de un servicio ya que el consumidor puede tener varios deseos alternativos. Hay varios estudios sobre el aprendizaje de conceptos disyuntivos a través del Espacio de Versiones. Algunos de estos enfoques utilizan múltiples espacios de versión. Por ejemplo, Hong et al. mantienen varios espacios de versión mediante operaciones de división y fusión [7]. Para poder aprender conceptos disyuntivos, crean nuevos espacios de versión examinando la consistencia entre G y S. Nos ocupamos del problema de no admitir conceptos disyuntivos de CEA al extender nuestro lenguaje de hipótesis para incluir hipótesis disyuntivas además de las conjunciones y la negación. Cada atributo de la hipótesis tiene dos partes: la lista inclusiva, que contiene la lista de valores válidos para ese atributo, y la lista exclusiva, que es la lista de valores que no pueden ser tomados para esa característica. EJEMPLO 1. Suponga que el conjunto más específico es {(Luz, Delicado, Rojo)} y llega un ejemplo positivo, (Luz, Delicado, Blanco). El CEA original generalizará esto como (Claro, Delicado, ?), lo que significa que el color puede tomar cualquier valor. Sin embargo, de hecho, solo sabemos que el color puede ser rojo o blanco. En el DCEA, lo generalizamos como {(Claro, Delicado, [Blanco, Rojo])}. Solo cuando todos los valores existan en la lista, serán reemplazados por ?. En otras palabras, permitimos que el algoritmo generalice más lentamente que antes. Modificamos el algoritmo CEA para hacer frente a este cambio. El algoritmo modificado, DCEA, se presenta como Algoritmo 1. Nótese que, en comparación con los estudios anteriores de versiones disyuntivas, nuestro enfoque utiliza solo un espacio de versiones en lugar de múltiples espacios de versiones. La fase de inicialización es la misma que el algoritmo original (líneas 1, 2). Si llega alguna muestra positiva, agregamos la muestra al conjunto especial como antes (línea 4). Sin embargo, no eliminamos las hipótesis en G que no cubren esta muestra, ya que G ahora contiene una disyunción de muchas hipótesis, algunas de las cuales entrarán en conflicto entre sí. Eliminar una hipótesis específica de G resultará en la pérdida de información, ya que no se garantiza que otras hipótesis la cubran. Después de algún tiempo, algunas hipótesis en S pueden fusionarse y construir una hipótesis (líneas 6, 7). Cuando llega una muestra negativa, no cambiamos S como antes. Solo modificamos las hipótesis más generales para no cubrir esta muestra negativa (líneas 11-15). A diferencia del CEA original, intentamos especializar el G mínimamente. El algoritmo elimina la hipótesis que cubre la muestra negativa (línea 13). Luego, generamos nuevas hipótesis utilizando el número de todos los atributos posibles mediante el uso de la hipótesis eliminada. Para cada atributo en la muestra negativa, agregamos uno de ellos a la lista exclusiva de hipótesis eliminadas cada vez. Por lo tanto, se generan todas las hipótesis posibles que no cubren la muestra negativa (línea 14). Ten en cuenta que la lista exclusiva contiene los valores que el atributo no puede tomar. Por ejemplo, considera el atributo del color. Si una hipótesis incluye rojo en su lista exclusiva y ? en su lista inclusiva, esto significa que el color puede tomar cualquier valor excepto rojo. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Algoritmo 1: Algoritmo de Eliminación de Candidatos Disyuntivos 1: G ← el conjunto de hipótesis maximalmente generales en H 2: S ← el conjunto de hipótesis maximalmente específicas en H 3: Para cada ejemplo de entrenamiento, d 4: si d es un ejemplo positivo entonces 5: Agregar d a S 6: si s en S puede combinarse con d para formar un solo elemento entonces 7: Combinar s y d en sd {sd es la regla que cubre s y d} 8: fin si 9: fin si 10: si d es un ejemplo negativo entonces 11: Para cada hipótesis g en G que cubre d 12: * Suponer: g = (x1, x2, ..., xn) y d = (d1, d2, ..., dn) 13: - Eliminar g de G 14: - Agregar hipótesis g1, g2, gn donde g1 = (x1-d1, x2,..., xn), g2 = (x1, x2-d2,..., xn),..., y gn = (x1, x2,..., xn-dn) 15: - Eliminar de G cualquier hipótesis que sea menos general que otra hipótesis en G 16: fin si EJEMPLO 2. La Tabla 1 ilustra las primeras tres interacciones y el funcionamiento de DCEA. El conjunto más general y el conjunto más específico muestran los contenidos de G y S después de que llega la muestra. Después de la primera muestra positiva, S se generaliza para cubrir también la instancia. La segunda muestra es negativa. Por lo tanto, reemplazamos (?, ?, ?) por tres hipótesis disyuntivas; cada hipótesis siendo mínimamente especializada. En este proceso, en cada momento se aplica un valor de atributo de muestra negativa a la hipótesis en el conjunto general. La tercera muestra es positiva y generaliza S aún más. Ten en cuenta que en la Tabla 1, no eliminamos {(?-Completo), ?, ?} del conjunto general al tener una muestra positiva como (Completo, Fuerte, Blanco). Esto se deriva de la posibilidad de utilizar esta regla en la generación de otras hipótesis. Por ejemplo, si el ejemplo continúa con una muestra negativa (Lleno, Fuerte, Rojo), podemos especializar la regla anterior como {(?-Lleno), ?, (?-Rojo)}. Por el Algoritmo 1, no perdemos ninguna información. 3.3 ID3 ID3 [13] es un algoritmo que construye árboles de decisión de manera descendente a partir de los ejemplos observados representados en un vector con pares atributo-valor. Aplicar este algoritmo a nuestro sistema con la intención de aprender las preferencias de los consumidores es apropiado, ya que este algoritmo también admite el aprendizaje de conceptos disyuntivos además de conceptos conjuntivos. El algoritmo ID3 se utiliza en el proceso de aprendizaje con el propósito de clasificar ofertas. Hay dos clases: positiva y negativa. Positivo significa que la descripción del servicio posiblemente será aceptada por el agente del consumidor, mientras que el negativo implica que potencialmente será rechazada por el consumidor. Las solicitudes de los consumidores se consideran como ejemplos de entrenamiento positivos y todas las contraofertas rechazadas se consideran como negativas. El árbol de decisión tiene dos tipos de nodos: nodo hoja en el que se almacenan las etiquetas de clase de las instancias y nodos no hoja en los que se almacenan los atributos de prueba. El atributo de prueba en un nodo no hoja es uno de los atributos que conforman la descripción del servicio. Por ejemplo, el cuerpo, sabor, color, entre otros, son atributos potenciales para la degustación de vinos. Cuando queremos determinar si la descripción del servicio proporcionada es aceptable, comenzamos buscando desde el nodo raíz examinando el valor de los atributos de prueba hasta llegar a un nodo hoja. El problema con este algoritmo es que no es un algoritmo incremental, lo que significa que todos los ejemplos de entrenamiento deben existir antes de aprender. Para superar este problema, el sistema mantiene las solicitudes de los consumidores a lo largo de la interacción de negociación como ejemplos positivos y todas las contraofertas rechazadas por el consumidor como ejemplos negativos. Después de cada solicitud entrante, el árbol de decisiones se reconstruye. Sin duda, hay una desventaja de la reconstrucción, como una carga adicional en el proceso. Sin embargo, en la práctica hemos evaluado que el ID3 es rápido y el costo de reconstrucción es insignificante. 4. OFERTA DE SERVICIO Después de conocer las preferencias de los consumidores, el productor necesita hacer una contraoferta que sea compatible con las preferencias de los consumidores. 4.1 Oferta de Servicio a través de CEA y DCEA Para generar la mejor oferta, el agente productor utiliza su ontología de servicios y el algoritmo CEA. El mecanismo de oferta de servicios es el mismo tanto para el CEA original como para el DCEA, pero como se explicó anteriormente, sus métodos para actualizar G y S son diferentes. Cuando el productor recibe una solicitud del consumidor, el conjunto de aprendizaje del productor se entrena con esta solicitud como una muestra positiva. Los componentes de aprendizaje, el conjunto más específico S y el conjunto más general G se utilizan activamente en la prestación de servicios. El conjunto más general, G, es utilizado por el productor para evitar ofrecer los servicios que serán rechazados por el agente consumidor. En otras palabras, filtra el conjunto de servicios de los servicios no deseados, ya que G contiene hipótesis que son consistentes con las solicitudes del consumidor. El conjunto más específico, S, se utiliza para encontrar la mejor oferta, que es similar a las preferencias de los consumidores. Dado que el conjunto más específico S contiene las solicitudes anteriores y la solicitud actual, estimar la similitud entre este conjunto y cada servicio en la lista de servicios es muy conveniente para encontrar la mejor oferta de la lista de servicios. Cuando el consumidor inicia la interacción con el agente productor, el agente productor carga todos los servicios relacionados en el objeto de lista de servicios. Esta lista constituye el inventario de servicios de los proveedores. Al recibir una solicitud, si el productor puede ofrecer un servicio exactamente coincidente, entonces lo hace. Por ejemplo, para un vino esto corresponde a vender un vino que coincida exactamente con las características especificadas en la solicitud del consumidor. Cuando el productor no puede ofrecer el servicio solicitado, intenta encontrar el servicio que sea más similar a los servicios solicitados por el consumidor durante la negociación. Para hacer esto, el productor tiene que calcular la similitud entre los servicios que puede ofrecer y los servicios que han sido solicitados (en S). Calculamos las similitudes de varias maneras, como se explicará en la Sección 5. Después de calcular la similitud de los servicios disponibles con el actual S, puede haber más de un servicio con la máxima similitud. El agente productor puede romper el empate de varias maneras. Aquí, hemos asociado un valor de calificación con cada servicio y el productor prefiere el servicio con la calificación más alta sobre los demás. 4.2 Oferta de Servicio a través de ID3 Si el productor aprende las preferencias de los consumidores con ID3, se aplica un mecanismo similar con dos diferencias. Primero, dado que ID3 no mantiene G, se eliminan de la lista de servicios aquellos no aceptados que se clasifican como negativos. Segundo, las similitudes de los posibles servicios no se miden con respecto a S, sino en cambio a todas las solicitudes previamente realizadas. 4.3 Mecanismos Alternativos de Oferta de Servicios Además de estos tres mecanismos de oferta de servicios (Oferta de Servicio con CEA, Oferta de Servicio con DCEA y Oferta de Servicio con ID3), incluimos otros dos mecanismos. 1304 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) • Oferta de Servicio Aleatoria (RO): El productor genera una contraoferta aleatoriamente de la lista de servicios disponibles, sin considerar las preferencias de los consumidores. • Oferta de Servicio considerando solo la solicitud actual (SCR): El productor selecciona una contraoferta de acuerdo con la similitud de la solicitud actual del consumidor pero no considera solicitudes anteriores. 5. ESTIMACIÓN DE SIMILITUD La similitud puede ser estimada con una métrica de similitud que toma dos entradas y devuelve qué tan similares son. Existen varios métricos de similitud utilizados en sistemas de razonamiento basado en casos, como la suma ponderada de la distancia euclidiana, la distancia de Hamming, entre otros [12]. La métrica de similitud afecta el rendimiento del sistema al decidir qué servicio es el más cercano a la solicitud del consumidor. Primero analizamos algunas métricas existentes y luego proponemos una nueva métrica de <br>similitud semántica</br> llamada Similitud RP. La métrica de similitud de Tversky compara dos vectores en términos del número de características que coinciden exactamente. En la Ecuación (1), común representa la cantidad de atributos coincidentes, mientras que diferente representa la cantidad de atributos diferentes. Nuestra suposición actual es que α y β son iguales entre sí. SMpq = α(común) α(común) + β(diferente) (1) Aquí, al comparar dos características, asignamos cero para la disimilitud y uno para la similitud al omitir la cercanía semántica entre los valores de las características. La métrica de similitud de Tversky está diseñada para comparar dos vectores de características. En nuestro sistema, mientras que la lista de servicios que puede ofrecer el productor son cada uno un vector de características, el conjunto más específico S no es un vector de características. S consiste en hipótesis de vectores de características. Por lo tanto, estimamos la similitud de cada hipótesis dentro del conjunto más específico S y luego calculamos el promedio de las similitudes. EJEMPLO 3. Suponga que S contiene las siguientes dos hipótesis: { {Luz, Moderado, (Rojo, Blanco)} , {Completo, Fuerte, Rosa}}. Toma el servicio s como (Ligero, Resistente, Rosa). Entonces, la similitud del primero es igual a 1/3 y la del segundo es igual a 2/3 de acuerdo con la Ecuación (1). Normalmente, tomamos el promedio de ello y obtenemos (1/3 + 2/3)/2, que es igual a 1/2. Sin embargo, la primera hipótesis implica el efecto de dos solicitudes y la segunda hipótesis implica solo una solicitud. Por lo tanto, esperamos que el efecto de la primera hipótesis sea mayor que el de la segunda. Por lo tanto, calculamos la similitud promedio teniendo en cuenta la cantidad de muestras que las hipótesis cubren. Que ch denote el número de muestras que cubre la hipótesis h y (SM(h,servicio)) denote la similitud de la hipótesis h con el servicio dado. Calculamos la similitud de cada hipótesis con el servicio dado y las ponderamos con el número de muestras que cubren. Encontramos la similitud dividiendo la suma ponderada de las similitudes de todas las hipótesis en S con el servicio por el número de todas las muestras que están cubiertas en S. AV G−SM(servicio, S) = |S| |h| (ch ∗ SM(h, servicio)) |S| |h| ch (2) Figura 2: Taxonomía de muestra para estimación de similitud EJEMPLO 4. Para el ejemplo anterior, la similitud de (Luz, Fuerte, Rosa) con el conjunto específico es (2 ∗ 1/3 + 2/3)/3, igual a 4/9. El número posible de muestras que abarca una hipótesis se puede estimar multiplicando las cardinalidades de cada atributo. Por ejemplo, la cardinalidad del primer atributo es dos y la de los demás es igual a uno para la hipótesis dada, como {Luz, Moderado, (Rojo, Blanco)}. Cuando los multiplicamos, obtenemos dos (2 ∗ 1 ∗ 1 = 2). 5.2 La métrica de similitud de Lins Un taxonomía puede ser utilizada al estimar la <br>similitud semántica</br> entre dos conceptos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}