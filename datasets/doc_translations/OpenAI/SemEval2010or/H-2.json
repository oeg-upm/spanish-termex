{
    "id": "H-2",
    "original_text": "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr. 9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval. In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output. We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri. Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings. Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query. A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1. INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web. Yet keyword queries are inherently ambiguous. The query canon book for example covers several different areas of interest: religion, photography, literature, and music. Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category. Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones. Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly. It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]). This is exactly the Web search scenario! In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc. Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably). First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user. Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy. Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search). Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output. After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1. We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best. In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri. The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%. In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query. This yields an additional improvement of 8.47% over the previously identified best algorithm. We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs. However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2. PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion. There exists a vast amount of algorithms for both domains. However, not much has been done specifically aimed at combining them. In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm. This section splits the relevant background according to the focus of each article into either one of these elements. Approaches focused on the User Profile. Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages. Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile. Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13]. User profiling based on browsing history has the advantage of being rather easy to obtain and process. This is probably why it is also employed by several industrial search engines (e.g., Yahoo! MyWeb2 ). However, it is definitely not sufficient for gathering a thorough insight into users interests. More, it requires to store all personal information at the server side, which raises significant privacy concerns. Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests. Moreover, none of these investigated the adaptive application of personalization. Approaches focused on the Personalization Algorithm. Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently. Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics. More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings. Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination. As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval. It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output. In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information. Some other approaches are also addressed in the end of the section. Relevance Feedback Techniques. The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query. First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents. Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc. We used some of these as inspiration for our Desktop specific techniques. Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary. RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF). Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query. Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection. Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein. Co-occurrence Based Techniques. Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17]. Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc. We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository. Thesaurus Based Techniques. A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords. Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined. Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36]. Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22]. We also use WordNet based expansion terms. However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords. Other Techniques. There are many other attempts to extract expansion terms. Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs. Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3. QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information. However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics. In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository. Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search. In the proposed algorithms we gradually increase the amount of personal information utilized. Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best. We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents. In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results. We distinguish three granularity levels for this process and we investigate each of them separately. Term and Document Frequency. As the simplest possible measures, TF and DF have the advantage of being very fast to compute. Previous experiments with small data sets have showed them to yield very good results [11]. We thus independently associate a score with each term, based on each of the two statistics. The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document. This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10]. The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query. The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request. This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise. Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with. Ties are resolved using the corresponding TF scores. Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web. For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF. However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic. Lexical Compounds. Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set. Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1]. We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository. Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time. Sentence Selection. This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output. Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences). Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line. We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein. A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details). The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items. This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning. However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant. The final term biases the summary towards the query. It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query. It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms. In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus. Term Co-occurrence Statistics. For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query. Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1. Co-occurrence based keyword similarity search. Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores. The off-line computation needs an initial trimming phase (step 1) for optimization purposes. In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably. During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query. Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b). We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below. DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y. To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms. We set W to be the same as the maximum amount of expansion keywords desired. Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 . It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other. This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present. Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together. We compare the two binomial processes by using likelihood ratios of their associated hypotheses. First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ . Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k . Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion. Large scale thesauri encapsulate global knowledge about term relationships. Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request. In the end, those suggestions with the highest frequencies are kept. The algorithm is as follows: Algorithm 3.1.2.2. Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values. We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts). As they represent quite different types of association, we investigated them separately. We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs). We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education). First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache. Without loss of generality, we focused the experiments on single-user machines. Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001. In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop. To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings. The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones. Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations. The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances. Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on. To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest. The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones. For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1. These results were then shuffled into one set containing usually between 70 and 90 URLs. Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL. Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment. For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant. Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15]. DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise. We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones. As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries. All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions. Algorithmic specific aspects. The main parameter of our algorithms is the number of generated expansion keywords. For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation. In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four). For all algorithms we also investigated bigger limitations. This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected. We therefore chose to experiment with this new approach as well. For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain. We labeled the algorithms we evaluated as follows: 0. Google: The actual Google query output, as returned by the Google API; 1. TF, DF: Term and Document Frequency; 2. LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3. SS: Sentence Selection; 4. TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5. WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively. Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information. This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34]. However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries. We evaluated all variants of our algorithms using NDCG. For log queries, the best performance was achieved with TF, LC[O], and TC[LR]. The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O]. A summary of all results is depicted in Table 1. Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task. LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04. Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item. The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR]. Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects. We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering). As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific. NDCG Signific. Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries. Algorithm NDCG Signific. NDCG Signific. Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions. Finally, we noticed Google to be very optimized for some top frequent queries. However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]). Self-selected Queries. The NDCG values obtained with selfselected queries are depicted in Table 2. While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries. In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type. In general, the relative differences between our algorithms were similar to those observed for the log based queries. As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best. Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics. There were no visible differences between the behavior of the three different approaches to cooccurrence calculation. Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements. We thus pursued this idea with the adaptive algorithms presented in the next section. 4. INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query. However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it. In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process. Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms. We start by discussing adaptation by analyzing the query clarity level. Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task. Query Clarity. The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic. Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario. Also, the success of IR systems clearly varies across different topics. We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm. The following metrics are available: • The Query Length is expressed simply by the number of words in the user query. The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far. It measures the divergence between the language model associated to the user query and the language model associated to the collection. In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents. Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications. We thus decided to investigate only C1 and C2. First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web. As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis. As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case. Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good. Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion. Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR. Note that the ambiguity level is related to the number of documents covering a certain query. Thus, to some extent, it has different meanings on the Web and within PIRs. While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop. Take for example the query PageRank. If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous. However, when analyzed against the Web, this is definitely a clear query. Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop. Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them. The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3. Query Formulation Process. Interactive query expansion has a high potential for enhancing search [29]. We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms. For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request. Thus, the newly added terms are more likely to convey information about her search goals. For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords. Within our personalized scenario, the generated expansions can similarly be biased towards these terms. Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach. Other Features. The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature. Only some approaches have been investigated, usually indirectly. There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc. However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm. The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF. The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4). For top frequent queries, Algorithm NDCG Signific. NDCG Signific. Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries. Algorithm NDCG Signific. NDCG Signific. Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01. They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07). For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms. The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity. Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level. The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5). For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01. Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05). Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively. All results are depicted graphically in Figure 1. We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected. The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5. CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output. In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents. Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri. Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios. We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach. We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms. We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data. Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6. ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented. We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log. Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7. REFERENCES [1] J. Allan and H. Raghavan. Using part-of-speech patterns to reduce query ambiguity. In Proc. of the 25th Intl. ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni. The paraphrase search assistant: Terminological feedback for iterative information seeking. In Proc. of the 22nd Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer. Automatic query wefinement using lexical affinities with maximal information gain. In Proc. of the 25th Intl. ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi. An information-theoretic approach to automatic query expansion. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu. Integrating query expansion and conceptual relevance feedback for personalized web information retrieval. In Proc. of the 7th Intl. Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl. Summarizing local context to personalize global web search. In Proc. of the 15th Intl. CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In Proc. of the 25th Intl. ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. Ma. Probabilistic query expansion using query logs. In Proc. of the 11th Intl. Conf. on World Wide Web, 2002. [9] T. Dunning. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson. New methods in automatic extracting. Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion. Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz. Scaling link based similarity search. In Proc. of the 14th Intl. World Wide Web Conf., 2005. [13] T. Haveliwala. Topic-sensitive pagerank. In Proc. of the 11th Intl. World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B. He and I. Ounis. Inferring query performance using pre-retrieval predictors. In Proc. of the 11th Intl. SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen. Ir evaluation methods for retrieving highly relevant documents. In Proc. of the 23th Intl. ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom. Scaling personalized web search. In Proc. of the 12th Intl. World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi. A comparison of collocation-based similarity measures in query expansion. Inf. Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo, and H.-C. Rim. Information retrieval using word senses: root sense tagging approach. In Proc. of the 27th Intl. ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien. Mining anchor text for query refinement. In Proc. of the 13th Intl. Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft. Lexical ambiguity and information retrieval. ACM Trans. Inf. Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones. Applying summarization techniques for term selection in relevance feedback. In Proc. of the 24th Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng. An effective approach to document retrieval via utilizing wordnet and recognizing phrases. In Proc. of the 27th Intl. ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller. Wordnet: An electronic lexical database. Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi. Topical link analysis for web search. In Proc. of the 29th Intl. ACM SIGIR Conf. on Res. and Development in Inf. Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd. The PageRank citation ranking: Bringing order to the web. Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho. Automatic indentification of user interest for personalized search. In Proc. of the 15th Intl. WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei. Concept based query expansion. In Proc. of the 16th Intl. ACM SIGIR Conf. on Research and Development in Inf. Retr., 1993. [28] J. Rocchio. Relevance feedback in information retrieval. The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven. Re-examining the potential effectiveness of interactive query expansion. In Proc. of the 26th Intl. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras, and B. Racz. To randomize or not to randomize: Space optimal summaries for hyperlink analysis. In Proc. of the 15th Intl. WWW Conf., 2006. [31] C. Shah and W. B. Croft. Evaluating high accuracy retrieval techniques. In Proc. of the 27th Intl. ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive web search based on user profile constructed without any effort from users. In Proc. of the 13th Intl. World Wide Web Conf., 2004. [33] D. Sullivan. The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz. Personalizing search via automated analysis of interests and activities. In Proc. of the 28th Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh. Personalization and privacy. Commun. ACM, 43(8), 2000. [36] E. M. Voorhees. Query expansion using lexical-semantic relations. In Proc. of the 17th Intl. ACM SIGIR Conf. on Res. and development in Inf. Retr., 1994. [37] J. Xu and W. B. Croft. Query expansion using local and global document analysis. In Proc. of the 19th Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y. Ma. Improving pseudo-relevance feedback in web information retrieval using web page segmentation. In Proc. of the 12th Intl. Conf. on World Wide Web, 2003.",
    "original_translation": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003.",
    "original_sentences": [
        "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
        "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
        "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
        "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
        "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
        "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
        "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
        "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
        "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
        "Yet keyword queries are inherently ambiguous.",
        "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
        "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
        "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
        "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
        "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
        "This is exactly the Web search scenario!",
        "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
        "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
        "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
        "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
        "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
        "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
        "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
        "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
        "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
        "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
        "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
        "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
        "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
        "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
        "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
        "There exists a vast amount of algorithms for both domains.",
        "However, not much has been done specifically aimed at combining them.",
        "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
        "This section splits the relevant background according to the focus of each article into either one of these elements.",
        "Approaches focused on the User Profile.",
        "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
        "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
        "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
        "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
        "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
        "MyWeb2 ).",
        "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
        "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
        "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
        "Moreover, none of these investigated the adaptive application of personalization.",
        "Approaches focused on the Personalization Algorithm.",
        "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
        "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
        "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
        "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
        "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
        "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
        "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
        "Some other approaches are also addressed in the end of the section.",
        "Relevance Feedback Techniques.",
        "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
        "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
        "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
        "We used some of these as inspiration for our Desktop specific techniques.",
        "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
        "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
        "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
        "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
        "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
        "Co-occurrence Based Techniques.",
        "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
        "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
        "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
        "Thesaurus Based Techniques.",
        "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
        "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
        "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
        "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
        "We also use WordNet based expansion terms.",
        "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
        "Other Techniques.",
        "There are many other attempts to extract expansion terms.",
        "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
        "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
        "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
        "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
        "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
        "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
        "In the proposed algorithms we gradually increase the amount of personal information utilized.",
        "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
        "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
        "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
        "We distinguish three granularity levels for this process and we investigate each of them separately.",
        "Term and Document Frequency.",
        "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
        "Previous experiments with small data sets have showed them to yield very good results [11].",
        "We thus independently associate a score with each term, based on each of the two statistics.",
        "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
        "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
        "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
        "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
        "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
        "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
        "Ties are resolved using the corresponding TF scores.",
        "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
        "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
        "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
        "Lexical Compounds.",
        "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
        "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
        "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
        "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
        "Sentence Selection.",
        "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
        "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
        "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
        "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
        "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
        "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
        "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
        "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
        "The final term biases the summary towards the query.",
        "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
        "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
        "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
        "Term Co-occurrence Statistics.",
        "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
        "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
        "Co-occurrence based keyword similarity search.",
        "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
        "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
        "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
        "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
        "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
        "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
        "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
        "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
        "We set W to be the same as the maximum amount of expansion keywords desired.",
        "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
        "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
        "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
        "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
        "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
        "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
        "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
        "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
        "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
        "Large scale thesauri encapsulate global knowledge about term relationships.",
        "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
        "In the end, those suggestions with the highest frequencies are kept.",
        "The algorithm is as follows: Algorithm 3.1.2.2.",
        "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
        "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
        "As they represent quite different types of association, we investigated them separately.",
        "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
        "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
        "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
        "Without loss of generality, we focused the experiments on single-user machines.",
        "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
        "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
        "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
        "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
        "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
        "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
        "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
        "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
        "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
        "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
        "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
        "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
        "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
        "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
        "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
        "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
        "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
        "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
        "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
        "Algorithmic specific aspects.",
        "The main parameter of our algorithms is the number of generated expansion keywords.",
        "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
        "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
        "For all algorithms we also investigated bigger limitations.",
        "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
        "We therefore chose to experiment with this new approach as well.",
        "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
        "We labeled the algorithms we evaluated as follows: 0.",
        "Google: The actual Google query output, as returned by the Google API; 1.",
        "TF, DF: Term and Document Frequency; 2.",
        "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
        "SS: Sentence Selection; 4.",
        "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
        "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
        "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
        "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
        "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
        "We evaluated all variants of our algorithms using NDCG.",
        "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
        "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
        "A summary of all results is depicted in Table 1.",
        "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
        "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
        "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
        "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
        "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
        "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
        "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
        "NDCG Signific.",
        "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
        "Algorithm NDCG Signific.",
        "NDCG Signific.",
        "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
        "Finally, we noticed Google to be very optimized for some top frequent queries.",
        "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
        "Self-selected Queries.",
        "The NDCG values obtained with selfselected queries are depicted in Table 2.",
        "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
        "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
        "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
        "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
        "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
        "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
        "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
        "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
        "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
        "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
        "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
        "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
        "We start by discussing adaptation by analyzing the query clarity level.",
        "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
        "Query Clarity.",
        "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
        "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
        "Also, the success of IR systems clearly varies across different topics.",
        "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
        "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
        "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
        "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
        "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
        "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
        "We thus decided to investigate only C1 and C2.",
        "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
        "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
        "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
        "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
        "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
        "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
        "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
        "Note that the ambiguity level is related to the number of documents covering a certain query.",
        "Thus, to some extent, it has different meanings on the Web and within PIRs.",
        "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
        "Take for example the query PageRank.",
        "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
        "However, when analyzed against the Web, this is definitely a clear query.",
        "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
        "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
        "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
        "Query Formulation Process.",
        "Interactive query expansion has a high potential for enhancing search [29].",
        "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
        "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
        "Thus, the newly added terms are more likely to convey information about her search goals.",
        "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
        "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
        "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
        "Other Features.",
        "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
        "Only some approaches have been investigated, usually indirectly.",
        "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
        "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
        "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
        "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
        "For top frequent queries, Algorithm NDCG Signific.",
        "NDCG Signific.",
        "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
        "Algorithm NDCG Signific.",
        "NDCG Signific.",
        "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
        "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
        "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
        "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
        "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
        "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
        "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
        "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
        "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
        "All results are depicted graphically in Figure 1.",
        "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
        "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
        "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
        "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
        "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
        "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
        "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
        "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
        "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
        "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
        "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
        "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
        "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
        "REFERENCES [1] J. Allan and H. Raghavan.",
        "Using part-of-speech patterns to reduce query ambiguity.",
        "In Proc. of the 25th Intl.",
        "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
        "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
        "In Proc. of the 22nd Intl.",
        "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
        "Automatic query wefinement using lexical affinities with maximal information gain.",
        "In Proc. of the 25th Intl.",
        "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
        "An information-theoretic approach to automatic query expansion.",
        "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
        "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
        "In Proc. of the 7th Intl.",
        "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
        "Summarizing local context to personalize global web search.",
        "In Proc. of the 15th Intl.",
        "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
        "Predicting query performance.",
        "In Proc. of the 25th Intl.",
        "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
        "Nie, and W.-Y.",
        "Ma.",
        "Probabilistic query expansion using query logs.",
        "In Proc. of the 11th Intl.",
        "Conf. on World Wide Web, 2002. [9] T. Dunning.",
        "Accurate methods for the statistics of surprise and coincidence.",
        "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
        "New methods in automatic extracting.",
        "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
        "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
        "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
        "Scaling link based similarity search.",
        "In Proc. of the 14th Intl.",
        "World Wide Web Conf., 2005. [13] T. Haveliwala.",
        "Topic-sensitive pagerank.",
        "In Proc. of the 11th Intl.",
        "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
        "He and I. Ounis.",
        "Inferring query performance using pre-retrieval predictors.",
        "In Proc. of the 11th Intl.",
        "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
        "Ir evaluation methods for retrieving highly relevant documents.",
        "In Proc. of the 23th Intl.",
        "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
        "Scaling personalized web search.",
        "In Proc. of the 12th Intl.",
        "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
        "A comparison of collocation-based similarity measures in query expansion.",
        "Inf.",
        "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
        "Kim, H.-C. Seo, and H.-C. Rim.",
        "Information retrieval using word senses: root sense tagging approach.",
        "In Proc. of the 27th Intl.",
        "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
        "Mining anchor text for query refinement.",
        "In Proc. of the 13th Intl.",
        "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
        "Lexical ambiguity and information retrieval.",
        "ACM Trans.",
        "Inf.",
        "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
        "Applying summarization techniques for term selection in relevance feedback.",
        "In Proc. of the 24th Intl.",
        "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
        "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
        "In Proc. of the 27th Intl.",
        "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
        "Wordnet: An electronic lexical database.",
        "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
        "Topical link analysis for web search.",
        "In Proc. of the 29th Intl.",
        "ACM SIGIR Conf. on Res. and Development in Inf.",
        "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
        "The PageRank citation ranking: Bringing order to the web.",
        "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
        "Automatic indentification of user interest for personalized search.",
        "In Proc. of the 15th Intl.",
        "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
        "Concept based query expansion.",
        "In Proc. of the 16th Intl.",
        "ACM SIGIR Conf. on Research and Development in Inf.",
        "Retr., 1993. [28] J. Rocchio.",
        "Relevance feedback in information retrieval.",
        "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
        "Re-examining the potential effectiveness of interactive query expansion.",
        "In Proc. of the 26th Intl.",
        "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
        "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
        "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
        "In Proc. of the 15th Intl.",
        "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
        "Evaluating high accuracy retrieval techniques.",
        "In Proc. of the 27th Intl.",
        "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
        "Adaptive web search based on user profile constructed without any effort from users.",
        "In Proc. of the 13th Intl.",
        "World Wide Web Conf., 2004. [33] D. Sullivan.",
        "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
        "Personalizing search via automated analysis of interests and activities.",
        "In Proc. of the 28th Intl.",
        "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
        "Personalization and privacy.",
        "Commun.",
        "ACM, 43(8), 2000. [36] E. M. Voorhees.",
        "Query expansion using lexical-semantic relations.",
        "In Proc. of the 17th Intl.",
        "ACM SIGIR Conf. on Res. and development in Inf.",
        "Retr., 1994. [37] J. Xu and W. B. Croft.",
        "Query expansion using local and global document analysis.",
        "In Proc. of the 19th Intl.",
        "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
        "Ma.",
        "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
        "In Proc. of the 12th Intl.",
        "Conf. on World Wide Web, 2003."
    ],
    "translated_text_sentences": [
        "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr.",
        "La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web.",
        "En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda.",
        "Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos.",
        "Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida.",
        "Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta.",
        "Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1.",
        "La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web.",
        "Sin embargo, las consultas de palabras clave son inherentemente ambiguas.",
        "El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música.",
        "Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría.",
        "Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente.",
        "La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada.",
        "Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]).",
        "¡Este es exactamente el escenario de búsqueda en la web!",
        "En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc.",
        "Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable).",
        "Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario.",
        "Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad.",
        "Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada).",
        "Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda.",
        "Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1.",
        "Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web.",
        "En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos.",
        "Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%.",
        "En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta.",
        "Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado.",
        "Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas.",
        "Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2.",
        "TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas.",
        "Existe una gran cantidad de algoritmos para ambos dominios.",
        "Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos.",
        "En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real.",
        "Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos.",
        "Enfoques centrados en el Perfil del Usuario.",
        "Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas.",
        "Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario.",
        "Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13].",
        "La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar.",
        "Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!).",
        "MiWeb2.",
        "Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios.",
        "Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad.",
        "Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios.",
        "Además, ninguno de ellos investigó la aplicación adaptativa de la personalización.",
        "Enfoques centrados en el Algoritmo de Personalización.",
        "Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente.",
        "Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas.",
        "Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas.",
        "Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal.",
        "Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación.",
        "Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado.",
        "En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro.",
        "Algunos enfoques adicionales también se abordan al final de la sección.",
        "Técnicas de retroalimentación de relevancia.",
        "La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial.",
        "Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados.",
        "Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc.",
        "Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio.",
        "Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria.",
        "RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF).",
        "Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario.",
        "Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección.",
        "Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas.",
        "Técnicas basadas en co-ocurrencia.",
        "Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17].",
        "Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc.",
        "También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal.",
        "Técnicas basadas en tesauros.",
        "Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada.",
        "Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc.",
        "Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36].",
        "Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22].",
        "También utilizamos términos de expansión basados en WordNet.",
        "Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas.",
        "Otras técnicas.",
        "Hay muchos otros intentos de extraer términos de expansión.",
        "Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda.",
        "Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3.",
        "AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado.",
        "Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma.",
        "En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal.",
        "Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web.",
        "En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada.",
        "Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios.",
        "Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos.",
        "En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados.",
        "Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado.",
        "Frecuencia de término y de documento.",
        "Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular.",
        "Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11].",
        "Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas.",
        "La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento.",
        "Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10].",
        "La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios.",
        "La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original.",
        "Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas.",
        "Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados.",
        "Las disputas se resuelven utilizando los puntajes de TF correspondientes.",
        "Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web.",
        "Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF.",
        "Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto.",
        "Compuestos léxicos.",
        "Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada.",
        "Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1].",
        "Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local.",
        "Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución.",
        "Selección de oraciones.",
        "Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado.",
        "La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones).",
        "Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión.",
        "Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella.",
        "Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles).",
        "El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio.",
        "De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio.",
        "Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes.",
        "El término final sesga el resumen hacia la consulta.",
        "Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda.",
        "Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes.",
        "En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo.",
        "Estadísticas de co-ocurrencia de términos.",
        "Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario.",
        "Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1.",
        "Búsqueda de similitud de palabras clave basada en la co-ocurrencia.",
        "Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas.",
        "La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización.",
        "Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia.",
        "Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta.",
        "Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b).",
        "Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes.",
        "DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y.",
        "Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos.",
        "Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas.",
        "El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2.",
        "Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí.",
        "Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente.",
        "Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos.",
        "Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas.",
        "Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡.",
        "Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k.",
        "Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
        "Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro.",
        "Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos.",
        "Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial.",
        "Al final, se conservan aquellas sugerencias con las frecuencias más altas.",
        "El algoritmo es el siguiente: Algoritmo 3.1.2.2.",
        "Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H.",
        "Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes).",
        "Dado que representan tipos de asociación bastante diferentes, los investigamos por separado.",
        "Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs).",
        "Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación).",
        "Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web.",
        "Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario.",
        "Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001.",
        "Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas.",
        "Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados.",
        "Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios.",
        "Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones.",
        "Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores.",
        "Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar.",
        "Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés.",
        "Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario.",
        "Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1.",
        "Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL.",
        "Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada.",
        "En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento.",
        "Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante.",
        "Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15].",
        "DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso.",
        "Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes.",
        "Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas.",
        "Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación.",
        "Aspectos específicos del algoritmo.",
        "El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas.",
        "Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior.",
        "Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro).",
        "Para todos los algoritmos también investigamos limitaciones más grandes.",
        "Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento.",
        "Por lo tanto, decidimos experimentar con este nuevo enfoque también.",
        "Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional.",
        "Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0.",
        "Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1.",
        "TF, DF: Frecuencia del término y del documento; 2.",
        "LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3.",
        "SS: Selección de oraciones; 4.",
        "TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5.",
        "WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente.",
        "Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios.",
        "Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34].",
        "Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro.",
        "Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG.",
        "Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR].",
        "Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O].",
        "Un resumen de todos los resultados se muestra en la Tabla 1.",
        "Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio.",
        "LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04.",
        "Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal.",
        "Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR].",
        "Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos.",
        "Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos).",
        "Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific.",
        "NDCG significa \"Normalized Discounted Cumulative Gain\".",
        "Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro.",
        "Algoritmo NDCG Signific.",
        "NDCG significa \"Normalized Discounted Cumulative Gain\".",
        "Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales.",
        "Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales.",
        "Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]).",
        "Consultas auto-seleccionadas.",
        "Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2.",
        "Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas.",
        "De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta.",
        "En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros.",
        "Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento.",
        "Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos.",
        "No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia.",
        "Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras.",
        "Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4.",
        "INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario.",
        "Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza.",
        "En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad.",
        "Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión.",
        "Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta.",
        "Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea.",
        "Claridad de la consulta.",
        "El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema.",
        "Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo.",
        "Además, el éxito de los sistemas de IR claramente varía según los diferentes temas.",
        "Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo.",
        "Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario.",
        "La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora.",
        "Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección.",
        "En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos.",
        "Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web.",
        "Por lo tanto, decidimos investigar solo C1 y C2.",
        "Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
        "Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web.",
        "Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior.",
        "Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular.",
        "Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos.",
        "Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa.",
        "Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios.",
        "Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta.",
        "Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs.",
        "Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio.",
        "Tomemos como ejemplo la consulta PageRank.",
        "Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua.",
        "Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara.",
        "En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio.",
        "Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas.",
        "El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3.",
        "Proceso de formulación de consultas.",
        "La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29].",
        "Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos.",
        "Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original.",
        "Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda.",
        "Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave.",
        "Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos.",
        "Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque.",
        "Otras características.",
        "La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura.",
        "Solo se han investigado algunos enfoques, generalmente de forma indirecta.",
        "Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc.",
        "Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo.",
        "Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN].",
        "Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4).",
        "Para las consultas más frecuentes, Algoritmo NDCG Signific.",
        "NDCG significa \"Normalized Discounted Cumulative Gain\".",
        "Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google.",
        "Algoritmo NDCG Signific.",
        "NDCG significa \"Normalized Discounted Cumulative Gain\".",
        "Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01.",
        "También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07).",
        "Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos.",
        "La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta.",
        "Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta.",
        "El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5).",
        "Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01.",
        "La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05).",
        "Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente.",
        "Todos los resultados se representan gráficamente en la Figura 1.",
        "Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario.",
        "Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5.",
        "CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda.",
        "En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales.",
        "Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos.",
        "Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes.",
        "Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente.",
        "Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión.",
        "También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora.",
        "Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos.",
        "AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos.",
        "Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista.",
        "Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7.",
        "REFERENCIAS [1] J. Allan y H. Raghavan.",
        "Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta.",
        "En Actas de la 25ª Conferencia Internacional.",
        "Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni.",
        "El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información.",
        "En Actas del 22º Congreso Internacional.",
        "Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer.",
        "Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima.",
        "En Actas de la 25ª Conferencia Internacional.",
        "ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi.",
        "Un enfoque de teoría de la información para la expansión automática de consultas.",
        "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu.",
        "Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web.",
        "En Actas de la 7ª Conferencia Internacional.",
        "Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl.",
        "Resumiendo el contexto local para personalizar la búsqueda web global.",
        "En Actas de la 15ª Conferencia Internacional.",
        "CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft.",
        "Prediciendo el rendimiento de la consulta.",
        "En Actas de la 25ª Conferencia Internacional.",
        "Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
        "Nie, and W.-Y. -> Nie, y W.-Y.",
        "This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish.",
        "Expansión de consulta probabilística utilizando registros de consulta.",
        "En Actas de la 11ª Conferencia Internacional.",
        "Conf. en la World Wide Web, 2002. [9] T. Dunning.",
        "Métodos precisos para la estadística de sorpresa y coincidencia.",
        "Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson.",
        "Nuevos métodos en extracción automática.",
        "Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
        "Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas.",
        "Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz.",
        "Búsqueda de similitud basada en enlaces escalables.",
        "En Actas de la 14ª Conferencia Internacional.",
        "Conferencia de la World Wide Web, 2005. [13] T. Haveliwala.",
        "PageRank sensible al tema.",
        "En Actas de la 11ª Conferencia Internacional.",
        "Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B.",
        "Él y yo. Ounis.",
        "Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación.",
        "En Actas de la 11ª Conferencia Internacional.",
        "Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen.",
        "Métodos de evaluación para recuperar documentos altamente relevantes.",
        "En Actas de la 23ª Conferencia Internacional.",
        "Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom.",
        "Escalando la búsqueda web personalizada.",
        "En Actas de la 12ª Conferencia Internacional.",
        "Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi.",
        "Una comparación de medidas de similitud basadas en colocación en la expansión de consultas.",
        "I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation.",
        "Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B.",
        "Kim, H.-C. Seo y H.-C. Rim.",
        "Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz.",
        "En Actas de la 27ª Conferencia Internacional.",
        "Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien.",
        "Extracción de texto ancla para el refinamiento de consultas.",
        "En Actas de la 13ª Conferencia Internacional.",
        "Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft.",
        "Ambigüedad léxica y recuperación de información.",
        "ACM Trans.",
        "I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish?",
        "Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones.",
        "Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia.",
        "En Actas del 24º Congreso Internacional.",
        "Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng.",
        "Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases.",
        "En Actas de la 27ª Conferencia Internacional.",
        "Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller.",
        "Wordnet: Una base de datos léxica electrónica.",
        "Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi.",
        "Análisis de enlaces temáticos para búsqueda web.",
        "En Actas de la 29ª Conferencia Internacional.",
        "Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información.",
        "Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd.",
        "El ranking de citas PageRank: Trayendo orden al web.",
        "Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho.",
        "Identificación automática de intereses del usuario para búsqueda personalizada.",
        "En Actas de la 15ª Conferencia Internacional.",
        "WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei.",
        "Expansión de consulta basada en conceptos.",
        "En Actas del 16º Congreso Internacional.",
        "Conf. ACM SIGIR sobre Investigación y Desarrollo en Información.",
        "Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio.",
        "Retroalimentación de relevancia en la recuperación de información.",
        "El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven.",
        "Reexaminando la efectividad potencial de la expansión interactiva de consultas.",
        "En Actas de la 26ª Conferencia Internacional.",
        "ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A.",
        "A. Benczur, K. Csalogany, D. Fogaras y B. Racz.",
        "Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos.",
        "En Actas de la 15ª Conferencia Internacional.",
        "WWW Conf., 2006. [31] C. Shah y W. B. Croft.",
        "Evaluando técnicas de recuperación de alta precisión.",
        "En Actas de la 27ª Conferencia Internacional.",
        "ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa.",
        "Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios.",
        "En Actas de la 13ª Conferencia Internacional.",
        "Conferencia de la World Wide Web, 2004. [33] D. Sullivan.",
        "Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz.",
        "Personalización de la búsqueda a través del análisis automatizado de intereses y actividades.",
        "En Actas de la 28ª Conferencia Internacional.",
        "Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh.",
        "Personalización y privacidad.",
        "This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish?",
        "ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees.",
        "Expansión de consulta utilizando relaciones léxico-semánticas.",
        "En Actas de la 17ª Conferencia Internacional.",
        "Conf. ACM SIGIR sobre Investigación y Desarrollo en Información.",
        "Retr., 1994. [37] J. Xu y W. B. Croft.",
        "Expansión de consulta utilizando análisis local y global de documentos.",
        "En Actas de la 19ª Conferencia Internacional.",
        "Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y.",
        "I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish?",
        "Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web.",
        "En Actas de la 12ª Conferencia Internacional.",
        "Conferencia sobre la World Wide Web, 2003."
    ],
    "error_count": 11,
    "keys": {
        "short keyword queries": {
            "translated_key": "consultas de palabras clave cortas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of <br>short keyword queries</br> demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of <br>short keyword queries</br> demands for enhanced methods for Web retrieval."
            ],
            "translated_annotated_samples": [
                "La ambigüedad inherente de las <br>consultas de palabras clave cortas</br> exige métodos mejorados para la recuperación web."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las <br>consultas de palabras clave cortas</br> exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "web retrieval": {
            "translated_key": "recuperación web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for <br>web retrieval</br>.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for <br>web retrieval</br>."
            ],
            "translated_annotated_samples": [
                "La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la <br>recuperación web</br>."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la <br>recuperación web</br>. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "web query": {
            "translated_key": "consulta web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance <br>web query</br> reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the <br>web query</br> best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the <br>web query</br> the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users <br>web query</br>, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users <br>web query</br>.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "In this paper we propose to enhance <br>web query</br> reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the <br>web query</br> best.",
                "We append to the <br>web query</br> the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users <br>web query</br>, rather than from the top ranked Web search results.",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users <br>web query</br>."
            ],
            "translated_annotated_samples": [
                "En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc.",
                "Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la <br>consulta web</br>.",
                "Añadimos a la <br>consulta web</br> los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos.",
                "En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la <br>consulta web</br> de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados.",
                "La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la <br>consulta web</br> de los usuarios."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la <br>consulta web</br>. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la <br>consulta web</br> los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la <br>consulta web</br> de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la <br>consulta web</br> de los usuarios. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "personal information repository": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users <br>personal information repository</br>, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users <br>personal information repository</br> (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex <br>personal information repository</br>.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the <br>personal information repository</br>.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users <br>personal information repository</br> in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users <br>personal information repository</br>, thus implicitly personalizing the search output.",
                "In this paper we propose to enhance Web query reformulation by exploiting the users <br>personal information repository</br> (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex <br>personal information repository</br>.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the <br>personal information repository</br>.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users <br>personal information repository</br> in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output."
            ],
            "translated_annotated_samples": [
                "En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los <br>repositorios de información personal</br> de cada usuario, personalizando así implícitamente los resultados de la búsqueda.",
                "En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del <br>Repositorio de Información Personal</br> (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc.",
                "También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal.",
                "En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el <br>repositorio de información personal</br>.",
                "CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del <br>Repositorio de Información Personal</br> de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los <br>repositorios de información personal</br> de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del <br>Repositorio de Información Personal</br> (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el <br>repositorio de información personal</br>. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del <br>Repositorio de Información Personal</br> de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. ",
            "candidates": [],
            "error": [
                [
                    "repositorios de información personal",
                    "Repositorio de Información Personal",
                    "repositorio de información personal",
                    "Repositorio de Información Personal"
                ]
            ]
        },
        "search output": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the <br>search output</br>.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer <br>search output</br> to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web <br>search output</br> accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the <br>search output</br>.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web <br>search output</br> according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the <br>search output</br>.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the <br>search output</br>.",
                "Clearly, one would prefer <br>search output</br> to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web <br>search output</br> accordingly.",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the <br>search output</br>.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web <br>search output</br> according to the cosine distance between each URL and a set of Desktop terms describing users interests."
            ],
            "translated_annotated_samples": [
                "En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los <br>resultados de la búsqueda</br>.",
                "Claramente, uno preferiría que los <br>resultados de búsqueda</br> estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría.",
                "La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la <br>salida de la búsqueda</br> web de manera adecuada.",
                "Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los <br>resultados de la búsqueda</br>.",
                "Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la <br>salida de la búsqueda</br> web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los <br>resultados de la búsqueda</br>. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los <br>resultados de búsqueda</br> estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la <br>salida de la búsqueda</br> web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los <br>resultados de la búsqueda</br>. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la <br>salida de la búsqueda</br> web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. ",
            "candidates": [],
            "error": [
                [
                    "resultados de la búsqueda",
                    "resultados de búsqueda",
                    "salida de la búsqueda",
                    "resultados de la búsqueda",
                    "salida de la búsqueda"
                ]
            ]
        },
        "additional query keyword": {
            "translated_key": "palabras clave de consulta adicionales",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the <br>additional query keyword</br>s by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces <br>additional query keyword</br>s by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "We introduce five broad techniques for generating the <br>additional query keyword</br>s by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Each of them produces <br>additional query keyword</br>s by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri."
            ],
            "translated_annotated_samples": [
                "Introducimos cinco técnicas amplias para generar <br>palabras clave de consulta adicionales</br> mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos.",
                "Cada uno de ellos produce <br>palabras clave de consulta adicionales</br> mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar <br>palabras clave de consulta adicionales</br> mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce <br>palabras clave de consulta adicionales</br> mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "granularity level": {
            "translated_key": "niveles de granularidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing <br>granularity level</br>s, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three <br>granularity level</br>s for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing <br>granularity level</br>s, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing <br>granularity level</br>s, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "We distinguish three <br>granularity level</br>s for this process and we investigate each of them separately.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing <br>granularity level</br>s, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri."
            ],
            "translated_annotated_samples": [
                "Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos.",
                "Distinguimos tres <br>niveles de granularidad</br> para este proceso e investigamos cada uno de ellos por separado.",
                "Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres <br>niveles de granularidad</br> para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "term and compound level analysis": {
            "translated_key": "nivel de términos y compuestos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from <br>term and compound level analysis</br> up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from <br>term and compound level analysis</br> up to global co-occurrence statistics, as well as to using external thesauri."
            ],
            "translated_annotated_samples": [
                "Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a <br>nivel de términos y compuestos</br> hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a <br>nivel de términos y compuestos</br> hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "global co-occurrence statistics": {
            "translated_key": "estadísticas de co-ocurrencia global",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to <br>global co-occurrence statistics</br>, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing <br>global co-occurrence statistics</br> over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to <br>global co-occurrence statistics</br> and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to <br>global co-occurrence statistics</br>, as well as to using external thesauri.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing <br>global co-occurrence statistics</br> over the personal information repository.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to <br>global co-occurrence statistics</br> and external thesauri."
            ],
            "translated_annotated_samples": [
                "Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta <br>estadísticas de co-ocurrencia global</br>, así como el uso de tesauros externos.",
                "En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de <br>estadísticas de co-ocurrencia global</br> sobre el repositorio de información personal.",
                "Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta <br>estadísticas de co-ocurrencia global</br> y tesauros externos."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta <br>estadísticas de co-ocurrencia global</br>, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de <br>estadísticas de co-ocurrencia global</br> sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta <br>estadísticas de co-ocurrencia global</br> y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "external thesaurus": {
            "translated_key": "tesauro externo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an <br>external thesaurus</br>.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an <br>external thesaurus</br>."
            ],
            "translated_annotated_samples": [
                "En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un <br>tesauro externo</br>."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un <br>tesauro externo</br>. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "extensive empirical analysis": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our <br>extensive empirical analysis</br> under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "Our <br>extensive empirical analysis</br> under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings."
            ],
            "translated_annotated_samples": [
                "Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "ambiguous query": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected <br>ambiguous query</br>, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using <br>ambiguous query</br> keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-<br>ambiguous query</br>: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / <br>ambiguous query</br>: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected <br>ambiguous query</br>, which they thought to have at least three meanings.",
                "Even though our algorithms are mainly intended to enhance search when using <br>ambiguous query</br> keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-<br>ambiguous query</br>: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / <br>ambiguous query</br>: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]."
            ],
            "translated_annotated_samples": [
                "Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una <br>consulta ambigua</br> seleccionada por ellos mismos, que pensaban que tenía al menos tres significados.",
                "Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar <br>palabras clave ambiguas</br> en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones.",
                "Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una <br>consulta ambigua</br> seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar <br>palabras clave ambiguas</br> en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                [
                    "consulta ambigua",
                    "palabras clave ambiguas"
                ]
            ]
        },
        "quality": {
            "translated_key": "calidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the <br>quality</br> of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the <br>quality</br> of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output <br>quality</br> [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the <br>quality</br> of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the <br>quality</br> of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its <br>quality</br> with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the <br>quality</br> of the output rankings.",
                "First is of course the <br>quality</br> of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output <br>quality</br> [36].",
                "To further increase the <br>quality</br> of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "Finally, the <br>quality</br> of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15]."
            ],
            "translated_annotated_samples": [
                "Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la <br>calidad</br> de las clasificaciones de salida.",
                "Primero está, por supuesto, la <br>calidad</br> de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario.",
                "Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la <br>calidad</br> de la producción [36].",
                "Para aumentar aún más la <br>calidad</br> de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos.",
                "Finalmente, la <br>calidad</br> de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la <br>calidad</br> de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la <br>calidad</br> de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la <br>calidad</br> de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la <br>calidad</br> de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la <br>calidad</br> de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "output ranking": {
            "translated_key": "clasificaciones de salida",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the <br>output ranking</br>s.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the <br>output ranking</br>s."
            ],
            "translated_annotated_samples": [
                "Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las <br>clasificaciones de salida</br>."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las <br>clasificaciones de salida</br>. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "personalized search framework": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this <br>personalized search framework</br> one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this <br>personalized search framework</br> further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "Subsequently, we move this <br>personalized search framework</br> one step further and propose to make the expansion process adaptive to various features of each query.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this <br>personalized search framework</br> further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach."
            ],
            "translated_annotated_samples": [
                "Posteriormente, llevamos este <br>marco de búsqueda personalizado</br> un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta.",
                "Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este <br>marco de búsqueda personalizada</br> un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este <br>marco de búsqueda personalizado</br> un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este <br>marco de búsqueda personalizada</br> un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                [
                    "marco de búsqueda personalizado",
                    "marco de búsqueda personalizada"
                ]
            ]
        },
        "expansion process": {
            "translated_key": "proceso de expansión",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the <br>expansion process</br> adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the <br>expansion process</br> adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the <br>expansion process</br>. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the <br>expansion process</br> adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "Subsequently, we move this personalized search framework one step further and propose to make the <br>expansion process</br> adaptive to various features of each query.",
                "In Section 4 we move this algorithmic framework further and propose to make the <br>expansion process</br> adaptive to the clarity level of the query.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the <br>expansion process</br>. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the <br>expansion process</br> adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach."
            ],
            "translated_annotated_samples": [
                "Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el <br>proceso de expansión</br> sea adaptable a varias características de cada consulta.",
                "En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el <br>proceso de expansión</br> sea adaptable al nivel de claridad de la consulta.",
                "En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el <br>proceso de expansión</br>. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados.",
                "Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el <br>proceso de expansión</br> sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el <br>proceso de expansión</br> sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el <br>proceso de expansión</br> sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el <br>proceso de expansión</br>. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el <br>proceso de expansión</br> sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "various features of each query": {
            "translated_key": "varias características de cada consulta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to <br>various features of each query</br>.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to <br>various features of each query</br>."
            ],
            "translated_annotated_samples": [
                "Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a <br>varias características de cada consulta</br>."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a <br>varias características de cada consulta</br>. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "each query various feature": {
            "translated_key": "cada consulta varias características",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "adaptive algorithm": {
            "translated_key": "algoritmos adaptativos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the <br>adaptive algorithm</br>s to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the <br>adaptive algorithm</br>s presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both <br>adaptive algorithm</br>s, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our <br>adaptive algorithm</br>s to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "A separate set of experiments indicates the <br>adaptive algorithm</br>s to bring an additional statistically significant improvement over the best static expansion approach.",
                "We thus pursued this idea with the <br>adaptive algorithm</br>s presented in the next section. 4.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both <br>adaptive algorithm</br>s, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our <br>adaptive algorithm</br>s to provide an additional improvement of 8.47% over the previously identified best approach."
            ],
            "translated_annotated_samples": [
                "Un conjunto separado de experimentos indica que los <br>algoritmos adaptativos</br> logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión.",
                "Así que seguimos esta idea con los <br>algoritmos adaptativos</br> presentados en la siguiente sección. 4.",
                "Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos <br>algoritmos adaptativos</br>, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01.",
                "Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros <br>algoritmos adaptativos</br> proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los <br>algoritmos adaptativos</br> logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los <br>algoritmos adaptativos</br> presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos <br>algoritmos adaptativos</br>, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros <br>algoritmos adaptativos</br> proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "significant improvement": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically <br>significant improvement</br> over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically <br>significant improvement</br> over the best static expansion approach."
            ],
            "translated_annotated_samples": [
                "Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "static expansion approach": {
            "translated_key": "enfoque estático de expansión",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best <br>static expansion approach</br>.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best <br>static expansion approach</br>."
            ],
            "translated_annotated_samples": [
                "Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor <br>enfoque estático de expansión</br>."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor <br>enfoque estático de expansión</br>. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de extracción de palabras clave basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "personalize web search": {
            "translated_key": "personalizar la búsqueda web",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "query expansion": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized <br>query expansion</br> for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "<br>query expansion</br> assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic <br>query expansion</br>.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several <br>query expansion</br> techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic <br>query expansion</br> Automatic <br>query expansion</br> aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative <br>query expansion</br> works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "<br>query expansion</br> USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate <br>query expansion</br> keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based <br>query expansion</br> algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based <br>query expansion</br>. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based <br>query expansion</br> task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized <br>query expansion</br> algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized <br>query expansion</br>.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive <br>query expansion</br> has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized <br>query expansion</br> algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify <br>query expansion</br> suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic <br>query expansion</br>.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating <br>query expansion</br> and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic <br>query expansion</br> using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive <br>query expansion</br>.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in <br>query expansion</br>.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based <br>query expansion</br>.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive <br>query expansion</br>.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "<br>query expansion</br> using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "<br>query expansion</br> using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "Personalized <br>query expansion</br> for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "<br>query expansion</br> assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic <br>query expansion</br>.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several <br>query expansion</br> techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic <br>query expansion</br> Automatic <br>query expansion</br> aims at deriving a better formulation of the user query in order to enhance retrieval."
            ],
            "translated_annotated_samples": [
                "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr.",
                "La <br>expansión de la consulta</br> ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada.",
                "TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas.",
                "En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de <br>expansión de consultas</br> y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real.",
                "Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La <br>expansión automática de consultas</br> tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La <br>expansión de la consulta</br> ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de <br>expansión de consultas</br> y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La <br>expansión automática de consultas</br> tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. ",
            "candidates": [],
            "error": [
                [
                    "expansión de la consulta",
                    "expansión de consultas",
                    "expansión automática de consultas"
                ]
            ]
        },
        "desktop profile": {
            "translated_key": "perfil de escritorio",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "keyword extraction": {
            "translated_key": "extracción de palabras clave",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based <br>keyword extraction</br> formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [
                "The complete TF based <br>keyword extraction</br> formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query."
            ],
            "translated_annotated_samples": [
                "La fórmula completa de <br>extracción de palabras clave</br> basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios."
            ],
            "translated_text": "Expansión de consultas personalizada para la web de Paul - Alexandru Chirita Centro de Investigación L3S∗ Appelstr. La ambigüedad inherente de las consultas de palabras clave cortas exige métodos mejorados para la recuperación web. En este artículo proponemos mejorar dichas consultas web expandiéndolas con términos recopilados de los repositorios de información personal de cada usuario, personalizando así implícitamente los resultados de la búsqueda. Introducimos cinco técnicas amplias para generar palabras clave de consulta adicionales mediante el análisis de datos de usuario en niveles de granularidad creciente, que van desde el análisis a nivel de términos y compuestos hasta estadísticas de co-ocurrencia global, así como el uso de tesauros externos. Nuestro extenso análisis empírico bajo cuatro escenarios diferentes muestra que algunos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo un aumento muy fuerte en la calidad de las clasificaciones de salida. Posteriormente, llevamos este marco de búsqueda personalizado un paso más allá y proponemos hacer que el proceso de expansión sea adaptable a varias características de cada consulta. Un conjunto separado de experimentos indica que los algoritmos adaptativos logran una mejora adicional estadísticamente significativa sobre el mejor enfoque estático de expansión. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; H.3.5 [Almacenamiento y Recuperación de Información]: Servicios de Información en Línea - Servicios basados en la web Términos Generales Algoritmos, Experimentación, Medición 1. La creciente popularidad de los motores de búsqueda ha determinado que la simple búsqueda de palabras clave se convierta en la única interfaz de usuario ampliamente aceptada para buscar información en la Web. Sin embargo, las consultas de palabras clave son inherentemente ambiguas. El libro de consulta Canon, por ejemplo, abarca varias áreas de interés: religión, fotografía, literatura y música. Claramente, uno preferiría que los resultados de búsqueda estén alineados con el tema o temas de interés de los usuarios, en lugar de mostrar una selección de URL populares de cada categoría. Estudios han demostrado que más del 80% de los usuarios preferirían recibir resultados de búsqueda personalizados [33] en lugar de los genéricos que se ofrecen actualmente. La expansión de la consulta ayuda al usuario a formular una mejor consulta, al agregar palabras clave adicionales a la solicitud de búsqueda inicial para abarcar sus intereses, así como para enfocar la salida de la búsqueda web de manera adecuada. Se ha demostrado que funciona muy bien con conjuntos de datos grandes, especialmente con consultas de entrada cortas (ver, por ejemplo, [19, 3]). ¡Este es exactamente el escenario de búsqueda en la web! En este artículo proponemos mejorar la reformulación de consultas web mediante la explotación del Repositorio de Información Personal (PIR) de los usuarios, es decir, la colección personal de documentos de texto, correos electrónicos, páginas web en caché, etc. Varios beneficios surgen al trasladar la personalización de la búsqueda web al nivel del Escritorio (tenga en cuenta que con Escritorio nos referimos a PIR, y utilizamos ambos términos de manera intercambiable). Primero está, por supuesto, la calidad de personalización: el Escritorio local es un rico repositorio de información, describiendo con precisión la mayoría, si no todos, los intereses del usuario. Segundo, dado que toda la información del perfil se almacena y se utiliza localmente, en la máquina personal, otro beneficio muy importante es la privacidad. Los motores de búsqueda no deberían poder conocer los intereses de una persona, es decir, no deberían poder relacionar a una persona específica con las consultas que emitió, o peor aún, con las URL de salida en las que hizo clic dentro de la interfaz de búsqueda (consultar a Volokh [35] para una discusión sobre problemas de privacidad relacionados con la búsqueda web personalizada). Nuestros algoritmos amplían las consultas web con palabras clave extraídas de los PIR de los usuarios, personalizando así de forma implícita los resultados de la búsqueda. Después de una discusión de trabajos previos en la Sección 2, primero investigamos el análisis del contexto de consulta local de escritorio en la Sección 3.1.1. Proponemos varias técnicas basadas en palabras clave, expresiones y resúmenes para determinar términos de expansión a partir de esos documentos personales que mejor coincidan con la consulta web. En la Sección 3.1.2 trasladamos nuestro análisis a la colección global de Escritorio e investigamos expansiones basadas en métricas de co-ocurrencia y tesauros externos. Los experimentos presentados en la Sección 3.2 muestran que muchos de estos enfoques funcionan muy bien, especialmente en consultas ambiguas, produciendo mejoras en NDCG [15] de hasta un 51.28%. En la Sección 4 llevamos este marco algorítmico un paso más allá y proponemos hacer que el proceso de expansión sea adaptable al nivel de claridad de la consulta. Esto produce una mejora adicional del 8.47% sobre el mejor algoritmo previamente identificado. Concluimos y discutimos trabajos futuros en la Sección 5.1. Los motores de búsqueda pueden mapear consultas al menos a direcciones IP, por ejemplo, utilizando cookies y analizando los registros de consultas. Sin embargo, al mover el perfil de usuario al nivel del Escritorio, nos aseguramos de que dicha información no esté explícitamente asociada a un usuario en particular y se almacene en el lado del motor de búsqueda. 2. TRABAJO PREVIO Este artículo reúne dos áreas de IR: Personalización de la búsqueda y Expansión automática de consultas. Existe una gran cantidad de algoritmos para ambos dominios. Sin embargo, no se ha hecho mucho específicamente dirigido a combinarlos. En esta sección presentamos un análisis separado, primero introduciendo algunos enfoques para personalizar la búsqueda, ya que esto representa el principal objetivo de nuestra investigación, y luego discutiendo varias técnicas de expansión de consultas y su relación con nuestros algoritmos. 2.1 Búsqueda Personalizada La búsqueda personalizada comprende dos componentes principales: (1) Perfiles de usuario y (2) El algoritmo de búsqueda real. Esta sección divide el trasfondo relevante según el enfoque de cada artículo en uno de estos elementos. Enfoques centrados en el Perfil del Usuario. Sugiyama et al. [32] analizaron el comportamiento de navegación por internet y generaron perfiles de usuario como características (términos) de las páginas visitadas. Al emitir una nueva consulta, los resultados de búsqueda se clasificaron según la similitud entre cada URL y el perfil del usuario. Qiu y Cho [26] utilizaron Aprendizaje Automático en el historial de clics pasado del usuario para determinar vectores de preferencia de temas y luego aplicar PageRank Sensible al Tema [13]. La creación de perfiles de usuario basada en el historial de navegación tiene la ventaja de ser bastante fácil de obtener y procesar. Probablemente por eso también es utilizado por varios motores de búsqueda industriales (por ejemplo, Yahoo!). MiWeb2. Sin embargo, definitivamente no es suficiente para obtener una comprensión completa de los intereses de los usuarios. Además, requiere almacenar toda la información personal en el servidor, lo cual plantea importantes preocupaciones de privacidad. Solo dos enfoques adicionales mejoraron la búsqueda web utilizando datos de escritorio, sin embargo, ambos utilizaron ideas centrales diferentes: (1) Teevan et al. [34] modificaron los pesos de los términos de consulta del esquema de ponderación BM25 para incorporar los intereses de los usuarios capturados por sus índices de escritorio; (2) En Chirita et al. [6], nos enfocamos en volver a clasificar la salida de la búsqueda web de acuerdo con la distancia del coseno entre cada URL y un conjunto de términos de escritorio que describen los intereses de los usuarios. Además, ninguno de ellos investigó la aplicación adaptativa de la personalización. Enfoques centrados en el Algoritmo de Personalización. Incorporar de manera efectiva el aspecto de personalización directamente en PageRank [25] (es decir, sesgándolo hacia un conjunto objetivo de páginas) ha recibido mucha atención recientemente. Haveliwala [13] calculó un PageRank orientado a temas, en el que inicialmente se calcularon fuera de línea 16 vectores de PageRank sesgados en cada uno de los temas principales del Directorio Abierto, y luego se combinaron en tiempo de ejecución en función de la similitud entre la consulta del usuario y cada uno de los 16 temas. Más recientemente, Nie et al. [24] modificaron la idea distribuyendo el PageRank de una página entre los temas que contiene para generar clasificaciones orientadas a temas. Jeh y Widom propusieron un algoritmo que evita los recursos masivos necesarios para almacenar un Vector de PageRank Personalizado (PPV) por usuario al precalcular PPVs solo para un pequeño conjunto de páginas y luego aplicar una combinación lineal. Dado que el cálculo de los valores predictivos positivos para conjuntos más grandes de páginas seguía siendo bastante costoso, se han investigado varias soluciones, siendo las más importantes las de Fogaras y Racz [12], y Sarlos et al. [30], estos últimos utilizando redondeo y esbozo de conteo mínimo para obtener rápidamente aproximaciones lo suficientemente precisas de las puntuaciones personalizadas. 2.2 Expansión Automática de Consultas La expansión automática de consultas tiene como objetivo derivar una mejor formulación de la consulta del usuario para mejorar la recuperación. Se basa en explotar diversas características sociales o de colección específicas para generar términos adicionales, que se añaden al original en http://myWeb2.search.yahoo.com antes de identificar los documentos coincidentes devueltos como resultado. En esta sección revisamos algunos de los trabajos representativos de expansión de consultas agrupados según la fuente utilizada para generar términos adicionales: (1) Retroalimentación de relevancia, (2) Estadísticas de co-ocurrencia basadas en la colección y (3) Información de tesauro. Algunos enfoques adicionales también se abordan al final de la sección. Técnicas de retroalimentación de relevancia. La idea principal de la Retroalimentación Relevante (RF) es que se puede extraer información útil de los documentos relevantes devueltos para la consulta inicial. Los primeros enfoques fueron manuales [28] en el sentido de que el usuario era quien elegía los resultados relevantes, y luego se aplicaron varios métodos para extraer nuevos términos relacionados con la consulta y los documentos seleccionados. Efthimiadis [11] presentó una revisión exhaustiva de la literatura y propuso varios métodos simples para extraer palabras clave nuevas basadas en la frecuencia de términos, la frecuencia de documentos, etc. Utilizamos algunas de estas como inspiración para nuestras técnicas específicas de escritorio. Chang y Hsu [5] pidieron a los usuarios que eligieran grupos relevantes en lugar de documentos, reduciendo así la cantidad de interacción necesaria. RF también se ha demostrado que se automatiza de manera efectiva al considerar los documentos mejor clasificados como relevantes [37] (esto se conoce como Pseudo RF). Lam y Jones [21] utilizaron la sumarización para extraer frases informativas de los documentos mejor clasificados, y las añadieron a la consulta del usuario. Carpineto et al. [4] maximizaron la divergencia entre el modelo de lenguaje definido por los documentos recuperados en la parte superior y el definido por toda la colección. Finalmente, Yu et al. [38] seleccionaron los términos de expansión de segmentos basados en la visión de páginas web para hacer frente a los múltiples temas que residen en ellas. Técnicas basadas en co-ocurrencia. Los términos que co-ocurren con alta frecuencia con las palabras clave emitidas han demostrado aumentar la precisión cuando se agregan a la consulta [17]. Se han desarrollado muchas medidas estadísticas para evaluar de la mejor manera los niveles de relación entre términos, ya sea analizando documentos completos [27], relaciones de afinidad léxica [3] (es decir, pares de palabras estrechamente relacionadas que contienen exactamente uno de los términos de la consulta inicial), etc. También hemos investigado tres enfoques de este tipo para identificar palabras clave relevantes de la consulta en el rico, aunque bastante complejo Repositorio de Información Personal. Técnicas basadas en tesauros. Un método ampliamente explorado es expandir la consulta del usuario con nuevos términos, cuyo significado esté estrechamente relacionado con las palabras clave de entrada. Tales relaciones suelen extraerse de tesauros a gran escala, como WordNet [23], en los que se encuentran predefinidos diversos conjuntos de sinónimos, hiperónimos, etc. Al igual que con los métodos de co-ocurrencia, los experimentos iniciales con este enfoque fueron controvertidos, reportando tanto mejoras como reducciones en la calidad de la producción [36]. Recientemente, a medida que las colecciones experimentales crecieron en tamaño y los algoritmos empleados se volvieron más complejos, se han obtenido mejores resultados [31, 18, 22]. También utilizamos términos de expansión basados en WordNet. Sin embargo, basamos este proceso en analizar la relación a nivel de escritorio entre la consulta original y las nuevas palabras clave propuestas. Otras técnicas. Hay muchos otros intentos de extraer términos de expansión. Aunque son ortogonales a nuestro enfoque, dos trabajos son muy relevantes para el entorno web: Cui et al. [8] generaron correlaciones de palabras utilizando la probabilidad de que los términos de búsqueda aparezcan en cada documento, calculada a partir de los registros del motor de búsqueda. Kraft y Zien [19] demostraron que el texto de anclaje es muy similar a las consultas de los usuarios, y por lo tanto lo explotaron para adquirir palabras clave adicionales. 3. AMPLIACIÓN DE CONSULTA USANDO DATOS DE ESCRITORIO Los datos de escritorio representan un repositorio muy rico de información de perfilado. Sin embargo, esta información se presenta de una manera muy desestructurada, abarcando documentos que son altamente diversos en formato, contenido e incluso características de idioma. En esta sección abordamos este problema proponiendo varios algoritmos de análisis léxico que aprovechan el PIR de los usuarios para extraer términos de expansión de palabras clave en diversas granularidades, que van desde la frecuencia de términos dentro de los documentos de escritorio hasta la utilización de estadísticas de co-ocurrencia global sobre el repositorio de información personal. Luego, en la segunda parte de la sección analizamos empíricamente el rendimiento de cada enfoque. 3.1 Algoritmos Esta sección presenta los cinco enfoques genéricos para analizar los datos de escritorio de los usuarios con el fin de proporcionar términos de expansión para la búsqueda web. En los algoritmos propuestos aumentamos gradualmente la cantidad de información personal utilizada. Por lo tanto, en la primera parte investigamos tres técnicas de análisis local enfocadas únicamente en aquellos documentos de escritorio que mejor coinciden con la consulta de los usuarios. Añadimos a la consulta web los términos más relevantes, compuestos y resúmenes de oraciones de estos documentos. En la segunda parte de la sección nos dirigimos hacia un análisis global de escritorio, proponiendo investigar las co-ocurrencias de términos, así como los tesauros, en el proceso de expansión. 3.1.1 Expansión con Análisis de Escritorio Local El Análisis de Escritorio Local está relacionado con mejorar la Retroalimentación de Relevancia Pseudo para generar palabras clave de expansión de consulta a partir de los mejores resultados de PIR para la consulta web de los usuarios, en lugar de los resultados de búsqueda web mejor clasificados. Distinguimos tres niveles de granularidad para este proceso e investigamos cada uno de ellos por separado. Frecuencia de término y de documento. Como medidas más simples posibles, TF y DF tienen la ventaja de ser muy rápidas de calcular. Experimentos previos con conjuntos de datos pequeños han demostrado que producen resultados muy buenos [11]. Asociamos de forma independiente una puntuación a cada término, basada en cada una de las dos estadísticas. La versión basada en TF se obtiene multiplicando la frecuencia real de un término con una puntuación de posición descendente a medida que el término aparece más cerca del final del documento. Esto es necesario especialmente para documentos más largos, ya que los términos más informativos tienden a aparecer al principio de los mismos [10]. La fórmula completa de <br>extracción de palabras clave</br> basada en TF es la siguiente: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) donde nrWords es el número total de términos en el documento y pos es la posición de la primera aparición del término; TF representa la frecuencia de cada término en el documento de escritorio que coincide con la consulta web de los usuarios. La identificación de términos de expansión adecuados es aún más sencilla al usar DF: Dado el conjunto de documentos de escritorio relevantes de Top-K, genere sus fragmentos enfocados en la solicitud de búsqueda original. Esta orientación de consulta es necesaria, ya que las puntuaciones de DF se calculan a nivel de todo el PIR y de lo contrario producirían sugerencias demasiado ruidosas. Una vez que se ha identificado el conjunto de términos candidatos, la selección continúa ordenándolos según las puntuaciones de DF con las que están asociados. Las disputas se resuelven utilizando los puntajes de TF correspondientes. Ten en cuenta que un enfoque híbrido TFxIDF no es necesariamente eficiente, ya que un término de escritorio puede tener un alto DF en el escritorio, mientras que es bastante raro en la web. Por ejemplo, el término PageRank sería bastante frecuente en el escritorio de un científico de RI, logrando así una puntuación baja con TFxIDF. Sin embargo, como es bastante raro en la web, sería una buena resolución de la consulta hacia el tema correcto. Compuestos léxicos. Anick y Tipirneni [2] definieron la hipótesis de dispersión léxica, según la cual la dispersión léxica de una expresión (es decir, el número de compuestos diferentes en los que aparece dentro de un documento o grupo de documentos) se puede utilizar para identificar automáticamente conceptos clave en el conjunto de documentos de entrada. Aunque existen varias expresiones compuestas posibles, se ha demostrado que los enfoques simples basados en el análisis de sustantivos son casi tan buenos como los algoritmos altamente complejos de identificación de patrones de partes del discurso [1]. Por lo tanto, inspeccionamos los documentos de escritorio coincidentes en busca de todos sus compuestos léxicos de la siguiente forma: { ¿adjetivo? sustantivo+ } Todos estos compuestos podrían generarse fácilmente sin conexión, en el momento de indexación, para todos los documentos en el repositorio local. Además, una vez identificados, pueden ser clasificados aún más dependiendo de su dispersión dentro de cada documento para facilitar la recuperación rápida de los compuestos más frecuentes en tiempo de ejecución. Selección de oraciones. Esta técnica se basa en la sumarización de documentos orientada a oraciones: primero, se identifica el conjunto de documentos de escritorio relevantes; luego, se genera un resumen que contiene sus oraciones más importantes como resultado. La selección de oraciones es el enfoque de análisis local más completo, ya que produce las expansiones más detalladas (es decir, oraciones). Su desventaja es que, a diferencia de los dos primeros algoritmos, su salida no se puede almacenar de manera eficiente y, en consecuencia, no se puede calcular sin conexión. Generamos resúmenes basados en oraciones clasificando las oraciones del documento según su puntuación de relevancia, de la siguiente manera [21]: Puntuación de la Oración = SW2 TW + PS + TQ2 NQ El primer término es la proporción entre la cantidad cuadrada de palabras significativas dentro de la oración y el número total de palabras en ella. Una palabra es significativa en un documento si su frecuencia es superior a un umbral de la siguiente manera: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , si NS < 25 7 , si NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , si NS > 40, donde NS es el número total de oraciones en el documento (ver [21] para más detalles). El segundo término es un puntaje de posición establecido en (Promedio(NS) − ÍndiceDeOración)/Promedio2(NS) para las primeras diez oraciones, y en 0 en caso contrario, donde Promedio(NS) es el número promedio de oraciones en todos los elementos de escritorio. De esta manera, los documentos cortos como los correos electrónicos no se ven afectados, lo cual es correcto, ya que generalmente no contienen un resumen al principio. Sin embargo, dado que los documentos más largos suelen incluir frases descriptivas generales al principio [10], es más probable que estas frases sean relevantes. El término final sesga el resumen hacia la consulta. Es la proporción entre el cuadrado del número de términos de búsqueda presentes en la oración y el número total de términos de la búsqueda. Se basa en la creencia de que cuantos más términos de consulta contenga una oración, es más probable que esa oración transmita información altamente relacionada con la consulta. 3.1.2 Ampliación con Análisis Global de Escritorio En contraste con el enfoque presentado anteriormente, el análisis global se basa en información de todo el Escritorio personal para inferir los nuevos términos de consulta relevantes. En esta sección proponemos dos técnicas, a saber, estadísticas de co-ocurrencia de términos y filtrado de la salida de un tesauro externo. Estadísticas de co-ocurrencia de términos. Para cada término, podemos calcular fácilmente fuera de línea aquellos términos que co-ocurren con él con mayor frecuencia en una colección dada (es decir, PIR en nuestro caso), y luego aprovechar esta información en tiempo de ejecución para inferir palabras clave altamente correlacionadas con la consulta del usuario. Nuestro algoritmo de expansión de consulta basado en co-ocurrencia genérica es el siguiente: Algoritmo 3.1.2.1. Búsqueda de similitud de palabras clave basada en la co-ocurrencia. Cómputo fuera de línea: 1: Filtrar palabras clave potenciales k con DF ∈ [10, . . . , 20% · N] 2: Para cada palabra clave ki 3: Para cada palabra clave kj 4: Calcular SCki,kj, el coeficiente de similitud de (ki, kj) Cómputo en línea: 1: Sea S el conjunto de palabras clave, potencialmente similares a una expresión de entrada E. 2: Para cada palabra clave k de E: 3: S ← S ∪ TSC(k), donde TSC(k) contiene los términos Top-K más similares a k 4: Para cada término t de S: 5a: Sea Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Sea Score(t) ← #DesktopHits(E|t) 6: Seleccionar los términos Top-K de S con las puntuaciones más altas. La computación fuera de línea necesita una fase inicial de recorte (paso 1) con fines de optimización. Además, también restringimos el algoritmo para calcular niveles de co-ocurrencia solo entre sustantivos, ya que contienen de lejos la mayor cantidad de información conceptual, y este enfoque reduce considerablemente el tamaño de la matriz de co-ocurrencia. Durante la fase de tiempo de ejecución, una vez identificados los términos más correlacionados con cada palabra clave de consulta en particular, es necesaria una operación adicional, a saber, calcular la correlación de cada término de salida con toda la consulta. Dos enfoques son posibles: (1) utilizando un producto de la correlación entre el término y todas las palabras clave en la expresión original (paso 5a), o (2) simplemente contando el número de documentos en los que el término propuesto co-ocurre con la consulta completa del usuario (paso 5b). Consideramos las siguientes fórmulas para los Coeficientes de Similitud [17]: • Similitud Coseno, definida como: CS = DFx,y pDFx · DFy (2) • Información Mutua, definida como: MI = log N · DFx,y DFx · DFy (3) • Razón de Verosimilitud, definida en los párrafos siguientes. DFx es la Frecuencia del Documento del término x, y DFx,y es el número de documentos que contienen tanto x como y. Para aumentar aún más la calidad de las puntuaciones generadas, limitamos este último indicador a las coocurrencias dentro de una ventana de W términos. Establecemos W para que sea igual a la cantidad máxima de palabras clave de expansión deseadas. El Ratio de Verosimilitud de Dunnings λ [9] es una métrica basada en la co-ocurrencia similar a χ2. Comienza intentando rechazar la hipótesis nula, según la cual los dos términos A y B aparecerían en el texto de forma independiente entre sí. Esto significa que P(A B) = P(A¬B) = P(A), donde P(A¬B) es la probabilidad de que el término A no esté seguido por el término B. Por lo tanto, la prueba de independencia de A y B se puede realizar observando si la distribución de A dado que B está presente es la misma que la distribución de A dado que B no está presente. Por supuesto, en realidad sabemos que estos términos no son independientes en el texto, y solo utilizamos las métricas estadísticas para resaltar los términos que aparecen con frecuencia juntos. Comparamos los dos procesos binomiales utilizando las razones de verosimilitud de sus hipótesis asociadas. Primero, definamos la razón de verosimilitud para una hipótesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) donde ω es un punto en el espacio de parámetros Ω, Ω0 es la hipótesis particular que se está probando, y k es un punto en el espacio de observaciones K. Si asumimos que dos distribuciones binomiales tienen el mismo parámetro subyacente, es decir, {(p1, p2) | p1 = p2}, podemos escribir: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) donde H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡. Dado que las máximas se obtienen con p1 = k1 n1 , p2 = k2 n2 , y p = k1+k2 n1+n2 , tenemos: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) donde L(p, k, n) = pk · (1 − p)n−k. Tomando el logaritmo de la verosimilitud, obtenemos: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] donde log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finalmente, si escribimos O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B) y O22 = P(¬A¬B), entonces la probabilidad de co-ocurrencia de los términos A y B se convierte en: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] donde p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , y p = k1+k2 n1+n2 Expansión basada en tesauro. Los tesauros a gran escala encapsulan el conocimiento global sobre las relaciones entre términos. Por lo tanto, primero identificamos el conjunto de términos estrechamente relacionados con cada palabra clave de la consulta, y luego calculamos el nivel de co-ocurrencia en el escritorio de cada uno de estos posibles términos de expansión con toda la solicitud de búsqueda inicial. Al final, se conservan aquellas sugerencias con las frecuencias más altas. El algoritmo es el siguiente: Algoritmo 3.1.2.2. Expansión de consulta basada en tesauro filtrado. 1: Para cada palabra clave k de una consulta de entrada Q: 2: Seleccionar los siguientes conjuntos de términos relacionados utilizando WordNet: 2a: Sin: Todos los sinónimos 2b: Sub: Todos los subconceptos que residen un nivel por debajo de k 2c: Super: Todos los superconceptos que residen un nivel por encima de k 3: Para cada conjunto Si de los conjuntos mencionados anteriormente: 4: Para cada término t de Si: 5: Buscar en el PIR con (Q|t), es decir, la consulta original, expandida con t 6: Dejar que H sea el número de coincidencias de la búsqueda anterior (es decir, el nivel de co-ocurrencia de t con Q) 7: Devolver los términos Top-K ordenados por sus valores de H. Observamos tres tipos de relaciones de términos (pasos 2a-2c): (1) sinónimos, (2) subconceptos, es decir, hipónimos (es decir, subclases) y merónimos (es decir, subpartes), y (3) superconceptos, es decir, hiperónimos (es decir, superclases) y holónimos (es decir, superpartes). Dado que representan tipos de asociación bastante diferentes, los investigamos por separado. Limitamos el conjunto de expansión de salida (paso 7) para contener solo términos que aparecen al menos T veces en el escritorio, con el fin de evitar sugerencias ruidosas, donde T = min(N DocsPerTopic, MinDocs). Establecimos DocsPerTopic = 2, 500, y MinDocs = 5, este último abordando el caso de PIRs pequeños. 3.2 Experimentos 3.2.1 Configuración Experimental Evaluamos nuestros algoritmos con 18 sujetos (estudiantes de doctorado y posdoctorado en diferentes áreas de informática y educación). Primero, instalaron nuestro motor de búsqueda basado en Lucene y, claramente, si ya se había instalado una aplicación de búsqueda de escritorio, entonces este sobrecosto no estaría presente. Indexaron todo su contenido almacenado localmente: archivos dentro de rutas seleccionadas por el usuario, correos electrónicos y caché web. Sin pérdida de generalidad, enfocamos los experimentos en máquinas de un solo usuario. Luego, eligieron 4 consultas relacionadas con sus actividades diarias, de la siguiente manera: • Una consulta muy frecuente en AltaVista, extraída de las consultas más emitidas al motor de búsqueda dentro de un registro de 7.2 millones de entradas de octubre de 2001. Para conectar una consulta de este tipo con los intereses de cada usuario, agregamos una fase de preprocesamiento fuera de línea: Generamos las solicitudes de búsqueda más frecuentes y luego seleccionamos aleatoriamente una consulta con al menos 10 resultados en cada escritorio de los temas. Para garantizar aún más un escenario de la vida real, se permitió a los usuarios rechazar la consulta propuesta y pedir una nueva, si la consideraban totalmente fuera de sus áreas de interés. • Una consulta de registro seleccionada al azar, filtrada utilizando el mismo procedimiento que se mencionó anteriormente. • Una consulta específica seleccionada por ellos mismos, que pensaban que tenía un solo significado. • Una consulta ambigua seleccionada por ellos mismos, que pensaban que tenía al menos tres significados. Las longitudes promedio de las consultas fueron de 2.0 y 2.3 términos para las consultas de registro, así como de 2.9 y 1.8 para las seleccionadas por los usuarios. Aunque nuestros algoritmos están principalmente diseñados para mejorar la búsqueda al utilizar palabras clave ambiguas en las consultas, decidimos investigar su rendimiento en una amplia gama de tipos de consultas, para ver cómo se desempeñan en todas las situaciones. Las consultas de registro evalúan solicitudes de la vida real, en contraste con las auto-seleccionadas, que se centran más en la identificación de los rendimientos superiores e inferiores. Ten en cuenta que los anteriores estaban algo más alejados del interés de cada sujeto, por lo que también eran más difíciles de personalizar. Para obtener una idea de la relación entre cada tipo de consulta y los intereses de los usuarios, pedimos a cada persona que calificara la consulta en sí misma con una puntuación del 1 al 5, con las siguientes interpretaciones: (1) nunca lo ha escuchado, (2) no lo conoce, pero ha oído hablar de ello, (3) lo conoce parcialmente, (4) lo conoce bien, (5) gran interés. Las calificaciones obtenidas fueron 3.11 para las consultas principales, 3.72 para las seleccionadas al azar, 4.45 para las específicas seleccionadas por el usuario y 4.39 para las ambiguas seleccionadas por el usuario. Para cada consulta, recopilamos las 5 URL principales generadas por 20 versiones de los algoritmos presentados en la Sección 3.1. Estos resultados fueron luego mezclados en un conjunto que generalmente contiene entre 70 y 90 URL. Por lo tanto, cada sujeto tuvo que evaluar alrededor de 325 documentos para las cuatro consultas, sin conocer ni el algoritmo ni la clasificación de cada URL evaluada. En total, se emitieron 72 consultas y se evaluaron más de 6,000 URL durante el experimento. Para cada una de estas URL, los probadores tenían que dar una calificación que iba de 0 a 2, dividiendo los resultados relevantes en dos categorías, (1) relevante y (2) altamente relevante. Finalmente, la calidad de cada clasificación fue evaluada utilizando la versión normalizada de la Ganancia Acumulada Descontada (DCG) [15]. DCG es una medida rica, ya que otorga más peso a los documentos altamente clasificados, al mismo tiempo que incorpora diferentes niveles de relevancia al asignarles diferentes valores de ganancia: DCG(i) = G(1) , si i = 1 DCG(i − 1) + G(i)/log(i) , en otro caso. Utilizamos G(i) = 1 para los resultados relevantes, y G(i) = 2 para los altamente relevantes. Dado que las consultas con más documentos de salida relevantes tendrán un DCG más alto, también normalizamos su valor a una puntuación entre 0 (el peor DCG posible dadas las calificaciones) y 1 (el mejor DCG posible dadas las calificaciones) para facilitar el promedio de las consultas. Todos los resultados fueron probados para determinar su significancia estadística utilizando pruebas T. Nota que todas las partes de nivel de escritorio de nuestros algoritmos fueron realizadas con Lucene utilizando sus funciones predefinidas de búsqueda y clasificación. Aspectos específicos del algoritmo. El parámetro principal de nuestros algoritmos es el número de palabras clave de expansión generadas. Para este experimento lo configuramos a 4 términos para todas las técnicas, dejando un análisis en este nivel para una investigación posterior. Para optimizar la velocidad de cálculo en tiempo de ejecución, decidimos limitar el número de palabras clave de salida por documento de escritorio al número de palabras clave de expansión deseadas (es decir, cuatro). Para todos los algoritmos también investigamos limitaciones más grandes. Esto nos permitió observar que el método de Compuestos Léxicos funcionaría mejor si solo se seleccionara como máximo un compuesto por documento. Por lo tanto, decidimos experimentar con este nuevo enfoque también. Para todas las demás técnicas, considerar menos de cuatro términos por documento no pareció producir consistentemente ninguna ganancia cualitativa adicional. Etiquetamos los algoritmos que evaluamos de la siguiente manera: 0. Google: La salida real de la consulta de Google, tal como la devuelve la API de Google; 1. TF, DF: Frecuencia del término y del documento; 2. LC, LC[O]: Compuestos léxicos regulares y optimizados (considerando solo un compuesto principal por documento); 3. SS: Selección de oraciones; 4. TC[CS], TC[MI], TC[LR]: Estadísticas de co-ocurrencia de términos utilizando respectivamente la Similitud de Coseno, la Información Mutua y la Razón de Verosimilitud como coeficientes de similitud; 5. WN[SYN], WN[SUB], WN[SUP]: Expansión basada en WordNet con sinónimos, subconceptos y superconceptos, respectivamente. Excepto por la expansión basada en tesauros, en todos los casos también investigamos el rendimiento de nuestros algoritmos al aprovechar solo la caché del navegador web para representar la información personal de los usuarios. Esto se debe al hecho de que otros documentos personales, como por ejemplo correos electrónicos, se sabe que tienen un lenguaje algo diferente al que se encuentra en la World Wide Web [34]. Sin embargo, dado que este enfoque tuvo un rendimiento notablemente inferior al utilizar todos los datos del escritorio, decidimos omitirlo del análisis posterior. 3.2.2 Resultados de las consultas de registro. Evaluamos todas las variantes de nuestros algoritmos utilizando NDCG. Para consultas de registro, se logró el mejor rendimiento con TF, LC[O] y TC[LR]. Las mejoras que trajeron fueron de hasta un 5.2% para las consultas principales (p = 0.14) y un 13.8% para las consultas seleccionadas al azar (p = 0.01, estadísticamente significativo), ambas obtenidas con LC[O]. Un resumen de todos los resultados se muestra en la Tabla 1. Tanto TF como LC[O] arrojaron resultados muy buenos, lo que indica que enfoques simples basados en palabras clave y expresiones podrían ser suficientes para la tarea de expansión de consultas basada en el escritorio. LC[O] fue mucho mejor que LC, mejorando su calidad hasta en un 25.8% en el caso de consultas de registro seleccionadas al azar, mejora que también fue significativa con p = 0.04. Por lo tanto, una selección de compuestos que abarque varios documentos de escritorio es más informativa sobre los intereses de los usuarios que el enfoque general, en el que no hay restricción en el número de compuestos producidos a partir de cada artículo personal. Los enfoques más complejos orientados a escritorio, es decir, la selección de oraciones y todos los algoritmos basados en la co-ocurrencia de términos, mostraron un rendimiento bastante promedio, sin mejoras visibles, excepto para TC[LR]. Además, la expansión basada en el tesauro generalmente producía muy pocas sugerencias, posiblemente debido a las numerosas consultas técnicas utilizadas por nuestros sujetos. Observamos, sin embargo, que expandir con subconceptos es muy beneficioso para términos de la vida cotidiana (por ejemplo, automóvil), mientras que el uso de superconceptos es valioso para compuestos que tienen al menos un término con baja tecnicidad (por ejemplo, agrupamiento de documentos). Como se esperaba, la expansión basada en sinónimos tuvo un rendimiento generalmente bueno, aunque en algunos casos muy Algorithm NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 1: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar \"top\" (izquierda) y \"aleatorio\" (derecha) en consultas de registro. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Claro vs. Google Ambiguo vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Tabla 2: Ganancia acumulada descontada normalizada en los primeros 5 resultados al buscar consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. En casos técnicos, proporcionó sugerencias bastante generales. Finalmente, notamos que Google está muy optimizado para algunas consultas frecuentes principales. Sin embargo, incluso dentro de este escenario más difícil, algunos de nuestros algoritmos de personalización produjeron mejoras estadísticamente significativas sobre la búsqueda regular (es decir, TF y LC[O]). Consultas auto-seleccionadas. Los valores de NDCG obtenidos con consultas auto-seleccionadas se muestran en la Tabla 2. Si bien nuestros algoritmos no mejoraron Google para las tareas de búsqueda claras, sí produjeron mejoras significativas de hasta un 52.9% (que, por supuesto, también fueron altamente significativas con p 0.01) cuando se utilizaron con consultas ambiguas. De hecho, casi todos nuestros algoritmos resultaron en mejoras estadísticamente significativas sobre Google para este tipo de consulta. En general, las diferencias relativas entre nuestros algoritmos fueron similares a las observadas para las consultas basadas en registros. Como en el análisis anterior, las métricas simples de Frecuencia de Términos y Compuestos Léxicos basadas en el escritorio tuvieron el mejor rendimiento. Sin embargo, también se obtuvo un resultado muy bueno para la selección de oraciones basada en escritorio y todas las métricas de co-ocurrencia de términos. No hubo diferencias visibles entre el comportamiento de los tres enfoques diferentes para el cálculo de la coocurrencia. Finalmente, para el caso de consultas claras, notamos que menos de 4 términos de expansión podrían ser menos ruidosos y, por lo tanto, útiles para lograr más mejoras. Así que seguimos esta idea con los algoritmos adaptativos presentados en la siguiente sección. 4. INTRODUCCIÓN A LA ADAPTABILIDAD En la sección anterior hemos investigado el comportamiento de cada técnica al agregar un número fijo de palabras clave a la consulta del usuario. Sin embargo, un algoritmo óptimo de expansión de consultas personalizadas debería adaptarse automáticamente a varios aspectos de cada consulta, así como a las particularidades de la persona que lo utiliza. En esta sección discutimos los factores que influyen en el comportamiento de nuestros algoritmos de expansión, los cuales podrían ser utilizados como entrada para el proceso de adaptabilidad. Luego, en la segunda parte presentamos algunos experimentos iniciales con uno de ellos, a saber, la claridad de la consulta. 4.1 Factores de Adaptabilidad Varios indicadores podrían ayudar al algoritmo a ajustar automáticamente el número de términos de expansión. Comenzamos discutiendo la adaptación analizando el nivel de claridad de la consulta. Luego, presentamos brevemente un enfoque para modelar el proceso de formulación de consultas genéricas con el fin de adaptar automáticamente el algoritmo de búsqueda, y discutimos algunos otros posibles factores que podrían ser útiles para esta tarea. Claridad de la consulta. El interés por analizar la dificultad de las consultas ha aumentado solo recientemente, y no hay muchos artículos que aborden este tema. Sin embargo, desde hace mucho tiempo se sabe que la desambiguación de consultas tiene un alto potencial para mejorar la efectividad de recuperación en búsquedas de baja recuperación con consultas muy cortas [20], que es exactamente nuestro escenario objetivo. Además, el éxito de los sistemas de IR claramente varía según los diferentes temas. Por lo tanto, proponemos utilizar un número estimado que exprese el nivel calculado de claridad de la consulta para ajustar automáticamente la cantidad de personalización alimentada en el algoritmo. Las siguientes métricas están disponibles: • La Longitud de la Consulta se expresa simplemente por el número de palabras en la consulta del usuario. La solución es bastante ineficiente, según lo informado por He y Ounis [14]. • El Alcance de la Consulta se relaciona con el IDF de toda la consulta, como en: C1 = log( #DocumentosEnColección #Resultados(Consulta) ) (7) Esta métrica funciona bien cuando se utiliza con colecciones de documentos que cubren un solo tema, pero mal en otros casos [7, 14]. • La Claridad de la Consulta [7] parece ser la mejor, así como la técnica más aplicada hasta ahora. Mide la divergencia entre el modelo de lenguaje asociado a la consulta del usuario y el modelo de lenguaje asociado a la colección. En una versión simplificada (es decir, sin suavizar los términos que no están presentes en la consulta), se puede expresar de la siguiente manera: C2 =  w∈Consulta Pml(w|Consulta) · log Pml(w|Consulta) Pcoll(w) (8) donde Pml(w|Consulta) es la probabilidad de la palabra w dentro de la consulta enviada, y Pcoll(w) es la probabilidad de w dentro de toda la colección de documentos. Otras soluciones existen, pero creemos que son demasiado costosas computacionalmente para la gran cantidad de datos que necesitan ser procesados dentro de las aplicaciones web. Por lo tanto, decidimos investigar solo C1 y C2. Primero, analizamos su rendimiento en un gran conjunto de consultas y dividimos sus predicciones de claridad en tres categorías: • Pequeño alcance / Consulta clara: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Mediano alcance / Consulta semi-ambigua: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Gran alcance / Consulta ambigua: C1 ∈ [17, ∞), C2 ∈ [0, 2.5]. Para limitar la cantidad de experimentos, analizamos solo los resultados producidos al emplear C1 para el PIR y C2 para la Web. Como base algorítmica utilizamos LC[O], es decir, compuestos léxicos optimizados, que claramente fue el método ganador en el análisis anterior. Como la investigación manual mostró que se ajustaba ligeramente a los términos de expansión para consultas claras, utilizamos un sustituto para este caso particular. Dos candidatos fueron considerados: (1) TF, es decir, el segundo mejor enfoque, y (2) WN[SYN], ya que observamos que sus primeros y segundos términos de expansión eran frecuentemente muy buenos. Alcance de escritorio Claridad web N.º de términos Algoritmo Grande Ambiguo 4 LC[O] Grande Semi-ambiguo 3 LC[O] Grande Claro 2 LC[O] Mediano Ambiguo 3 LC[O] Mediano Semi-ambiguo 2 LC[O] Mediano Claro 1 TF / WN[SYN] Pequeño Ambiguo 2 TF / WN[SYN] Pequeño Semi-ambiguo 1 TF / WN[SYN] Pequeño Claro 0Tabla 3: Expansión de consulta personalizada adaptativa. Dado los algoritmos y medidas de claridad, implementamos el procedimiento de adaptabilidad ajustando la cantidad de términos de expansión añadidos a la consulta original, como función de su ambigüedad en la Web, así como dentro de los PIR de los usuarios. Ten en cuenta que el nivel de ambigüedad está relacionado con la cantidad de documentos que cubren una determinada consulta. Por lo tanto, en cierta medida, tiene diferentes significados en la web y dentro de los PIRs. Si una consulta considerada ambigua en una gran colección como la Web muy probablemente tenga de hecho un gran número de significados, esto puede no ser el caso para el Escritorio. Tomemos como ejemplo la consulta PageRank. Si el usuario es un experto en análisis de enlaces, muchos de sus documentos podrían coincidir con este término, y por lo tanto, la consulta se clasificaría como ambigua. Sin embargo, al analizarlo en la web, esta es definitivamente una consulta clara. En consecuencia, empleamos más términos adicionales cuando la consulta era más ambigua en la Web, pero también en el escritorio. Dicho de otra manera, las consultas consideradas claras en el escritorio no estaban bien cubiertas en el PIR de los usuarios y, por lo tanto, tenían menos palabras clave añadidas a ellas. El número de términos de expansión que utilizamos para cada combinación de niveles de alcance y claridad se muestra en la Tabla 3. Proceso de formulación de consultas. La expansión interactiva de consultas tiene un alto potencial para mejorar la búsqueda [29]. Creemos que modelar su proceso subyacente sería muy útil para producir algoritmos de búsqueda web adaptativos cualitativos. Por ejemplo, cuando el usuario está agregando un nuevo término a su consulta previamente emitida, básicamente está reformulando su solicitud original. Por lo tanto, es más probable que los términos recién agregados transmitan información sobre sus objetivos de búsqueda. Para un motor de búsqueda general y no personalizado, esto podría corresponder a darle más peso a estas nuevas palabras clave. Dentro de nuestro escenario personalizado, las expansiones generadas también pueden estar sesgadas hacia estos términos. Sin embargo, se necesitan más investigaciones para resolver los desafíos planteados por este enfoque. Otras características. La idea de adaptar el proceso de recuperación a varios aspectos de la consulta, del propio usuario e incluso del algoritmo empleado ha recibido poca atención en la literatura. Solo se han investigado algunos enfoques, generalmente de forma indirecta. Existen estudios sobre los comportamientos de búsqueda en diferentes momentos del día, o sobre los temas abarcados por las consultas de distintas clases de usuarios, etc. Sin embargo, generalmente no discuten cómo estas características pueden ser realmente incorporadas en el proceso de búsqueda en sí mismo y casi nunca han sido relacionadas con la tarea de personalización web. 4.2 Experimentos Utilizamos exactamente la misma configuración experimental que en nuestro análisis anterior, con dos consultas basadas en registros y dos seleccionadas por los usuarios (todas diferentes a las anteriores, para asegurarnos de que no haya sesgo en los nuevos enfoques), evaluadas con NDCG sobre los resultados de los cinco primeros obtenidos por cada algoritmo. Los algoritmos de expansión de consultas personalizadas adaptativas recientemente propuestos se denominan A[LCO/TF] para el enfoque que utiliza TF con las consultas claras de escritorio, y como A[LCO/WN] cuando en lugar de TF se utilizó WN[SYN]. Los resultados generales fueron al menos similares, o mejores que los de Google para todo tipo de consultas de registro (ver Tabla 4). Para las consultas más frecuentes, Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 4: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizada adaptativa en consultas de registro principales (izquierda) y aleatorias (derecha) en Google. Algoritmo NDCG Signific. NDCG significa \"Normalized Discounted Cumulative Gain\". Tabla 5: Ganancia acumulada descontada normalizada en los primeros 5 resultados al utilizar nuestros algoritmos de búsqueda personalizados adaptativos en consultas claras (izquierda) y ambiguas (derecha) seleccionadas por el usuario. Ambos algoritmos adaptativos, A[LCO/TF] y A[LCO/WN], mejoran en un 10.8% y 7.9% respectivamente, siendo ambas diferencias también estadísticamente significativas con p ≤ 0.01. También logran una mejora de hasta un 6.62% sobre el algoritmo estático de mejor rendimiento, LC[O] (p = 0.07). Para consultas seleccionadas al azar, aunque A[LCO/TF] arroja resultados significativamente mejores que Google (p = 0.04), ambos enfoques adaptativos quedan rezagados frente a los algoritmos estáticos. La razón principal parece ser la selección imperfecta del número de términos de expansión, en función de la claridad de la consulta. Por lo tanto, se necesitan más experimentos para determinar el número óptimo de palabras clave de expansión generadas, en función del nivel de ambigüedad de la consulta. El análisis de las consultas auto-seleccionadas muestra que la adaptabilidad puede llevar a mejoras aún mayores en la personalización de la búsqueda en la web (ver Tabla 5). Para consultas ambiguas, las puntuaciones otorgadas a la búsqueda en Google se mejoran en un 40.6% a través de A[LCO/TF] y en un 35.2% a través de A[LCO/WN], ambos altamente significativos con p 0.01. La adaptabilidad también aporta una mejora adicional del 8.9% sobre la personalización estática de LC[O] (p = 0.05). Incluso para consultas claras, los algoritmos flexibles recién propuestos tienen un rendimiento ligeramente mejor, mejorando en un 0.4% y 1.0% respectivamente. Todos los resultados se representan gráficamente en la Figura 1. Observamos que A[LCO/TF] es el mejor algoritmo en general, funcionando mejor que Google para todo tipo de consultas, ya sea extraídas del registro del motor de búsqueda o seleccionadas por el usuario. Los experimentos presentados en esta sección confirman claramente que la adaptabilidad es un paso necesario a seguir en la personalización de la búsqueda en la web. 5. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo propusimos ampliar las consultas de búsqueda en la web mediante la explotación del Repositorio de Información Personal de los usuarios para extraer automáticamente palabras clave adicionales relacionadas tanto con la consulta en sí como con los intereses de los usuarios, personalizando la salida de la búsqueda. En este contexto, el artículo incluye las siguientes contribuciones: • Propusimos cinco técnicas para determinar términos de expansión a partir de documentos personales. Cada uno de ellos produce palabras clave de consulta adicionales mediante el análisis de los escritorios de los usuarios en niveles de granularidad creciente, que van desde el análisis a nivel de términos y expresiones hasta estadísticas de co-ocurrencia global y tesauros externos. Figura 1: Ganancia relativa de NDCG (en %) para cada algoritmo en general, así como separada por categoría de consulta. • Realizamos un análisis empírico exhaustivo de varias variantes de nuestros enfoques, en cuatro escenarios diferentes. Mostramos que algunos de estos enfoques funcionan muy bien, produciendo mejoras en NDCG de hasta un 51.28%. • Llevamos este marco de búsqueda personalizada un paso más allá y propusimos hacer que el proceso de expansión sea adaptable a las características de cada consulta, poniendo un fuerte énfasis en su nivel de claridad. • En un conjunto separado de experimentos, demostramos que nuestros algoritmos adaptativos proporcionan una mejora adicional del 8.47% sobre el enfoque mejor identificado previamente. Actualmente estamos realizando investigaciones sobre la dependencia entre diversas características de la consulta y el número óptimo de términos de expansión. También estamos analizando otros tipos de enfoques para identificar sugerencias de expansión de consultas, como aplicar Análisis Semántico Latente en los datos de la computadora. Finalmente, estamos diseñando un conjunto de combinaciones más complejas de estas métricas para proporcionar una adaptabilidad mejorada a nuestros algoritmos. AGRADECIMIENTOS Agradecemos a Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo y Vanessa Murdock de Yahoo! por las interesantes discusiones sobre la configuración experimental y los algoritmos que presentamos. Estamos agradecidos a Fabrizio Silvestri del CNR y a Ronny Lempel de IBM por proporcionarnos el registro de consultas de AltaVista. Finalmente, agradecemos a nuestros colegas de L3S por participar en los experimentos que realizamos, así como a la Comisión Europea por el apoyo financiero (proyecto Nepomuk, 6º Programa Marco, contrato IST n.º 027705). 7. REFERENCIAS [1] J. Allan y H. Raghavan. Utilizando patrones de partes del discurso para reducir la ambigüedad de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [2] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: Retroalimentación terminológica para la búsqueda iterativa de información. En Actas del 22º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka y A. Soffer. Refinamiento automático de consultas utilizando afinidades léxicas con ganancia de información máxima. En Actas de la 25ª Conferencia Internacional. ACM SIGIR Conf. sobre investigación y desarrollo en recuperación de información, páginas 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano y B. Bigi. Un enfoque de teoría de la información para la expansión automática de consultas. ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang y C.-C. Hsu. Integrando la expansión de consultas y la retroalimentación de relevancia conceptual para la recuperación personalizada de información web. En Actas de la 7ª Conferencia Internacional. Conf. en la World Wide Web, 1998. [6] P. A. Chirita, C. Firan y W. Nejdl. Resumiendo el contexto local para personalizar la búsqueda web global. En Actas de la 15ª Conferencia Internacional. CIKM Conf. sobre Gestión de Información y Conocimiento, 2006. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Prediciendo el rendimiento de la consulta. En Actas de la 25ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2002. [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de la 11ª Conferencia Internacional. Conf. en la World Wide Web, 2002. [9] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19:61-74, 1993. [10] H. P. Edmundson. Nuevos métodos en extracción automática. Revista de la ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis. Una nueva vara de medir para la evaluación de algoritmos de clasificación para la expansión interactiva de consultas. Procesamiento y Gestión de la Información, 31(4):605-620, 1995. [12] D. Fogaras y B. Racz. Búsqueda de similitud basada en enlaces escalables. En Actas de la 14ª Conferencia Internacional. Conferencia de la World Wide Web, 2005. [13] T. Haveliwala. PageRank sensible al tema. En Actas de la 11ª Conferencia Internacional. Conferencia de la World Wide Web, Honolulu, Hawái, mayo de 2002. [14] B. Él y yo. Ounis. Inferir el rendimiento de la consulta utilizando predictores previos a la recuperación. En Actas de la 11ª Conferencia Internacional. Conferencia SPIRE sobre Procesamiento de Cadenas y Recuperación de Información, 2004. [15] K. Järvelin y J. Keklinen. Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la 23ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2000. [16] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de la 12ª Conferencia Internacional. Conferencia de la World Wide Web, 2003. [17] M.-C. Kim y K.-S. Choi. Una comparación de medidas de similitud basadas en colocación en la expansión de consultas. I'm sorry, but the sentence \"Inf.\" is not a complete sentence and cannot be translated without context. Please provide more information or another sentence for translation. Proc. y Mgmt., 35(1):19-30, 1999. [18] S.-B. Kim, H.-C. Seo y H.-C. Rim. Recuperación de información utilizando sentidos de palabras: enfoque de etiquetado del sentido raíz. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [19] R. Kraft y J. Zien. Extracción de texto ancla para el refinamiento de consultas. En Actas de la 13ª Conferencia Internacional. Conf. en la World Wide Web, 2004. [20] R. Krovetz y W. B. Croft. Ambigüedad léxica y recuperación de información. ACM Trans. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Syst., 10(2), 1992. [21] A. M. Lam-Adesina y G. J. F. Jones. Aplicando técnicas de resumen para la selección de términos en la retroalimentación de relevancia. En Actas del 24º Congreso Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2001. [22] S. Liu, F. Liu, C. Yu y W. Meng. Un enfoque efectivo para la recuperación de documentos mediante el uso de WordNet y el reconocimiento de frases. En Actas de la 27ª Conferencia Internacional. Conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2004. [23] G. Miller. Wordnet: Una base de datos léxica electrónica. Comunicaciones de la ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison y X. Qi. Análisis de enlaces temáticos para búsqueda web. En Actas de la 29ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 2006. [25] L. Page, S. Brin, R. Motwani y T. Winograd. El ranking de citas PageRank: Trayendo orden al web. Informe técnico, Universidad de Stanford, 1998. [26] F. Qiu y J. Cho. Identificación automática de intereses del usuario para búsqueda personalizada. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [27] Y. Qiu y H.-P. Frei. Expansión de consulta basada en conceptos. En Actas del 16º Congreso Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1993. [28] J. Rocchio.\nTraducción: Retr., 1993. [28] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. El Sistema de Recuperación Inteligente: Experimentos en Procesamiento Automático de Documentos, páginas 313-323, 1971. [29] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª Conferencia Internacional. ACM SIGIR Conf., 2003. [30] T. Sarlos, A. \n\nConferencia ACM SIGIR, 2003. [30] T. Sarlos, A. A. Benczur, K. Csalogany, D. Fogaras y B. Racz. Aleatorizar o no aleatorizar: Resúmenes óptimos de espacio para análisis de hipervínculos. En Actas de la 15ª Conferencia Internacional. WWW Conf., 2006. [31] C. Shah y W. B. Croft. Evaluando técnicas de recuperación de alta precisión. En Actas de la 27ª Conferencia Internacional. ACM SIGIR Conf. sobre Investigación y desarrollo en recuperación de información, páginas 2-9, 2004. [32] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de la 13ª Conferencia Internacional. Conferencia de la World Wide Web, 2004. [33] D. Sullivan. Cuanto más mayor eres, más deseas una búsqueda personalizada, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, y E. Horvitz. Personalización de la búsqueda a través del análisis automatizado de intereses y actividades. En Actas de la 28ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2005. [35] E. Volokh. Personalización y privacidad. This seems to be an incomplete sentence. Could you please provide more context or clarify the text you would like me to translate to Spanish? ACM, 43(8), 2000. [36] E. M. Voorhees. \n\nACM, 43(8), 2000. [36] E. M. Voorhees. Expansión de consulta utilizando relaciones léxico-semánticas. En Actas de la 17ª Conferencia Internacional. Conf. ACM SIGIR sobre Investigación y Desarrollo en Información. Retr., 1994. [37] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª Conferencia Internacional. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 1996. [38] S. Yu, D. Cai, J.-R. Wen y W.-Y. I'm sorry, but \"Ma.\" is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Mejorando la retroalimentación de pseudo relevancia en la recuperación de información web utilizando la segmentación de páginas web. En Actas de la 12ª Conferencia Internacional. Conferencia sobre la World Wide Web, 2003. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "keyword co-occurrence": {
            "translated_key": "co-ocurrencia de palabras clave",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Personalized Query Expansion for the Web Paul - Alexandru Chirita L3S Research Center∗ Appelstr.",
                "9a 30167 Hannover, Germany chirita@l3s.de Claudiu S. Firan L3S Research Center Appelstr. 9a 30167 Hannover, Germany firan@l3s.de Wolfgang Nejdl L3S Research Center Appelstr. 9a 30167 Hannover, Germany nejdl@l3s.de ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval.",
                "In this paper we propose to improve such Web queries by expanding them with terms collected from each users Personal Information Repository, thus implicitly personalizing the search output.",
                "We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri.",
                "Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings.",
                "Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query.",
                "A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.5 [Information Storage and Retrieval]: Online Information Services-Web-based services General Terms Algorithms, Experimentation, Measurement 1.",
                "INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web.",
                "Yet keyword queries are inherently ambiguous.",
                "The query canon book for example covers several different areas of interest: religion, photography, literature, and music.",
                "Clearly, one would prefer search output to be aligned with users topic(s) of interest, rather than displaying a selection of popular URLs from each category.",
                "Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones.",
                "Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly.",
                "It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]).",
                "This is exactly the Web search scenario!",
                "In this paper we propose to enhance Web query reformulation by exploiting the users Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc.",
                "Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably).",
                "First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user.",
                "Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy.",
                "Search engines should not be able to know about a persons interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search).",
                "Our algorithms expand Web queries with keywords extracted from users PIR, thus implicitly personalizing the search output.",
                "After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1.",
                "We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best.",
                "In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri.",
                "The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28%.",
                "In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query.",
                "This yields an additional improvement of 8.47% over the previously identified best algorithm.",
                "We conclude and discuss further work in Section 5. 1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs.",
                "However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. 2.",
                "PREVIOUS WORK This paper brings together two IR areas: Search Personalization and Automatic Query Expansion.",
                "There exists a vast amount of algorithms for both domains.",
                "However, not much has been done specifically aimed at combining them.",
                "In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm.",
                "This section splits the relevant background according to the focus of each article into either one of these elements.",
                "Approaches focused on the User Profile.",
                "Sugiyama et al. [32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages.",
                "Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile.",
                "Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank [13].",
                "User profiling based on browsing history has the advantage of being rather easy to obtain and process.",
                "This is probably why it is also employed by several industrial search engines (e.g., Yahoo!",
                "MyWeb2 ).",
                "However, it is definitely not sufficient for gathering a thorough insight into users interests.",
                "More, it requires to store all personal information at the server side, which raises significant privacy concerns.",
                "Only two other approaches enhanced Web search using Desktop data, yet both used different core ideas: (1) Teevan et al. [34] modified the query term weights from the BM25 weighting scheme to incorporate user interests as captured by their Desktop indexes; (2) In Chirita et al. [6], we focused on re-ranking the Web search output according to the cosine distance between each URL and a set of Desktop terms describing users interests.",
                "Moreover, none of these investigated the adaptive application of personalization.",
                "Approaches focused on the Personalization Algorithm.",
                "Effectively building the personalization aspect directly into PageRank [25] (i.e., by biasing it on a target set of pages) has received much attention recently.",
                "Haveliwala [13] computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics.",
                "More recently, Nie et al. [24] modified the idea by distributing the PageRank of a page across the topics it contains in order to generate topic oriented rankings.",
                "Jeh and Widom [16] proposed an algorithm that avoids the massive resources needed for storing one Personalized PageRank Vector (PPV) per user by precomputing PPVs only for a small set of pages and then applying linear combination.",
                "As the computation of PPVs for larger sets of pages was still quite expensive, several solutions have been investigated, the most important ones being those of Fogaras and Racz [12], and Sarlos et al. [30], the latter using rounding and count-min sketching in order to fastly obtain accurate enough approximations of the personalized scores. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval.",
                "It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in2 http://myWeb2.search.yahoo.com put keywords before identifying the matching documents returned as output.",
                "In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms: (1) Relevance feedback, (2) Collection based co-occurrence statistics, and (3) Thesaurus information.",
                "Some other approaches are also addressed in the end of the section.",
                "Relevance Feedback Techniques.",
                "The main idea of Relevance Feedback (RF) is that useful information can be extracted from the relevant documents returned for the initial query.",
                "First approaches were manual [28] in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents.",
                "Efthimiadis [11] presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.",
                "We used some of these as inspiration for our Desktop specific techniques.",
                "Chang and Hsu [5] asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary.",
                "RF has also been shown to be effectively automatized by considering the top ranked documents as relevant [37] (this is known as Pseudo RF).",
                "Lam and Jones [21] used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query.",
                "Carpineto et al. [4] maximized the divergence between the language model defined by the top retrieved documents and that defined by the entire collection.",
                "Finally, Yu et al. [38] selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein.",
                "Co-occurrence Based Techniques.",
                "Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query [17].",
                "Many statistical measures have been developed to best assess term relationship levels, either analyzing entire documents [27], lexical affinity relationships [3] (i.e., pairs of closely related words which contain exactly one of the initial query terms), etc.",
                "We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository.",
                "Thesaurus Based Techniques.",
                "A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords.",
                "Such relationships are usually extracted from large scale thesauri, as WordNet [23], in which various sets of synonyms, hypernyms, etc. are predefined.",
                "Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality [36].",
                "Recently, as the experimental collections grew larger, and as the employed algorithms became more complex, better results have been obtained [31, 18, 22].",
                "We also use WordNet based expansion terms.",
                "However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords.",
                "Other Techniques.",
                "There are many other attempts to extract expansion terms.",
                "Though orthogonal to our approach, two works are very relevant for the Web environment: Cui et al. [8] generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs.",
                "Kraft and Zien [19] showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 3.",
                "QUERY EXPANSION USING DESKTOP DATA Desktop data represents a very rich repository of profiling information.",
                "However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics.",
                "In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit users PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co-occurrence statistics over the personal information repository.",
                "Then, in the second part of the section we empirically analyze the performance of each approach. 3.1 Algorithms This section presents the five generic approaches for analyzing users Desktop data in order to provide expansion terms for Web search.",
                "In the proposed algorithms we gradually increase the amount of personal information utilized.",
                "Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching users query best.",
                "We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents.",
                "In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co-occurrences, as well as thesauri, in the expansion process. 3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for users Web query, rather than from the top ranked Web search results.",
                "We distinguish three granularity levels for this process and we investigate each of them separately.",
                "Term and Document Frequency.",
                "As the simplest possible measures, TF and DF have the advantage of being very fast to compute.",
                "Previous experiments with small data sets have showed them to yield very good results [11].",
                "We thus independently associate a score with each term, based on each of the two statistics.",
                "The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document.",
                "This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10].",
                "The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching users Web query.",
                "The identification of suitable expansion terms is even simpler when using DF: Given the set of Top-K relevant Desktop documents, generate their snippets as focused on the original search request.",
                "This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise.",
                "Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with.",
                "Ties are resolved using the corresponding TF scores.",
                "Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web.",
                "For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF.",
                "However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic.",
                "Lexical Compounds.",
                "Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expressions lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set.",
                "Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part-of-speech pattern identification algorithms [1].",
                "We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off-line, at indexing time, for all the documents in the local repository.",
                "Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run-time.",
                "Sentence Selection.",
                "This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output.",
                "Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences).",
                "Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off-line.",
                "We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein.",
                "A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details).",
                "The second term is a position score set to (Avg(NS) − SentenceIndex)/Avg2 (NS) for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items.",
                "This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning.",
                "However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant.",
                "The final term biases the summary towards the query.",
                "It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query.",
                "It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query. 3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms.",
                "In this section we propose two such techniques, namely term co-occurrence statistics, and filtering the output of an external thesaurus.",
                "Term Co-occurrence Statistics.",
                "For each term, we can easily compute off-line those terms co-occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run-time in order to infer keywords highly correlated with the user query.",
                "Our generic co-occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1.",
                "Co-occurrence based keyword similarity search.",
                "Off-line computation: 1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On-line computation: 1: Let S be the set of keywords, potentially similar to an input expression E. 2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top-K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top-K terms of S with the highest scores.",
                "The off-line computation needs an initial trimming phase (step 1) for optimization purposes.",
                "In addition, we also restricted the algorithm to computing co-occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co-occurrence matrix considerably.",
                "During the run-time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query.",
                "Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co-occurs with the entire user query (step 5b).",
                "We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below.",
                "DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y.",
                "To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms.",
                "We set W to be the same as the maximum amount of expansion keywords desired.",
                "Dunnings Likelihood Ratio λ [9] is a co-occurrence based metric similar to χ2 .",
                "It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other.",
                "This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B. Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present.",
                "Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together.",
                "We compare the two binomial processes by using likelihood ratios of their associated hypotheses.",
                "First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations K. If we assume that two binomial distributions have the same underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .",
                "Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .",
                "Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p).",
                "Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co-occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion.",
                "Large scale thesauri encapsulate global knowledge about term relationships.",
                "Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co-occurrence level of each of these possible expansion terms with the entire initial search request.",
                "In the end, those suggestions with the highest frequencies are kept.",
                "The algorithm is as follows: Algorithm 3.1.2.2.",
                "Filtered thesaurus based query expansion. 1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub-concepts residing one level below k 2c: Super: All super-concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co-occurence level of t with Q) 7: Return Top-K terms as ordered by their H values.",
                "We observe three types of term relationships (steps 2a-2c): (1) synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes) and meronyms (i.e., sub-parts), and (3) super-concepts, namely hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).",
                "As they represent quite different types of association, we investigated them separately.",
                "We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs).",
                "We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs. 3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D. and PostDoc. students in different areas of computer science and education).",
                "First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present. indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache.",
                "Without loss of generality, we focused the experiments on single-user machines.",
                "Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001.",
                "In order to connect such a query to each users interests, we added an off-line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subjects Desktop.",
                "To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas. • One randomly selected log query, filtered using the same procedure as above. • One self-selected specific query, which they thought to have only one meaning. • One self-selected ambiguous query, which they thought to have at least three meanings.",
                "The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self-selected ones.",
                "Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations.",
                "The log queries evaluate real life requests, in contrast to the self-selected ones, which target rather the identification of top and bottom performances.",
                "Note that the former ones were somewhat farther away from each subjects interest, thus being also more difficult to personalize on.",
                "To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest.",
                "The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self-selected specific ones, and 4.39 for the self-selected ambiguous ones.",
                "For each query, we collected the Top-5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1.",
                "These results were then shuffled into one set containing usually between 70 and 90 URLs.",
                "Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL.",
                "Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment.",
                "For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant.",
                "Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain (DCG) [15].",
                "DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i)/log(i) , otherwise.",
                "We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones.",
                "As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries.",
                "All results were tested for statistical significance using T-tests. 4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions.",
                "Algorithmic specific aspects.",
                "The main parameter of our algorithms is the number of generated expansion keywords.",
                "For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation.",
                "In order to optimize the run-time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four).",
                "For all algorithms we also investigated bigger limitations.",
                "This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected.",
                "We therefore chose to experiment with this new approach as well.",
                "For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain.",
                "We labeled the algorithms we evaluated as follows: 0.",
                "Google: The actual Google query output, as returned by the Google API; 1.",
                "TF, DF: Term and Document Frequency; 2.",
                "LC, LC[O]: Regular and Optimized (by considering only one top compound per document) Lexical Compounds; 3.",
                "SS: Sentence Selection; 4.",
                "TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; 5.",
                "WN[SYN], WN[SUB], WN[SUP]: WordNet based expansion with synonyms, sub-concepts, and super-concepts, respectively.",
                "Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent users personal information.",
                "This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34].",
                "However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis. 3.2.2 Results Log Queries.",
                "We evaluated all variants of our algorithms using NDCG.",
                "For log queries, the best performance was achieved with TF, LC[O], and TC[LR].",
                "The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O].",
                "A summary of all results is depicted in Table 1.",
                "Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task.",
                "LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04.",
                "Thus, a selection of compounds spanning over several Desktop documents is more informative about users interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item.",
                "The more complex Desktop oriented approaches, namely sentence selection and all term co-occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR].",
                "Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects.",
                "We observed however that expanding with sub-concepts is very good for everyday life terms (e.g., car), whereas the use of super-concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering).",
                "As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.42 - 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 - 0.23LC 0.39 - 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 - 0.36TC[CS] 0.37 - 0.35TC[MI] 0.40 - 0.36TC[LR] 0.41 - 0.42 p = 0.06 WN[SYN] 0.42 - 0.38WN[SUB] 0.28 - 0.33WN[SUP] 0.26 - 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.71 - 0.39TF 0.66 - 0.52 p 0.01 DF 0.37 - 0.31LC 0.65 - 0.54 p 0.01 LC[O] 0.69 - 0.59 p 0.01 SS 0.56 - 0.52 p 0.01 TC[CS] 0.60 - 0.50 p = 0.01 TC[MI] 0.60 - 0.47 p = 0.02 TC[LR] 0.56 - 0.47 p = 0.03 WN[SYN] 0.70 - 0.36WN[SUB] 0.46 - 0.32WN[SUP] 0.51 - 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries. technical cases it yielded rather general suggestions.",
                "Finally, we noticed Google to be very optimized for some top frequent queries.",
                "However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]).",
                "Self-selected Queries.",
                "The NDCG values obtained with selfselected queries are depicted in Table 2.",
                "While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries.",
                "In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type.",
                "In general, the relative differences between our algorithms were similar to those observed for the log based queries.",
                "As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best.",
                "Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co-occurrence metrics.",
                "There were no visible differences between the behavior of the three different approaches to cooccurrence calculation.",
                "Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements.",
                "We thus pursued this idea with the adaptive algorithms presented in the next section. 4.",
                "INTRODUCING ADAPTIVITY In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query.",
                "However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it.",
                "In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process.",
                "Then, in the second part we present some initial experiments with one of them, namely query clarity. 4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms.",
                "We start by discussing adaptation by analyzing the query clarity level.",
                "Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task.",
                "Query Clarity.",
                "The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic.",
                "Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario.",
                "Also, the success of IR systems clearly varies across different topics.",
                "We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm.",
                "The following metrics are available: • The Query Length is expressed simply by the number of words in the user query.",
                "The solution is rather inefficient, as reported by He and Ounis [14]. • The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14]. • The Query Clarity [7] seems to be the best, as well as the most applied technique so far.",
                "It measures the divergence between the language model associated to the user query and the language model associated to the collection.",
                "In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents.",
                "Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications.",
                "We thus decided to investigate only C1 and C2.",
                "First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope / Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞). • Medium Scope / Semi-Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4). • Large Scope / Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5].",
                "In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web.",
                "As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis.",
                "As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case.",
                "Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good.",
                "Desktop Scope Web Clarity No. of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi-Ambig. 3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi-Ambig. 2 LC[O] Medium Clear 1 TF / WN[SYN] Small Ambiguous 2 TF / WN[SYN] Small Semi-Ambig. 1 TF / WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion.",
                "Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within users PIR.",
                "Note that the ambiguity level is related to the number of documents covering a certain query.",
                "Thus, to some extent, it has different meanings on the Web and within PIRs.",
                "While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop.",
                "Take for example the query PageRank.",
                "If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous.",
                "However, when analyzed against the Web, this is definitely a clear query.",
                "Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop.",
                "Put another way, queries deemed clear on the Desktop were inherently not well covered within users PIR, and thus had fewer keywords appended to them.",
                "The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3.",
                "Query Formulation Process.",
                "Interactive query expansion has a high potential for enhancing search [29].",
                "We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms.",
                "For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request.",
                "Thus, the newly added terms are more likely to convey information about her search goals.",
                "For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords.",
                "Within our personalized scenario, the generated expansions can similarly be biased towards these terms.",
                "Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach.",
                "Other Features.",
                "The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature.",
                "Only some approaches have been investigated, usually indirectly.",
                "There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc.",
                "However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization. 4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log-based queries and two self-selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top-5 results output by each algorithm.",
                "The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO/TF] for the approach using TF with the clear Desktop queries, and as A[LCO/WN] when WN[SYN] was utilized instead of TF.",
                "The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4).",
                "For top frequent queries, Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Top vs. Google Random vs. Google Google 0.51 - 0.45TF 0.51 - 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 - 0.45A[LCO/TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO/WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries.",
                "Algorithm NDCG Signific.",
                "NDCG Signific.",
                "Clear vs. Google Ambiguous vs. Google Google 0.81 - 0.46TF 0.76 - 0.54 p = 0.03 LC[O] 0.77 - 0.59 p 0.01 WN[SYN] 0.79 - 0.44A[LCO/TF] 0.81 - 0.64 p 0.01 A[LCO/WN] 0.81 - 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries. both adaptive algorithms, A[LCO/TF] and A[LCO/WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01.",
                "They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07).",
                "For randomly selected queries, even though A[LCO/TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms.",
                "The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity.",
                "Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level.",
                "The analysis of the self-selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5).",
                "For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO/TF] and by 35.2% through A[LCO/WN], both strongly significant with p 0.01.",
                "Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05).",
                "Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively.",
                "All results are depicted graphically in Figure 1.",
                "We notice that A[LCO/TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self-selected.",
                "The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. 5.",
                "CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the users Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to users interests, personalizing the search output.",
                "In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents.",
                "Each of them produces additional query keywords by analyzing users Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri.",
                "Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category. • We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios.",
                "We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28%. • We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. • Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach.",
                "We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms.",
                "We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data.",
                "Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. 6.",
                "ACKNOWLEDGEMENTS We thank Ricardo Baeza-Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented.",
                "We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log.",
                "Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no. 027705). 7.",
                "REFERENCES [1] J. Allan and H. Raghavan.",
                "Using part-of-speech patterns to reduce query ambiguity.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [2] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: Terminological feedback for iterative information seeking.",
                "In Proc. of the 22nd Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1999. [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer.",
                "Automatic query wefinement using lexical affinities with maximal information gain.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 283-290, 2002. [4] C. Carpineto, R. de Mori, G. Romano, and B. Bigi.",
                "An information-theoretic approach to automatic query expansion.",
                "ACM TOIS, 19(1):1-27, 2001. [5] C.-H. Chang and C.-C. Hsu.",
                "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval.",
                "In Proc. of the 7th Intl.",
                "Conf. on World Wide Web, 1998. [6] P. A. Chirita, C. Firan, and W. Nejdl.",
                "Summarizing local context to personalize global web search.",
                "In Proc. of the 15th Intl.",
                "CIKM Conf. on Information and Knowledge Management, 2006. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proc. of the 25th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2002. [8] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proc. of the 11th Intl.",
                "Conf. on World Wide Web, 2002. [9] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19:61-74, 1993. [10] H. P. Edmundson.",
                "New methods in automatic extracting.",
                "Journal of the ACM, 16(2):264-285, 1969. [11] E. N. Efthimiadis.",
                "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion.",
                "Information Processing and Management, 31(4):605-620, 1995. [12] D. Fogaras and B. Racz.",
                "Scaling link based similarity search.",
                "In Proc. of the 14th Intl.",
                "World Wide Web Conf., 2005. [13] T. Haveliwala.",
                "Topic-sensitive pagerank.",
                "In Proc. of the 11th Intl.",
                "World Wide Web Conf., Honolulu, Hawaii, May 2002. [14] B.",
                "He and I. Ounis.",
                "Inferring query performance using pre-retrieval predictors.",
                "In Proc. of the 11th Intl.",
                "SPIRE Conf. on String Processing and Information Retrieval, 2004. [15] K. J¨arvelin and J. Keklinen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proc. of the 23th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2000. [16] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proc. of the 12th Intl.",
                "World Wide Web Conference, 2003. [17] M.-C. Kim and K.-S. Choi.",
                "A comparison of collocation-based similarity measures in query expansion.",
                "Inf.",
                "Proc. and Mgmt., 35(1):19-30, 1999. [18] S.-B.",
                "Kim, H.-C. Seo, and H.-C. Rim.",
                "Information retrieval using word senses: root sense tagging approach.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [19] R. Kraft and J. Zien.",
                "Mining anchor text for query refinement.",
                "In Proc. of the 13th Intl.",
                "Conf. on World Wide Web, 2004. [20] R. Krovetz and W. B. Croft.",
                "Lexical ambiguity and information retrieval.",
                "ACM Trans.",
                "Inf.",
                "Syst., 10(2), 1992. [21] A. M. Lam-Adesina and G. J. F. Jones.",
                "Applying summarization techniques for term selection in relevance feedback.",
                "In Proc. of the 24th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2001. [22] S. Liu, F. Liu, C. Yu, and W. Meng.",
                "An effective approach to document retrieval via utilizing wordnet and recognizing phrases.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, 2004. [23] G. Miller.",
                "Wordnet: An electronic lexical database.",
                "Communications of the ACM, 38(11):39-41, 1995. [24] L. Nie, B. Davison, and X. Qi.",
                "Topical link analysis for web search.",
                "In Proc. of the 29th Intl.",
                "ACM SIGIR Conf. on Res. and Development in Inf.",
                "Retr., 2006. [25] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The PageRank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Univ., 1998. [26] F. Qiu and J. Cho.",
                "Automatic indentification of user interest for personalized search.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [27] Y. Qiu and H.-P. Frei.",
                "Concept based query expansion.",
                "In Proc. of the 16th Intl.",
                "ACM SIGIR Conf. on Research and Development in Inf.",
                "Retr., 1993. [28] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313-323, 1971. [29] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proc. of the 26th Intl.",
                "ACM SIGIR Conf., 2003. [30] T. Sarlos, A.",
                "A. Benczur, K. Csalogany, D. Fogaras, and B. Racz.",
                "To randomize or not to randomize: Space optimal summaries for hyperlink analysis.",
                "In Proc. of the 15th Intl.",
                "WWW Conf., 2006. [31] C. Shah and W. B. Croft.",
                "Evaluating high accuracy retrieval techniques.",
                "In Proc. of the 27th Intl.",
                "ACM SIGIR Conf. on Research and development in information retrieval, pages 2-9, 2004. [32] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proc. of the 13th Intl.",
                "World Wide Web Conf., 2004. [33] D. Sullivan.",
                "The older you are, the more you want personalized search, 2004. http://searchenginewatch.com/searchday/article.php/3385131. [34] J. Teevan, S. Dumais, and E. Horvitz.",
                "Personalizing search via automated analysis of interests and activities.",
                "In Proc. of the 28th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005. [35] E. Volokh.",
                "Personalization and privacy.",
                "Commun.",
                "ACM, 43(8), 2000. [36] E. M. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In Proc. of the 17th Intl.",
                "ACM SIGIR Conf. on Res. and development in Inf.",
                "Retr., 1994. [37] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proc. of the 19th Intl.",
                "ACM SIGIR Conf. on Research and Development in Information Retrieval, 1996. [38] S. Yu, D. Cai, J.-R. Wen, and W.-Y.",
                "Ma.",
                "Improving pseudo-relevance feedback in web information retrieval using web page segmentation.",
                "In Proc. of the 12th Intl.",
                "Conf. on World Wide Web, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        }
    }
}