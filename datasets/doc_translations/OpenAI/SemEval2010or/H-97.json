{
    "id": "H-97",
    "original_text": "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute. Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination. Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action. Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification. However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1. INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage. This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m. The current schedule looks like this: + 9:30 a.m. Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m. Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m. Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance. As a result, I will need each of your parts by Wednesday. Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action. Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request. Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent. We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions. The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request. Action-item detection differs from standard text classification in two important ways. First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body. In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22]. Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below. Instead we find that we need more information-laden features such as higher-order n-grams. Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17]. In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14]. Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20]. We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice. We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification. Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level. From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem. We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task. Finally, we summarize this papers contributions and consider interesting directions for future work. 2. RELATED WORK Several other researchers have considered very similar text classification tasks. Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts. We consider action-items to be an important specific type of speech act that falls within their more general classification. While they provide results for several classification methods, their methods only make use of human judgments at the document-level. In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest. Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List. This classification task is very similar to ours except they do not consider simple factual questions to belong to this category. We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?. From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer. Additionally, they do not study alternative choices or approaches to the classification task. Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list. In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems. Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature. For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail. In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3. PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments. Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification. Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items. Therefore, there are three basic problems: 1. Document detection: Classify a document as to whether or not it contains an action-item. 2. Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3. Sentence detection: Classify each sentence in a document as to whether or not it is an action-item. As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application. In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users. In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail. This can be the case for crisis managers during disaster management. Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest. We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries. Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes. To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data. The document-level view treats each e-mail as a learning instance with an associated class-label. Then, the document can be converted to a feature-value vector and learning progresses as usual. Applying a document-level classifier to document detection and ranking is straightforward. In order to apply it to sentence detection, one must make additional steps. For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated. The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences. In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label. Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance. Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction. This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you. Each of these phrases consists of common words that occur in many e-mails. However, when they occur in the same sentence, they are far more indicative of an action-item. Additionally, order can be important: consider have you versus you have. Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification. Therefore, we consider all n-grams up to size 4. When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it. We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall. Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting. Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable. Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence. Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation. In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document. This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level. We do not address how to use a document-level classifier to make predictions at the sentence-level. In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4]. Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item. When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth. Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods. If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive. The metric would need to punish overly long true predictions as well as too short predictions. Our criteria for converting to labeled instances implicitly includes both criteria. Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item. Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item. Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no. Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score. We use the simple policy of predicting positive when any of the sentences is predicted positive. In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document. In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold. When no sentence is predicted positive, the document score is the maximum sentence score normalized by length. As in other text problems, we are more likely to emit false positives for documents with more words or sentences. Thus we include a length normalization factor. 4. EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements. The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages. After identity anonymization, the corpora has three basic versions. Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail. Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant. To isolate the effects of quoted material, we have three versions of the corpora. The raw form contains the basic messages. The auto-stripped version contains the messages after quoted material has been automatically removed. The hand-stripped version contains the messages after quoted material has been removed by a human. Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message. The studies reported here are performed with the hand-stripped version. This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it. Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation. Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item. In addition, they identified each segment of the e-mail which contained an action-item. A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence. They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request. Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items. Annotator Two labeled 327 messages as containing action items. The agreement of the human annotators is shown in Tables 1 and 2. The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same. At the document-level, the annotators agreed 93% of the time. The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement. R is the empirical estimate of the probability of random agreement given the empirical class priors. A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected. At the document-level, the kappa statistic for inter-annotator agreement is 0.85. This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6]. In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences. This allows us to compare agreement over no judgments. We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc. Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well. This only reduces the number of no agreements. This leaves 6301 automatically segmented sentences. At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82. In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion. The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements. For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not. The first would be an action-item in most contexts while the second would not. Of course, many conditional statements are not so clearly interpretable. After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems. Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments. Two messages have four action-item segments, and one message has six action-item segments. Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two. In terms of message characteristics, there were on average 132 content tokens in the body after stripping. For action-item messages, there were 115. However, by examining Figure 2 we see the length distributions are nearly identical. As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms. In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager. We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand. This is important since it is easy to improve a strawman classifier by introducing a new representation. By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19]. We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it. In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set. The value of k is set to be 2( log2 N + 1) where N is the number of training points. This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8]. In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16]. In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length. A bin size of 20 words was used. Only tokens in the body after hand-stripping were counted. After stripping, the majority of words left are usually actual message content. Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics. The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11]. All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method. We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm. The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct. With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron. The output of the classifier uses the weights on the perceptra to make a final voted classification. When used in an offline-manner, multiple passes can be made through the training data. Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier. Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10]. Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin. Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy. The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents. For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold. For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05. Feature selection was performed using the chi-squared statistic. Different levels of feature selection were considered for each classifier. Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. There are approximately 4700 unigram tokens without feature selection. In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier. For this study, only the body of each e-mail message was used. Feature selection is always applied to all candidate features. That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3. The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram). Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes. This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4). Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting. In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes. More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well. As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small. This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams. Further improvement would signify that the order of the words matter even when only considering a small sentence-size window. Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes. Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers. When the F1 result is statistically significant, it is shown in bold. When the accuracy result is significant, it is shown with a † . F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem. When the result is statistically significant, it is shown in bold. Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve. In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall. This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox. Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score. Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance. Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here. This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance. The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier. Sentence detection results are presented in Table 6. With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem. That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare. Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no. Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification. Figure 4: Users find action-items quicker when assisted by a classification system. Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user. In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used. There were three distinct sets of e-mail in which users had to find action-items. These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help). In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering. Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets. This is typically handled by varying the ordering of the sets across users so that the means are comparable. While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects. Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes. Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting. As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection. For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss. Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know. Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase. Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document. As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document. Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items. A few examples are terms such as org, bob, and gov. We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions. This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on. We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5. FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers. Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis. Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task. We are currently pursuing some of these avenues to see what additional gains these offer. Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers. Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier. Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6. SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value. Further experiments are needed to see how this interacts with the amount of training data available. Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items. This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification. In this work, we examined how action-items can be effectively detected in e-mails. Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments. When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches. Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. NBCHD030010. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC). We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments. We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package. Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7. REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang. Topic detection and tracking pilot study: Final report. In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss. Automated learning of decision rules for text categorization. ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll. High precision extraction of grammatical relations. In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell. Learning to classify email into speech acts. In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell. Task-focused summarization of email. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum. Extracting social networks and contact information from email and the web. In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami. Inductive learning algorithms and representations for text categorization. In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277-296, 1999. [11] T. Joachims. Making large-scale svm learning practical. In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56. MIT Press, 1999. [12] L. S. Larkey. A patent search and classification system. In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis. An evaluation of phrasal and clustered representations on a text categorization task. In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin. A pairwise ensemble approach for accurate genre classification. In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell. A comparison study of kernels for multi-label text classification using category association. In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam. A comparison of event models for naive bayes text classification. In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998. TR WS-98-05. [17] F. Sebastiani. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen. Information Retrieval. Butterworths, London, 1979. [19] Y. Yang. An evaluation of statistical approaches to text categorization. Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu. Learning approaches to topic detection and tracking. IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu. A re-examination of text categorization methods. In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. Topic-conditioned novelty detection. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002.",
    "original_translation": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación. En la zona de alta precisión del gráfico, el borde consistente del clasificador a nivel de oraciones es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1. Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento. La precisión F1 de los clasificadores de nivel de oración en la detección de oraciones es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí. Esto refuerza aún más la evidencia de que un clasificador alternativo con la representación de bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador Voted Perceptron mejora cuando se aumenta el número de iteraciones de entrenamiento, pero sigue siendo inferior al clasificador SVM. Los resultados de detección de oraciones se presentan en la Tabla 6. En cuanto al problema de detección de oraciones, observamos que la medida F1 proporciona una mejor idea del espacio restante para mejorar en este problema difícil. Es decir, a diferencia de la detección de documentos donde los documentos de elementos de acción son bastante comunes, las frases de elementos de acción son muy raras. Por lo tanto, al igual que en otros problemas de texto, los números de precisión son engañosamente altos simplemente debido a la precisión predeterminada alcanzable al predecir siempre negativo. Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y el ranking. Figura 4: Los usuarios encuentran los elementos de acción más rápidamente cuando son asistidos por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso construido para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventana de tres oraciones. Había tres conjuntos distintos de correos electrónicos en los que los usuarios tenían que encontrar elementos de acción. Estos conjuntos fueron presentados en un orden aleatorio (Desordenado), ordenados por el clasificador (Ordenado), o ordenados por el clasificador y con la precisión de 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recuperación de Acciones de Detección de Elementos SVM Rendimiento (Post Selección de Modelo) Unigrama de Documento N-grama de Oración Figura 3: Tanto los n-gramas como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar. la oración central en la ventana de mayor confianza resaltada (Orden+ayuda). Para realizar comparaciones justas entre condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual; es decir, la carga cognitiva de lectura debe ser aproximadamente la misma antes de reordenar los clasificadores. Además, los usuarios suelen mostrar efectos de práctica al mejorar en la tarea general y, por lo tanto, desempeñarse mejor en los conjuntos de mensajes posteriores. Esto suele ser manejado variando el orden de los conjuntos entre usuarios para que las medias sean comparables. Sin entrar en más detalles, señalamos que los conjuntos se equilibraron en cuanto al número total de fichas y se utilizó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que en intervalos de 5, 10 y 15 minutos, los usuarios encontraron consistentemente significativamente más elementos de acción cuando fueron asistidos por el clasificador, pero fueron ayudados de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda consistentemente a los usuarios, no obtuvimos un impacto adicional en los usuarios finales al resaltar. Como se mencionó anteriormente, esto podría ser resultado del amplio margen de mejora que aún existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, resaltar la oración incorrecta cerca de una acción real perjudica la confianza de los usuarios, pero si un indicador vago (por ejemplo, una flecha) señala el área aproximada de la que el usuario no está al tanto, se evita el error por poco. Dado que el usuario estudia utilizó una ventana de tres oraciones, creemos que esto también jugó un papel en la precisión de la detección de oraciones. 4.6 Discusión En contraste con problemas donde los n-gramas han mostrado poca diferencia, creemos que su poder aquí se deriva del hecho de que muchos de los n-gramas significativos para las acciones consisten en palabras comunes, por ejemplo, házmelo saber. Por lo tanto, el enfoque de unigrama a nivel de documento no puede obtener mucha ventaja, incluso al modelar correctamente su probabilidad conjunta, ya que estas palabras a menudo co-ocurrirán en el documento pero no necesariamente en una frase. Además, la detección de elementos de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden depender de la agregación de evidencia de un gran número de indicadores débiles en todo el documento. A pesar de haber descartado la información del encabezado, al examinar las características mejor clasificadas a nivel de documento, se revela que muchas de las características son nombres o partes de direcciones de correo electrónico que aparecieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden a contener muchos o ningún elemento de acción. Algunos ejemplos son términos como org, bob y gov. Observamos que estas características serán sensibles a la distribución particular (emisores/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieran menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá del modelo de bolsa de palabras puede ser la metodología, y al investigar propiedades como las curvas de aprendizaje y qué tan bien un modelo se transfiere, se pueden resaltar diferencias en modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones en las que fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin necesidad de volver a entrenarlos. 5. TRABAJO FUTURO Si bien la aplicación de clasificadores de texto a nivel de documento está bastante bien entendida, existe el potencial de aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Tales métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas a tfidf, que pueden no ser apropiadas dado que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis frásico. Además, el etiquetado de entidades nombradas, las expresiones de tiempo, etc., parecen ser candidatos probables para características que pueden mejorar aún más esta tarea. Actualmente estamos explorando algunas de estas vías para ver qué beneficios adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el clasificador a nivel de oración. Un modelo así podría servir como un ejemplo general para otros problemas donde el enfoque de bolsa de palabras puede establecer un modelo base, pero se necesitan enfoques más complejos para lograr un rendimiento más allá de ese modelo base. RESUMEN Y CONCLUSIONES La efectividad de la detección a nivel de oración argumenta que etiquetar a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo esto interactúa con la cantidad de datos de entrenamiento disponibles. La detección de oraciones que luego se aglomera a nivel de documento funciona sorprendentemente mejor con un bajo índice de recuperación de lo que se esperaría con elementos a nivel de oración. Esto, a su vez, indica que métodos mejorados de segmentación de oraciones podrían generar más mejoras en la clasificación. En este trabajo, examinamos cómo se pueden detectar de manera efectiva los elementos de acción en correos electrónicos. Nuestro análisis empírico ha demostrado que los n-gramos son de vital importancia para aprovechar al máximo las evaluaciones a nivel de documento. Cuando están disponibles juicios más detallados, entonces un enfoque estándar de bolsa de palabras utilizando un tamaño de ventana pequeño (de oración) y técnicas de segmentación automática puede producir resultados casi tan buenos como los enfoques basados en n-gramos. Agradecimientos: Este material se basa en trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autor(es) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) o el Centro Nacional de Negocios del Departamento del Interior (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recolección de datos fueron esenciales para la construcción del corpus, y tanto a Jill como a Aaron Steinfeld por su dirección de los experimentos de HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de corpus y a Curtis Huttenhower por su apoyo al paquete de preprocesamiento de texto. Finalmente, agradecemos sinceramente a Scott Fahlman por su apoyo y las útiles discusiones sobre este tema. 7. REFERENCIAS [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron y Y. Yang. Estudio piloto de detección y seguimiento de temas: Informe final. En Actas del Taller de Transcripción y Comprensión de Noticias de Radiodifusión de DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automatizado de reglas de decisión para la categorización de texto. ACM Transactions on Information Systems, 12(3):233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo en tareas de clasificación: La estadística kappa. Lingüística Computacional, 22(2):249-254, 1996. [4] J. Carroll. Extracción de relaciones gramaticales de alta precisión. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (COLING), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprendiendo a clasificar correos electrónicos en actos de habla. En EMNLP-2004 (Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen centrado en la tarea del correo electrónico. En \"La ramificación de la síntesis de texto: Actas del taller ACL-04\", páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extrayendo redes sociales e información de contacto de correos electrónicos y la web. En CEAS-2004 (Conferencia sobre Correo Electrónico y Anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, Actas de la 7ª Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [11] T. Joachims. Haciendo que el aprendizaje svm a gran escala sea práctico. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en Métodos de Núcleo - Aprendizaje de Vectores de Soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. \n\nMIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia de Bibliotecas Digitales de la ACM, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto. En SIGIR 92, Actas de la 15ª Conferencia Anual Internacional de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin. Un enfoque de conjunto por pares para una clasificación precisa de géneros. En Actas de la Conferencia Europea sobre Aprendizaje Automático (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell. Un estudio comparativo de núcleos para la clasificación de texto multi-etiqueta utilizando asociación de categorías. En la Vigésimo-primera Conferencia Internacional sobre Aprendizaje Automático (ICML), 2004. [16] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En Notas de Trabajo de AAAI 98 (La 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre Aprendizaje para la Categorización de Textos, páginas 41-48, 1998. TR WS-98-05. [17] F. Sebastiani. \n\nTR WS-98-05. [17] F. Sebastiani. Aprendizaje automático en la categorización automatizada de textos. ACM Computing Surveys, 34(1):1-47, marzo de 2002. [18] C. J. van Rijsbergen. Recuperación de información. Butterworths, Londres, 1979. [19] Y. Yang. Una evaluación de enfoques estadísticos para la categorización de texto. Recuperación de información, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu. Enfoques de aprendizaje para la detección y seguimiento de temas. IEEE EXPERT, Número Especial sobre Aplicaciones de la Recuperación de Información Inteligente, 1999. [21] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de textos. En SIGIR 99, Actas de la 22ª Conferencia Internacional Anual de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada por el tema. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, julio de 2002.",
    "original_sentences": [
        "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
        "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
        "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
        "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
        "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
        "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
        "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
        "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
        "The current schedule looks like this: + 9:30 a.m.",
        "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
        "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
        "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
        "As a result, I will need each of your parts by Wednesday.",
        "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
        "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
        "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
        "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
        "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
        "Action-item detection differs from standard text classification in two important ways.",
        "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
        "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
        "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
        "Instead we find that we need more information-laden features such as higher-order n-grams.",
        "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
        "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
        "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
        "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
        "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
        "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
        "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
        "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
        "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
        "RELATED WORK Several other researchers have considered very similar text classification tasks.",
        "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
        "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
        "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
        "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
        "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
        "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
        "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
        "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
        "Additionally, they do not study alternative choices or approaches to the classification task.",
        "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
        "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
        "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
        "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
        "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
        "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
        "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
        "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
        "Therefore, there are three basic problems: 1.",
        "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
        "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
        "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
        "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
        "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
        "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
        "This can be the case for crisis managers during disaster management.",
        "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
        "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
        "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
        "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
        "The document-level view treats each e-mail as a learning instance with an associated class-label.",
        "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
        "Applying a document-level classifier to document detection and ranking is straightforward.",
        "In order to apply it to sentence detection, one must make additional steps.",
        "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
        "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
        "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
        "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
        "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
        "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
        "Each of these phrases consists of common words that occur in many e-mails.",
        "However, when they occur in the same sentence, they are far more indicative of an action-item.",
        "Additionally, order can be important: consider have you versus you have.",
        "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
        "Therefore, we consider all n-grams up to size 4.",
        "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
        "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
        "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
        "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
        "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
        "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
        "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
        "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
        "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
        "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
        "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
        "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
        "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
        "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
        "The metric would need to punish overly long true predictions as well as too short predictions.",
        "Our criteria for converting to labeled instances implicitly includes both criteria.",
        "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
        "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
        "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
        "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
        "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
        "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
        "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
        "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
        "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
        "Thus we include a length normalization factor. 4.",
        "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
        "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
        "After identity anonymization, the corpora has three basic versions.",
        "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
        "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
        "To isolate the effects of quoted material, we have three versions of the corpora.",
        "The raw form contains the basic messages.",
        "The auto-stripped version contains the messages after quoted material has been automatically removed.",
        "The hand-stripped version contains the messages after quoted material has been removed by a human.",
        "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
        "The studies reported here are performed with the hand-stripped version.",
        "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
        "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
        "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
        "In addition, they identified each segment of the e-mail which contained an action-item.",
        "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
        "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
        "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
        "Annotator Two labeled 327 messages as containing action items.",
        "The agreement of the human annotators is shown in Tables 1 and 2.",
        "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
        "At the document-level, the annotators agreed 93% of the time.",
        "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
        "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
        "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
        "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
        "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
        "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
        "This allows us to compare agreement over no judgments.",
        "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
        "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
        "This only reduces the number of no agreements.",
        "This leaves 6301 automatically segmented sentences.",
        "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
        "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
        "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
        "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
        "The first would be an action-item in most contexts while the second would not.",
        "Of course, many conditional statements are not so clearly interpretable.",
        "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
        "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
        "Two messages have four action-item segments, and one message has six action-item segments.",
        "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
        "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
        "For action-item messages, there were 115.",
        "However, by examining Figure 2 we see the length distributions are nearly identical.",
        "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
        "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
        "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
        "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
        "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
        "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
        "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
        "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
        "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
        "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
        "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
        "A bin size of 20 words was used.",
        "Only tokens in the body after hand-stripping were counted.",
        "After stripping, the majority of words left are usually actual message content.",
        "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
        "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
        "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
        "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
        "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
        "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
        "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
        "When used in an offline-manner, multiple passes can be made through the training data.",
        "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
        "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
        "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
        "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
        "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
        "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
        "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
        "Feature selection was performed using the chi-squared statistic.",
        "Different levels of feature selection were considered for each classifier.",
        "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
        "There are approximately 4700 unigram tokens without feature selection.",
        "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
        "For this study, only the body of each e-mail message was used.",
        "Feature selection is always applied to all candidate features.",
        "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
        "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
        "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
        "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
        "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
        "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
        "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
        "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
        "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
        "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
        "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
        "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
        "When the F1 result is statistically significant, it is shown in bold.",
        "When the accuracy result is significant, it is shown with a † .",
        "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
        "When the result is statistically significant, it is shown in bold.",
        "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
        "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
        "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
        "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
        "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
        "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
        "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
        "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
        "Sentence detection results are presented in Table 6.",
        "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
        "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
        "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
        "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
        "Figure 4: Users find action-items quicker when assisted by a classification system.",
        "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
        "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
        "There were three distinct sets of e-mail in which users had to find action-items.",
        "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
        "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
        "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
        "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
        "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
        "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
        "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
        "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
        "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
        "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
        "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
        "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
        "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
        "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
        "A few examples are terms such as org, bob, and gov.",
        "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
        "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
        "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
        "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
        "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
        "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
        "We are currently pursuing some of these avenues to see what additional gains these offer.",
        "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
        "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
        "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
        "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
        "Further experiments are needed to see how this interacts with the amount of training data available.",
        "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
        "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
        "In this work, we examined how action-items can be effectively detected in e-mails.",
        "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
        "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
        "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
        "NBCHD030010.",
        "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
        "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
        "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
        "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
        "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
        "Topic detection and tracking pilot study: Final report.",
        "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
        "Automated learning of decision rules for text categorization.",
        "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
        "Assessing agreement on classification tasks: The kappa statistic.",
        "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
        "High precision extraction of grammatical relations.",
        "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
        "Learning to classify email into speech acts.",
        "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
        "Task-focused summarization of email.",
        "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
        "Extracting social networks and contact information from email and the web.",
        "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
        "A Probabilistic Theory of Pattern Recognition.",
        "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
        "Inductive learning algorithms and representations for text categorization.",
        "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
        "Large margin classification using the perceptron algorithm.",
        "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
        "Making large-scale svm learning practical.",
        "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
        "MIT Press, 1999. [12] L. S. Larkey.",
        "A patent search and classification system.",
        "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
        "An evaluation of phrasal and clustered representations on a text categorization task.",
        "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
        "A pairwise ensemble approach for accurate genre classification.",
        "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
        "A comparison study of kernels for multi-label text classification using category association.",
        "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
        "A comparison of event models for naive bayes text classification.",
        "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
        "TR WS-98-05. [17] F. Sebastiani.",
        "Machine learning in automated text categorization.",
        "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
        "Information Retrieval.",
        "Butterworths, London, 1979. [19] Y. Yang.",
        "An evaluation of statistical approaches to text categorization.",
        "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
        "Learning approaches to topic detection and tracking.",
        "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
        "A re-examination of text categorization methods.",
        "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
        "Topic-conditioned novelty detection.",
        "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
    ],
    "translated_text_sentences": [
        "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje.",
        "Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información.",
        "Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción.",
        "A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras.",
        "Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1.",
        "Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico.",
        "Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m.",
        "El horario actual se ve así: + 9:30 a.m.",
        "Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m.",
        "Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m.",
        "Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación.",
        "Como resultado, necesitaré cada una de sus partes para el miércoles.",
        "¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios.",
        "La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción.",
        "Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente.",
        "Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones.",
        "La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción.",
        "La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes.",
        "Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico.",
        "Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22].",
        "Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación.",
        "En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior.",
        "La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17].",
        "De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14].",
        "La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20].",
        "Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes.",
        "Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla.",
        "Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento.",
        "A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento.",
        "Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea.",
        "Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros.",
        "TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares.",
        "Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla.",
        "Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general.",
        "Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento.",
        "Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés.",
        "Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes.",
        "Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría.",
        "Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás?",
        "Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados.",
        "Además, no estudian opciones o enfoques alternativos para la tarea de clasificación.",
        "En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas.",
        "En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas.",
        "El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente.",
        "Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos.",
        "En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3.",
        "DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso.",
        "Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación.",
        "Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes.",
        "Por lo tanto, hay tres problemas básicos: 1.",
        "Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2.",
        "Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3.",
        "Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no.",
        "Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación.",
        "En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios.",
        "Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo.",
        "Esto puede ser el caso para los gestores de crisis durante la gestión de desastres.",
        "Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés.",
        "Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos.",
        "Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí.",
        "Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados.",
        "La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada.",
        "Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre.",
        "Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo.",
        "Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales.",
        "Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción.",
        "El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas.",
        "En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada.",
        "Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje.",
        "Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento.",
        "Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes?",
        "Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos.",
        "Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar.",
        "Además, el orden puede ser importante: considera tienes tú versus tú tienes.",
        "Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas.",
        "Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4.",
        "Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él.",
        "Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación.",
        "Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo.",
        "Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable.",
        "Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración.",
        "Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada.",
        "Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento.",
        "Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento.",
        "No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración.",
        "Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4].",
        "Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción.",
        "Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta.",
        "Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos.",
        "Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas.",
        "La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas.",
        "Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios.",
        "Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción.",
        "Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción.",
        "Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no.",
        "Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento.",
        "Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva.",
        "Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento.",
        "En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral.",
        "Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud.",
        "Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones.",
        "Así que incluimos un factor de normalización de longitud. 4.",
        "ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas.",
        "Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico.",
        "Después de la anonimización de la identidad, el corpus tiene tres versiones básicas.",
        "El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo.",
        "El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes.",
        "Para aislar los efectos del material citado, tenemos tres versiones del corpus.",
        "La forma cruda contiene los mensajes básicos.",
        "La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente.",
        "La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano.",
        "Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje.",
        "Los estudios reportados aquí se realizaron con la versión despojada a mano.",
        "Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen.",
        "Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa.",
        "Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción.",
        "Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente.",
        "Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración.",
        "Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud.",
        "El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción.",
        "El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción.",
        "El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2.",
        "Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos.",
        "A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo.",
        "El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo.",
        "R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase.",
        "Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar.",
        "A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85.",
        "Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6].",
        "Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones.",
        "Esto nos permite comparar el acuerdo sin juicios.",
        "Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc.",
        "Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje.",
        "Esto solo reduce la cantidad de desacuerdos.",
        "Esto deja 6301 oraciones segmentadas automáticamente.",
        "A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82.",
        "Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso.",
        "Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales.",
        "Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace.",
        "El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería.",
        "Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables.",
        "Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción.",
        "De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción.",
        "Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción.",
        "Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos.",
        "En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos.",
        "Para los mensajes de acciones pendientes, hubo 115.",
        "Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas.",
        "Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto.",
        "Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso.",
        "Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión.",
        "Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación.",
        "Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19].",
        "Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral.",
        "Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento.",
        "El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento.",
        "Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8].",
        "En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16].",
        "Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes.",
        "Se utilizó un tamaño de contenedor de 20 palabras.",
        "Solo se contaron los tokens en el cuerpo después de la extracción manual.",
        "Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje.",
        "Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva.",
        "El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11].",
        "Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos.",
        "Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2.",
        "El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto.",
        "Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón.",
        "La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada.",
        "Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento.",
        "Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal.",
        "Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10].",
        "Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande.",
        "Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión.",
        "El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos.",
        "Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue.",
        "Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05.",
        "La selección de características se realizó utilizando la estadística de chi-cuadrado.",
        "Se consideraron diferentes niveles de selección de características para cada clasificador.",
        "Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
        "Hay aproximadamente 4700 tokens unigram sin selección de características.",
        "Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador.",
        "Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico.",
        "La selección de características siempre se aplica a todas las características candidatas.",
        "Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3.",
        "La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento).",
        "Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes.",
        "Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4).",
        "El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble.",
        "En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes.",
        "Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento.",
        "Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña.",
        "Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos.",
        "Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña.",
        "Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos.",
        "Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración.",
        "Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita.",
        "Cuando el resultado de precisión es significativo, se muestra con un †.",
        "Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos.",
        "Cuando el resultado es estadísticamente significativo, se muestra en negrita.",
        "Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación.",
        "En la zona de alta precisión del gráfico, el borde consistente del clasificador a nivel de oraciones es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1.",
        "Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados.",
        "Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1.",
        "Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento.",
        "La precisión F1 de los clasificadores de nivel de oración en la detección de oraciones es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí.",
        "Esto refuerza aún más la evidencia de que un clasificador alternativo con la representación de bolsa de palabras no podría alcanzar el mismo nivel de rendimiento.",
        "El clasificador Voted Perceptron mejora cuando se aumenta el número de iteraciones de entrenamiento, pero sigue siendo inferior al clasificador SVM.",
        "Los resultados de detección de oraciones se presentan en la Tabla 6.",
        "En cuanto al problema de detección de oraciones, observamos que la medida F1 proporciona una mejor idea del espacio restante para mejorar en este problema difícil.",
        "Es decir, a diferencia de la detección de documentos donde los documentos de elementos de acción son bastante comunes, las frases de elementos de acción son muy raras.",
        "Por lo tanto, al igual que en otros problemas de texto, los números de precisión son engañosamente altos simplemente debido a la precisión predeterminada alcanzable al predecir siempre negativo.",
        "Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y el ranking.",
        "Figura 4: Los usuarios encuentran los elementos de acción más rápidamente cuando son asistidos por un sistema de clasificación.",
        "Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso construido para la tarea puede tener un impacto en el usuario final.",
        "Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventana de tres oraciones.",
        "Había tres conjuntos distintos de correos electrónicos en los que los usuarios tenían que encontrar elementos de acción.",
        "Estos conjuntos fueron presentados en un orden aleatorio (Desordenado), ordenados por el clasificador (Ordenado), o ordenados por el clasificador y con la precisión de 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recuperación de Acciones de Detección de Elementos SVM Rendimiento (Post Selección de Modelo) Unigrama de Documento N-grama de Oración Figura 3: Tanto los n-gramas como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar. la oración central en la ventana de mayor confianza resaltada (Orden+ayuda).",
        "Para realizar comparaciones justas entre condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual; es decir, la carga cognitiva de lectura debe ser aproximadamente la misma antes de reordenar los clasificadores.",
        "Además, los usuarios suelen mostrar efectos de práctica al mejorar en la tarea general y, por lo tanto, desempeñarse mejor en los conjuntos de mensajes posteriores.",
        "Esto suele ser manejado variando el orden de los conjuntos entre usuarios para que las medias sean comparables.",
        "Sin entrar en más detalles, señalamos que los conjuntos se equilibraron en cuanto al número total de fichas y se utilizó un diseño de cuadrado latino para equilibrar los efectos de práctica.",
        "La Figura 4 muestra que en intervalos de 5, 10 y 15 minutos, los usuarios encontraron consistentemente significativamente más elementos de acción cuando fueron asistidos por el clasificador, pero fueron ayudados de manera más crítica en los primeros cinco minutos.",
        "Aunque el clasificador ayuda consistentemente a los usuarios, no obtuvimos un impacto adicional en los usuarios finales al resaltar.",
        "Como se mencionó anteriormente, esto podría ser resultado del amplio margen de mejora que aún existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones.",
        "Por ejemplo, resaltar la oración incorrecta cerca de una acción real perjudica la confianza de los usuarios, pero si un indicador vago (por ejemplo, una flecha) señala el área aproximada de la que el usuario no está al tanto, se evita el error por poco.",
        "Dado que el usuario estudia utilizó una ventana de tres oraciones, creemos que esto también jugó un papel en la precisión de la detección de oraciones. 4.6 Discusión En contraste con problemas donde los n-gramas han mostrado poca diferencia, creemos que su poder aquí se deriva del hecho de que muchos de los n-gramas significativos para las acciones consisten en palabras comunes, por ejemplo, házmelo saber.",
        "Por lo tanto, el enfoque de unigrama a nivel de documento no puede obtener mucha ventaja, incluso al modelar correctamente su probabilidad conjunta, ya que estas palabras a menudo co-ocurrirán en el documento pero no necesariamente en una frase.",
        "Además, la detección de elementos de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento.",
        "Como resultado, los buenos clasificadores no pueden depender de la agregación de evidencia de un gran número de indicadores débiles en todo el documento.",
        "A pesar de haber descartado la información del encabezado, al examinar las características mejor clasificadas a nivel de documento, se revela que muchas de las características son nombres o partes de direcciones de correo electrónico que aparecieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden a contener muchos o ningún elemento de acción.",
        "Algunos ejemplos son términos como org, bob y gov.",
        "Observamos que estas características serán sensibles a la distribución particular (emisores/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieran menos fácilmente a contextos y usuarios alternativos en diferentes instituciones.",
        "Esto señala que parte del problema de ir más allá del modelo de bolsa de palabras puede ser la metodología, y al investigar propiedades como las curvas de aprendizaje y qué tan bien un modelo se transfiere, se pueden resaltar diferencias en modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones en las que fueron entrenados.",
        "Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin necesidad de volver a entrenarlos. 5.",
        "TRABAJO FUTURO Si bien la aplicación de clasificadores de texto a nivel de documento está bastante bien entendida, existe el potencial de aumentar significativamente el rendimiento de los clasificadores a nivel de oración.",
        "Tales métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas a tfidf, que pueden no ser apropiadas dado que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis frásico.",
        "Además, el etiquetado de entidades nombradas, las expresiones de tiempo, etc., parecen ser candidatos probables para características que pueden mejorar aún más esta tarea.",
        "Actualmente estamos explorando algunas de estas vías para ver qué beneficios adicionales ofrecen.",
        "Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración.",
        "Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el clasificador a nivel de oración.",
        "Un modelo así podría servir como un ejemplo general para otros problemas donde el enfoque de bolsa de palabras puede establecer un modelo base, pero se necesitan enfoques más complejos para lograr un rendimiento más allá de ese modelo base.",
        "RESUMEN Y CONCLUSIONES La efectividad de la detección a nivel de oración argumenta que etiquetar a nivel de oración proporciona un valor significativo.",
        "Se necesitan más experimentos para ver cómo esto interactúa con la cantidad de datos de entrenamiento disponibles.",
        "La detección de oraciones que luego se aglomera a nivel de documento funciona sorprendentemente mejor con un bajo índice de recuperación de lo que se esperaría con elementos a nivel de oración.",
        "Esto, a su vez, indica que métodos mejorados de segmentación de oraciones podrían generar más mejoras en la clasificación.",
        "En este trabajo, examinamos cómo se pueden detectar de manera efectiva los elementos de acción en correos electrónicos.",
        "Nuestro análisis empírico ha demostrado que los n-gramos son de vital importancia para aprovechar al máximo las evaluaciones a nivel de documento.",
        "Cuando están disponibles juicios más detallados, entonces un enfoque estándar de bolsa de palabras utilizando un tamaño de ventana pequeño (de oración) y técnicas de segmentación automática puede producir resultados casi tan buenos como los enfoques basados en n-gramos.",
        "Agradecimientos: Este material se basa en trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No.",
        "NBCHD030010.",
        "Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autor(es) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) o el Centro Nacional de Negocios del Departamento del Interior (DOI-NBC).",
        "Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recolección de datos fueron esenciales para la construcción del corpus, y tanto a Jill como a Aaron Steinfeld por su dirección de los experimentos de HCI.",
        "También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de corpus y a Curtis Huttenhower por su apoyo al paquete de preprocesamiento de texto.",
        "Finalmente, agradecemos sinceramente a Scott Fahlman por su apoyo y las útiles discusiones sobre este tema. 7.",
        "REFERENCIAS [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron y Y. Yang.",
        "Estudio piloto de detección y seguimiento de temas: Informe final.",
        "En Actas del Taller de Transcripción y Comprensión de Noticias de Radiodifusión de DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss.",
        "Aprendizaje automatizado de reglas de decisión para la categorización de texto.",
        "ACM Transactions on Information Systems, 12(3):233-251, julio de 1994. [3] J. Carletta.",
        "Evaluación del acuerdo en tareas de clasificación: La estadística kappa.",
        "Lingüística Computacional, 22(2):249-254, 1996. [4] J. Carroll.",
        "Extracción de relaciones gramaticales de alta precisión.",
        "En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (COLING), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell.",
        "Aprendiendo a clasificar correos electrónicos en actos de habla.",
        "En EMNLP-2004 (Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell.",
        "Resumen centrado en la tarea del correo electrónico.",
        "En \"La ramificación de la síntesis de texto: Actas del taller ACL-04\", páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum.",
        "Extrayendo redes sociales e información de contacto de correos electrónicos y la web.",
        "En CEAS-2004 (Conferencia sobre Correo Electrónico y Anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi.",
        "Una teoría probabilística del reconocimiento de patrones.",
        "Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami.",
        "Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto.",
        "En CIKM 98, Actas de la 7ª Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire.",
        "Clasificación de márgen amplio utilizando el algoritmo del perceptrón.",
        "Aprendizaje automático, 37(3):277-296, 1999. [11] T. Joachims.",
        "Haciendo que el aprendizaje svm a gran escala sea práctico.",
        "En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en Métodos de Núcleo - Aprendizaje de Vectores de Soporte, páginas 41-56.",
        "MIT Press, 1999. [12] L. S. Larkey. \n\nMIT Press, 1999. [12] L. S. Larkey.",
        "Un sistema de búsqueda y clasificación de patentes.",
        "En Actas de la Cuarta Conferencia de Bibliotecas Digitales de la ACM, páginas 179 - 187, 1999. [13] D. D. Lewis.",
        "Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto.",
        "En SIGIR 92, Actas de la 15ª Conferencia Anual Internacional de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin.",
        "Un enfoque de conjunto por pares para una clasificación precisa de géneros.",
        "En Actas de la Conferencia Europea sobre Aprendizaje Automático (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell.",
        "Un estudio comparativo de núcleos para la clasificación de texto multi-etiqueta utilizando asociación de categorías.",
        "En la Vigésimo-primera Conferencia Internacional sobre Aprendizaje Automático (ICML), 2004. [16] A. McCallum y K. Nigam.",
        "Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes.",
        "En Notas de Trabajo de AAAI 98 (La 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre Aprendizaje para la Categorización de Textos, páginas 41-48, 1998.",
        "TR WS-98-05. [17] F. Sebastiani. \n\nTR WS-98-05. [17] F. Sebastiani.",
        "Aprendizaje automático en la categorización automatizada de textos.",
        "ACM Computing Surveys, 34(1):1-47, marzo de 2002. [18] C. J. van Rijsbergen.",
        "Recuperación de información.",
        "Butterworths, Londres, 1979. [19] Y. Yang.",
        "Una evaluación de enfoques estadísticos para la categorización de texto.",
        "Recuperación de información, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu.",
        "Enfoques de aprendizaje para la detección y seguimiento de temas.",
        "IEEE EXPERT, Número Especial sobre Aplicaciones de la Recuperación de Información Inteligente, 1999. [21] Y. Yang y X. Liu.",
        "Una reevaluación de los métodos de categorización de textos.",
        "En SIGIR 99, Actas de la 22ª Conferencia Internacional Anual de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin.",
        "Detección de novedades condicionada por el tema.",
        "En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, julio de 2002."
    ],
    "error_count": 6,
    "keys": {
        "information retrieval": {
            "translated_key": "Recuperación de Información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas <br>information retrieval</br> and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most <br>information retrieval</br> tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in <br>information retrieval</br>, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "<br>information retrieval</br>.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "<br>information retrieval</br>, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent <br>information retrieval</br>, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in <br>information retrieval</br>, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "Whereas <br>information retrieval</br> and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "As in most <br>information retrieval</br> tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in <br>information retrieval</br>, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "<br>information retrieval</br>.",
                "<br>information retrieval</br>, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu."
            ],
            "translated_annotated_samples": [
                "Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción.",
                "Como en la mayoría de las tareas de <br>Recuperación de Información</br>, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación.",
                "En SIGIR 92, Actas de la 15ª Conferencia Anual Internacional de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin.",
                "Recuperación de información.",
                "Recuperación de información, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de <br>Recuperación de Información</br>, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación. En la zona de alta precisión del gráfico, el borde consistente del clasificador a nivel de oraciones es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1. Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento. La precisión F1 de los clasificadores de nivel de oración en la detección de oraciones es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí. Esto refuerza aún más la evidencia de que un clasificador alternativo con la representación de bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador Voted Perceptron mejora cuando se aumenta el número de iteraciones de entrenamiento, pero sigue siendo inferior al clasificador SVM. Los resultados de detección de oraciones se presentan en la Tabla 6. En cuanto al problema de detección de oraciones, observamos que la medida F1 proporciona una mejor idea del espacio restante para mejorar en este problema difícil. Es decir, a diferencia de la detección de documentos donde los documentos de elementos de acción son bastante comunes, las frases de elementos de acción son muy raras. Por lo tanto, al igual que en otros problemas de texto, los números de precisión son engañosamente altos simplemente debido a la precisión predeterminada alcanzable al predecir siempre negativo. Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y el ranking. Figura 4: Los usuarios encuentran los elementos de acción más rápidamente cuando son asistidos por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso construido para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventana de tres oraciones. Había tres conjuntos distintos de correos electrónicos en los que los usuarios tenían que encontrar elementos de acción. Estos conjuntos fueron presentados en un orden aleatorio (Desordenado), ordenados por el clasificador (Ordenado), o ordenados por el clasificador y con la precisión de 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recuperación de Acciones de Detección de Elementos SVM Rendimiento (Post Selección de Modelo) Unigrama de Documento N-grama de Oración Figura 3: Tanto los n-gramas como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar. la oración central en la ventana de mayor confianza resaltada (Orden+ayuda). Para realizar comparaciones justas entre condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual; es decir, la carga cognitiva de lectura debe ser aproximadamente la misma antes de reordenar los clasificadores. Además, los usuarios suelen mostrar efectos de práctica al mejorar en la tarea general y, por lo tanto, desempeñarse mejor en los conjuntos de mensajes posteriores. Esto suele ser manejado variando el orden de los conjuntos entre usuarios para que las medias sean comparables. Sin entrar en más detalles, señalamos que los conjuntos se equilibraron en cuanto al número total de fichas y se utilizó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que en intervalos de 5, 10 y 15 minutos, los usuarios encontraron consistentemente significativamente más elementos de acción cuando fueron asistidos por el clasificador, pero fueron ayudados de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda consistentemente a los usuarios, no obtuvimos un impacto adicional en los usuarios finales al resaltar. Como se mencionó anteriormente, esto podría ser resultado del amplio margen de mejora que aún existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, resaltar la oración incorrecta cerca de una acción real perjudica la confianza de los usuarios, pero si un indicador vago (por ejemplo, una flecha) señala el área aproximada de la que el usuario no está al tanto, se evita el error por poco. Dado que el usuario estudia utilizó una ventana de tres oraciones, creemos que esto también jugó un papel en la precisión de la detección de oraciones. 4.6 Discusión En contraste con problemas donde los n-gramas han mostrado poca diferencia, creemos que su poder aquí se deriva del hecho de que muchos de los n-gramas significativos para las acciones consisten en palabras comunes, por ejemplo, házmelo saber. Por lo tanto, el enfoque de unigrama a nivel de documento no puede obtener mucha ventaja, incluso al modelar correctamente su probabilidad conjunta, ya que estas palabras a menudo co-ocurrirán en el documento pero no necesariamente en una frase. Además, la detección de elementos de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden depender de la agregación de evidencia de un gran número de indicadores débiles en todo el documento. A pesar de haber descartado la información del encabezado, al examinar las características mejor clasificadas a nivel de documento, se revela que muchas de las características son nombres o partes de direcciones de correo electrónico que aparecieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden a contener muchos o ningún elemento de acción. Algunos ejemplos son términos como org, bob y gov. Observamos que estas características serán sensibles a la distribución particular (emisores/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieran menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá del modelo de bolsa de palabras puede ser la metodología, y al investigar propiedades como las curvas de aprendizaje y qué tan bien un modelo se transfiere, se pueden resaltar diferencias en modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones en las que fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin necesidad de volver a entrenarlos. 5. TRABAJO FUTURO Si bien la aplicación de clasificadores de texto a nivel de documento está bastante bien entendida, existe el potencial de aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Tales métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas a tfidf, que pueden no ser apropiadas dado que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis frásico. Además, el etiquetado de entidades nombradas, las expresiones de tiempo, etc., parecen ser candidatos probables para características que pueden mejorar aún más esta tarea. Actualmente estamos explorando algunas de estas vías para ver qué beneficios adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el clasificador a nivel de oración. Un modelo así podría servir como un ejemplo general para otros problemas donde el enfoque de bolsa de palabras puede establecer un modelo base, pero se necesitan enfoques más complejos para lograr un rendimiento más allá de ese modelo base. RESUMEN Y CONCLUSIONES La efectividad de la detección a nivel de oración argumenta que etiquetar a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo esto interactúa con la cantidad de datos de entrenamiento disponibles. La detección de oraciones que luego se aglomera a nivel de documento funciona sorprendentemente mejor con un bajo índice de recuperación de lo que se esperaría con elementos a nivel de oración. Esto, a su vez, indica que métodos mejorados de segmentación de oraciones podrían generar más mejoras en la clasificación. En este trabajo, examinamos cómo se pueden detectar de manera efectiva los elementos de acción en correos electrónicos. Nuestro análisis empírico ha demostrado que los n-gramos son de vital importancia para aprovechar al máximo las evaluaciones a nivel de documento. Cuando están disponibles juicios más detallados, entonces un enfoque estándar de bolsa de palabras utilizando un tamaño de ventana pequeño (de oración) y técnicas de segmentación automática puede producir resultados casi tan buenos como los enfoques basados en n-gramos. Agradecimientos: Este material se basa en trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autor(es) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) o el Centro Nacional de Negocios del Departamento del Interior (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recolección de datos fueron esenciales para la construcción del corpus, y tanto a Jill como a Aaron Steinfeld por su dirección de los experimentos de HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de corpus y a Curtis Huttenhower por su apoyo al paquete de preprocesamiento de texto. Finalmente, agradecemos sinceramente a Scott Fahlman por su apoyo y las útiles discusiones sobre este tema. 7. REFERENCIAS [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron y Y. Yang. Estudio piloto de detección y seguimiento de temas: Informe final. En Actas del Taller de Transcripción y Comprensión de Noticias de Radiodifusión de DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automatizado de reglas de decisión para la categorización de texto. ACM Transactions on Information Systems, 12(3):233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo en tareas de clasificación: La estadística kappa. Lingüística Computacional, 22(2):249-254, 1996. [4] J. Carroll. Extracción de relaciones gramaticales de alta precisión. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (COLING), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprendiendo a clasificar correos electrónicos en actos de habla. En EMNLP-2004 (Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen centrado en la tarea del correo electrónico. En \"La ramificación de la síntesis de texto: Actas del taller ACL-04\", páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extrayendo redes sociales e información de contacto de correos electrónicos y la web. En CEAS-2004 (Conferencia sobre Correo Electrónico y Anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, Actas de la 7ª Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [11] T. Joachims. Haciendo que el aprendizaje svm a gran escala sea práctico. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en Métodos de Núcleo - Aprendizaje de Vectores de Soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. \n\nMIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia de Bibliotecas Digitales de la ACM, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto. En SIGIR 92, Actas de la 15ª Conferencia Anual Internacional de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin. Un enfoque de conjunto por pares para una clasificación precisa de géneros. En Actas de la Conferencia Europea sobre Aprendizaje Automático (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell. Un estudio comparativo de núcleos para la clasificación de texto multi-etiqueta utilizando asociación de categorías. En la Vigésimo-primera Conferencia Internacional sobre Aprendizaje Automático (ICML), 2004. [16] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En Notas de Trabajo de AAAI 98 (La 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre Aprendizaje para la Categorización de Textos, páginas 41-48, 1998. TR WS-98-05. [17] F. Sebastiani. \n\nTR WS-98-05. [17] F. Sebastiani. Aprendizaje automático en la categorización automatizada de textos. ACM Computing Surveys, 34(1):1-47, marzo de 2002. [18] C. J. van Rijsbergen. Recuperación de información. Butterworths, Londres, 1979. [19] Y. Yang. Una evaluación de enfoques estadísticos para la categorización de texto. Recuperación de información, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "topic-driven text classification": {
            "translated_key": "clasificación de texto estándar basada en temas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard <br>topic-driven text classification</br>, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "Unlike standard <br>topic-driven text classification</br>, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification."
            ],
            "translated_annotated_samples": [
                "A diferencia de la <br>clasificación de texto estándar basada en temas</br>, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la <br>clasificación de texto estándar basada en temas</br>, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación. En la zona de alta precisión del gráfico, el borde consistente del clasificador a nivel de oraciones es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1. Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento. La precisión F1 de los clasificadores de nivel de oración en la detección de oraciones es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí. Esto refuerza aún más la evidencia de que un clasificador alternativo con la representación de bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador Voted Perceptron mejora cuando se aumenta el número de iteraciones de entrenamiento, pero sigue siendo inferior al clasificador SVM. Los resultados de detección de oraciones se presentan en la Tabla 6. En cuanto al problema de detección de oraciones, observamos que la medida F1 proporciona una mejor idea del espacio restante para mejorar en este problema difícil. Es decir, a diferencia de la detección de documentos donde los documentos de elementos de acción son bastante comunes, las frases de elementos de acción son muy raras. Por lo tanto, al igual que en otros problemas de texto, los números de precisión son engañosamente altos simplemente debido a la precisión predeterminada alcanzable al predecir siempre negativo. Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y el ranking. Figura 4: Los usuarios encuentran los elementos de acción más rápidamente cuando son asistidos por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso construido para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventana de tres oraciones. Había tres conjuntos distintos de correos electrónicos en los que los usuarios tenían que encontrar elementos de acción. Estos conjuntos fueron presentados en un orden aleatorio (Desordenado), ordenados por el clasificador (Ordenado), o ordenados por el clasificador y con la precisión de 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recuperación de Acciones de Detección de Elementos SVM Rendimiento (Post Selección de Modelo) Unigrama de Documento N-grama de Oración Figura 3: Tanto los n-gramas como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar. la oración central en la ventana de mayor confianza resaltada (Orden+ayuda). Para realizar comparaciones justas entre condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual; es decir, la carga cognitiva de lectura debe ser aproximadamente la misma antes de reordenar los clasificadores. Además, los usuarios suelen mostrar efectos de práctica al mejorar en la tarea general y, por lo tanto, desempeñarse mejor en los conjuntos de mensajes posteriores. Esto suele ser manejado variando el orden de los conjuntos entre usuarios para que las medias sean comparables. Sin entrar en más detalles, señalamos que los conjuntos se equilibraron en cuanto al número total de fichas y se utilizó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que en intervalos de 5, 10 y 15 minutos, los usuarios encontraron consistentemente significativamente más elementos de acción cuando fueron asistidos por el clasificador, pero fueron ayudados de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda consistentemente a los usuarios, no obtuvimos un impacto adicional en los usuarios finales al resaltar. Como se mencionó anteriormente, esto podría ser resultado del amplio margen de mejora que aún existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, resaltar la oración incorrecta cerca de una acción real perjudica la confianza de los usuarios, pero si un indicador vago (por ejemplo, una flecha) señala el área aproximada de la que el usuario no está al tanto, se evita el error por poco. Dado que el usuario estudia utilizó una ventana de tres oraciones, creemos que esto también jugó un papel en la precisión de la detección de oraciones. 4.6 Discusión En contraste con problemas donde los n-gramas han mostrado poca diferencia, creemos que su poder aquí se deriva del hecho de que muchos de los n-gramas significativos para las acciones consisten en palabras comunes, por ejemplo, házmelo saber. Por lo tanto, el enfoque de unigrama a nivel de documento no puede obtener mucha ventaja, incluso al modelar correctamente su probabilidad conjunta, ya que estas palabras a menudo co-ocurrirán en el documento pero no necesariamente en una frase. Además, la detección de elementos de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden depender de la agregación de evidencia de un gran número de indicadores débiles en todo el documento. A pesar de haber descartado la información del encabezado, al examinar las características mejor clasificadas a nivel de documento, se revela que muchas de las características son nombres o partes de direcciones de correo electrónico que aparecieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden a contener muchos o ningún elemento de acción. Algunos ejemplos son términos como org, bob y gov. Observamos que estas características serán sensibles a la distribución particular (emisores/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieran menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá del modelo de bolsa de palabras puede ser la metodología, y al investigar propiedades como las curvas de aprendizaje y qué tan bien un modelo se transfiere, se pueden resaltar diferencias en modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones en las que fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin necesidad de volver a entrenarlos. 5. TRABAJO FUTURO Si bien la aplicación de clasificadores de texto a nivel de documento está bastante bien entendida, existe el potencial de aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Tales métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas a tfidf, que pueden no ser apropiadas dado que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis frásico. Además, el etiquetado de entidades nombradas, las expresiones de tiempo, etc., parecen ser candidatos probables para características que pueden mejorar aún más esta tarea. Actualmente estamos explorando algunas de estas vías para ver qué beneficios adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el clasificador a nivel de oración. Un modelo así podría servir como un ejemplo general para otros problemas donde el enfoque de bolsa de palabras puede establecer un modelo base, pero se necesitan enfoques más complejos para lograr un rendimiento más allá de ese modelo base. RESUMEN Y CONCLUSIONES La efectividad de la detección a nivel de oración argumenta que etiquetar a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo esto interactúa con la cantidad de datos de entrenamiento disponibles. La detección de oraciones que luego se aglomera a nivel de documento funciona sorprendentemente mejor con un bajo índice de recuperación de lo que se esperaría con elementos a nivel de oración. Esto, a su vez, indica que métodos mejorados de segmentación de oraciones podrían generar más mejoras en la clasificación. En este trabajo, examinamos cómo se pueden detectar de manera efectiva los elementos de acción en correos electrónicos. Nuestro análisis empírico ha demostrado que los n-gramos son de vital importancia para aprovechar al máximo las evaluaciones a nivel de documento. Cuando están disponibles juicios más detallados, entonces un enfoque estándar de bolsa de palabras utilizando un tamaño de ventana pequeño (de oración) y técnicas de segmentación automática puede producir resultados casi tan buenos como los enfoques basados en n-gramos. Agradecimientos: Este material se basa en trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autor(es) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) o el Centro Nacional de Negocios del Departamento del Interior (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recolección de datos fueron esenciales para la construcción del corpus, y tanto a Jill como a Aaron Steinfeld por su dirección de los experimentos de HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de corpus y a Curtis Huttenhower por su apoyo al paquete de preprocesamiento de texto. Finalmente, agradecemos sinceramente a Scott Fahlman por su apoyo y las útiles discusiones sobre este tema. 7. REFERENCIAS [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron y Y. Yang. Estudio piloto de detección y seguimiento de temas: Informe final. En Actas del Taller de Transcripción y Comprensión de Noticias de Radiodifusión de DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automatizado de reglas de decisión para la categorización de texto. ACM Transactions on Information Systems, 12(3):233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo en tareas de clasificación: La estadística kappa. Lingüística Computacional, 22(2):249-254, 1996. [4] J. Carroll. Extracción de relaciones gramaticales de alta precisión. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (COLING), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprendiendo a clasificar correos electrónicos en actos de habla. En EMNLP-2004 (Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen centrado en la tarea del correo electrónico. En \"La ramificación de la síntesis de texto: Actas del taller ACL-04\", páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extrayendo redes sociales e información de contacto de correos electrónicos y la web. En CEAS-2004 (Conferencia sobre Correo Electrónico y Anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, Actas de la 7ª Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [11] T. Joachims. Haciendo que el aprendizaje svm a gran escala sea práctico. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en Métodos de Núcleo - Aprendizaje de Vectores de Soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. \n\nMIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia de Bibliotecas Digitales de la ACM, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto. En SIGIR 92, Actas de la 15ª Conferencia Anual Internacional de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin. Un enfoque de conjunto por pares para una clasificación precisa de géneros. En Actas de la Conferencia Europea sobre Aprendizaje Automático (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell. Un estudio comparativo de núcleos para la clasificación de texto multi-etiqueta utilizando asociación de categorías. En la Vigésimo-primera Conferencia Internacional sobre Aprendizaje Automático (ICML), 2004. [16] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En Notas de Trabajo de AAAI 98 (La 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre Aprendizaje para la Categorización de Textos, páginas 41-48, 1998. TR WS-98-05. [17] F. Sebastiani. \n\nTR WS-98-05. [17] F. Sebastiani. Aprendizaje automático en la categorización automatizada de textos. ACM Computing Surveys, 34(1):1-47, marzo de 2002. [18] C. J. van Rijsbergen. Recuperación de información. Butterworths, Londres, 1979. [19] Y. Yang. Una evaluación de enfoques estadísticos para la categorización de texto. Recuperación de información, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu. Enfoques de aprendizaje para la detección y seguimiento de temas. IEEE EXPERT, Número Especial sobre Aplicaciones de la Recuperación de Información Inteligente, 1999. [21] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de textos. En SIGIR 99, Actas de la 22ª Conferencia Internacional Anual de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada por el tema. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, julio de 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "action-item detection": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective <br>action-item detection</br> Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated <br>action-item detection</br>, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, <br>action-item detection</br> requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated <br>action-item detection</br> targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view <br>action-item detection</br> as one necessary component of a successful e-mail agent which would perform spam detection, <br>action-item detection</br>, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "<br>action-item detection</br> differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, <br>action-item detection</br> attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that <br>action-item detection</br> is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the <br>action-item detection</br> problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall <br>action-item detection</br> SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, <br>action-item detection</br> is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "Feature Representation for Effective <br>action-item detection</br> Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated <br>action-item detection</br>, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, <br>action-item detection</br> requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "Automated <br>action-item detection</br> targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "We view <br>action-item detection</br> as one necessary component of a successful e-mail agent which would perform spam detection, <br>action-item detection</br>, topic classification and priority ranking, among other functions."
            ],
            "translated_annotated_samples": [
                "Representación de características para la <br>detección efectiva de elementos de acción</br>. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje.",
                "Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la <br>detección automatizada de elementos de acción</br>, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción.",
                "A diferencia de la clasificación de texto estándar basada en temas, la <br>detección de elementos de acción</br> requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras.",
                "La <br>detección automatizada de elementos de acción</br> se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción.",
                "Consideramos la <br>detección de elementos de acción</br> como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, <br>detección de elementos de acción</br>, clasificación de temas y ranking de prioridades, entre otras funciones."
            ],
            "translated_text": "Representación de características para la <br>detección efectiva de elementos de acción</br>. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la <br>detección automatizada de elementos de acción</br>, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la <br>detección de elementos de acción</br> requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La <br>detección automatizada de elementos de acción</br> se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la <br>detección de elementos de acción</br> como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, <br>detección de elementos de acción</br>, clasificación de temas y ranking de prioridades, entre otras funciones. ",
            "candidates": [],
            "error": [
                [
                    "detección efectiva de elementos de acción",
                    "detección automatizada de elementos de acción",
                    "detección de elementos de acción",
                    "detección automatizada de elementos de acción",
                    "detección de elementos de acción",
                    "detección de elementos de acción"
                ]
            ]
        },
        "chi-squared feature selection": {
            "translated_key": "selección de características chi-cuadrado",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with <br>chi-squared feature selection</br>, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "However, using enriched feature sets, such as n-grams (up to n=4) with <br>chi-squared feature selection</br>, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation."
            ],
            "translated_annotated_samples": [
                "Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con <br>selección de características chi-cuadrado</br>, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con <br>selección de características chi-cuadrado</br>, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación. En la zona de alta precisión del gráfico, el borde consistente del clasificador a nivel de oraciones es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1. Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento. La precisión F1 de los clasificadores de nivel de oración en la detección de oraciones es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí. Esto refuerza aún más la evidencia de que un clasificador alternativo con la representación de bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador Voted Perceptron mejora cuando se aumenta el número de iteraciones de entrenamiento, pero sigue siendo inferior al clasificador SVM. Los resultados de detección de oraciones se presentan en la Tabla 6. En cuanto al problema de detección de oraciones, observamos que la medida F1 proporciona una mejor idea del espacio restante para mejorar en este problema difícil. Es decir, a diferencia de la detección de documentos donde los documentos de elementos de acción son bastante comunes, las frases de elementos de acción son muy raras. Por lo tanto, al igual que en otros problemas de texto, los números de precisión son engañosamente altos simplemente debido a la precisión predeterminada alcanzable al predecir siempre negativo. Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y el ranking. Figura 4: Los usuarios encuentran los elementos de acción más rápidamente cuando son asistidos por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso construido para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventana de tres oraciones. Había tres conjuntos distintos de correos electrónicos en los que los usuarios tenían que encontrar elementos de acción. Estos conjuntos fueron presentados en un orden aleatorio (Desordenado), ordenados por el clasificador (Ordenado), o ordenados por el clasificador y con la precisión de 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recuperación de Acciones de Detección de Elementos SVM Rendimiento (Post Selección de Modelo) Unigrama de Documento N-grama de Oración Figura 3: Tanto los n-gramas como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar. la oración central en la ventana de mayor confianza resaltada (Orden+ayuda). Para realizar comparaciones justas entre condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual; es decir, la carga cognitiva de lectura debe ser aproximadamente la misma antes de reordenar los clasificadores. Además, los usuarios suelen mostrar efectos de práctica al mejorar en la tarea general y, por lo tanto, desempeñarse mejor en los conjuntos de mensajes posteriores. Esto suele ser manejado variando el orden de los conjuntos entre usuarios para que las medias sean comparables. Sin entrar en más detalles, señalamos que los conjuntos se equilibraron en cuanto al número total de fichas y se utilizó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que en intervalos de 5, 10 y 15 minutos, los usuarios encontraron consistentemente significativamente más elementos de acción cuando fueron asistidos por el clasificador, pero fueron ayudados de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda consistentemente a los usuarios, no obtuvimos un impacto adicional en los usuarios finales al resaltar. Como se mencionó anteriormente, esto podría ser resultado del amplio margen de mejora que aún existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, resaltar la oración incorrecta cerca de una acción real perjudica la confianza de los usuarios, pero si un indicador vago (por ejemplo, una flecha) señala el área aproximada de la que el usuario no está al tanto, se evita el error por poco. Dado que el usuario estudia utilizó una ventana de tres oraciones, creemos que esto también jugó un papel en la precisión de la detección de oraciones. 4.6 Discusión En contraste con problemas donde los n-gramas han mostrado poca diferencia, creemos que su poder aquí se deriva del hecho de que muchos de los n-gramas significativos para las acciones consisten en palabras comunes, por ejemplo, házmelo saber. Por lo tanto, el enfoque de unigrama a nivel de documento no puede obtener mucha ventaja, incluso al modelar correctamente su probabilidad conjunta, ya que estas palabras a menudo co-ocurrirán en el documento pero no necesariamente en una frase. Además, la detección de elementos de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden depender de la agregación de evidencia de un gran número de indicadores débiles en todo el documento. A pesar de haber descartado la información del encabezado, al examinar las características mejor clasificadas a nivel de documento, se revela que muchas de las características son nombres o partes de direcciones de correo electrónico que aparecieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden a contener muchos o ningún elemento de acción. Algunos ejemplos son términos como org, bob y gov. Observamos que estas características serán sensibles a la distribución particular (emisores/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieran menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá del modelo de bolsa de palabras puede ser la metodología, y al investigar propiedades como las curvas de aprendizaje y qué tan bien un modelo se transfiere, se pueden resaltar diferencias en modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones en las que fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin necesidad de volver a entrenarlos. 5. TRABAJO FUTURO Si bien la aplicación de clasificadores de texto a nivel de documento está bastante bien entendida, existe el potencial de aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Tales métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas a tfidf, que pueden no ser apropiadas dado que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis frásico. Además, el etiquetado de entidades nombradas, las expresiones de tiempo, etc., parecen ser candidatos probables para características que pueden mejorar aún más esta tarea. Actualmente estamos explorando algunas de estas vías para ver qué beneficios adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el clasificador a nivel de oración. Un modelo así podría servir como un ejemplo general para otros problemas donde el enfoque de bolsa de palabras puede establecer un modelo base, pero se necesitan enfoques más complejos para lograr un rendimiento más allá de ese modelo base. RESUMEN Y CONCLUSIONES La efectividad de la detección a nivel de oración argumenta que etiquetar a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo esto interactúa con la cantidad de datos de entrenamiento disponibles. La detección de oraciones que luego se aglomera a nivel de documento funciona sorprendentemente mejor con un bajo índice de recuperación de lo que se esperaría con elementos a nivel de oración. Esto, a su vez, indica que métodos mejorados de segmentación de oraciones podrían generar más mejoras en la clasificación. En este trabajo, examinamos cómo se pueden detectar de manera efectiva los elementos de acción en correos electrónicos. Nuestro análisis empírico ha demostrado que los n-gramos son de vital importancia para aprovechar al máximo las evaluaciones a nivel de documento. Cuando están disponibles juicios más detallados, entonces un enfoque estándar de bolsa de palabras utilizando un tamaño de ventana pequeño (de oración) y técnicas de segmentación automática puede producir resultados casi tan buenos como los enfoques basados en n-gramos. Agradecimientos: Este material se basa en trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autor(es) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) o el Centro Nacional de Negocios del Departamento del Interior (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recolección de datos fueron esenciales para la construcción del corpus, y tanto a Jill como a Aaron Steinfeld por su dirección de los experimentos de HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de corpus y a Curtis Huttenhower por su apoyo al paquete de preprocesamiento de texto. Finalmente, agradecemos sinceramente a Scott Fahlman por su apoyo y las útiles discusiones sobre este tema. 7. REFERENCIAS [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron y Y. Yang. Estudio piloto de detección y seguimiento de temas: Informe final. En Actas del Taller de Transcripción y Comprensión de Noticias de Radiodifusión de DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automatizado de reglas de decisión para la categorización de texto. ACM Transactions on Information Systems, 12(3):233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo en tareas de clasificación: La estadística kappa. Lingüística Computacional, 22(2):249-254, 1996. [4] J. Carroll. Extracción de relaciones gramaticales de alta precisión. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (COLING), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprendiendo a clasificar correos electrónicos en actos de habla. En EMNLP-2004 (Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen centrado en la tarea del correo electrónico. En \"La ramificación de la síntesis de texto: Actas del taller ACL-04\", páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extrayendo redes sociales e información de contacto de correos electrónicos y la web. En CEAS-2004 (Conferencia sobre Correo Electrónico y Anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, Actas de la 7ª Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [11] T. Joachims. Haciendo que el aprendizaje svm a gran escala sea práctico. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en Métodos de Núcleo - Aprendizaje de Vectores de Soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. \n\nMIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia de Bibliotecas Digitales de la ACM, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto. En SIGIR 92, Actas de la 15ª Conferencia Anual Internacional de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin. Un enfoque de conjunto por pares para una clasificación precisa de géneros. En Actas de la Conferencia Europea sobre Aprendizaje Automático (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell. Un estudio comparativo de núcleos para la clasificación de texto multi-etiqueta utilizando asociación de categorías. En la Vigésimo-primera Conferencia Internacional sobre Aprendizaje Automático (ICML), 2004. [16] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En Notas de Trabajo de AAAI 98 (La 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre Aprendizaje para la Categorización de Textos, páginas 41-48, 1998. TR WS-98-05. [17] F. Sebastiani. \n\nTR WS-98-05. [17] F. Sebastiani. Aprendizaje automático en la categorización automatizada de textos. ACM Computing Surveys, 34(1):1-47, marzo de 2002. [18] C. J. van Rijsbergen. Recuperación de información. Butterworths, Londres, 1979. [19] Y. Yang. Una evaluación de enfoques estadísticos para la categorización de texto. Recuperación de información, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu. Enfoques de aprendizaje para la detección y seguimiento de temas. IEEE EXPERT, Número Especial sobre Aplicaciones de la Recuperación de Información Inteligente, 1999. [21] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de textos. En SIGIR 99, Actas de la 22ª Conferencia Internacional Anual de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada por el tema. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, julio de 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "automated model selection": {
            "translated_key": "selección de modelo automatizada",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with <br>automated model selection</br> via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with <br>automated model selection</br> via embedded cross-validation."
            ],
            "translated_annotated_samples": [
                "Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con <br>selección de modelo automatizada</br> a través de validación cruzada incrustada."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con <br>selección de modelo automatizada</br> a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación. En la zona de alta precisión del gráfico, el borde consistente del clasificador a nivel de oraciones es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1. Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento. La precisión F1 de los clasificadores de nivel de oración en la detección de oraciones es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí. Esto refuerza aún más la evidencia de que un clasificador alternativo con la representación de bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador Voted Perceptron mejora cuando se aumenta el número de iteraciones de entrenamiento, pero sigue siendo inferior al clasificador SVM. Los resultados de detección de oraciones se presentan en la Tabla 6. En cuanto al problema de detección de oraciones, observamos que la medida F1 proporciona una mejor idea del espacio restante para mejorar en este problema difícil. Es decir, a diferencia de la detección de documentos donde los documentos de elementos de acción son bastante comunes, las frases de elementos de acción son muy raras. Por lo tanto, al igual que en otros problemas de texto, los números de precisión son engañosamente altos simplemente debido a la precisión predeterminada alcanzable al predecir siempre negativo. Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y el ranking. Figura 4: Los usuarios encuentran los elementos de acción más rápidamente cuando son asistidos por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso construido para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventana de tres oraciones. Había tres conjuntos distintos de correos electrónicos en los que los usuarios tenían que encontrar elementos de acción. Estos conjuntos fueron presentados en un orden aleatorio (Desordenado), ordenados por el clasificador (Ordenado), o ordenados por el clasificador y con la precisión de 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recuperación de Acciones de Detección de Elementos SVM Rendimiento (Post Selección de Modelo) Unigrama de Documento N-grama de Oración Figura 3: Tanto los n-gramas como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar. la oración central en la ventana de mayor confianza resaltada (Orden+ayuda). Para realizar comparaciones justas entre condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual; es decir, la carga cognitiva de lectura debe ser aproximadamente la misma antes de reordenar los clasificadores. Además, los usuarios suelen mostrar efectos de práctica al mejorar en la tarea general y, por lo tanto, desempeñarse mejor en los conjuntos de mensajes posteriores. Esto suele ser manejado variando el orden de los conjuntos entre usuarios para que las medias sean comparables. Sin entrar en más detalles, señalamos que los conjuntos se equilibraron en cuanto al número total de fichas y se utilizó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que en intervalos de 5, 10 y 15 minutos, los usuarios encontraron consistentemente significativamente más elementos de acción cuando fueron asistidos por el clasificador, pero fueron ayudados de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda consistentemente a los usuarios, no obtuvimos un impacto adicional en los usuarios finales al resaltar. Como se mencionó anteriormente, esto podría ser resultado del amplio margen de mejora que aún existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, resaltar la oración incorrecta cerca de una acción real perjudica la confianza de los usuarios, pero si un indicador vago (por ejemplo, una flecha) señala el área aproximada de la que el usuario no está al tanto, se evita el error por poco. Dado que el usuario estudia utilizó una ventana de tres oraciones, creemos que esto también jugó un papel en la precisión de la detección de oraciones. 4.6 Discusión En contraste con problemas donde los n-gramas han mostrado poca diferencia, creemos que su poder aquí se deriva del hecho de que muchos de los n-gramas significativos para las acciones consisten en palabras comunes, por ejemplo, házmelo saber. Por lo tanto, el enfoque de unigrama a nivel de documento no puede obtener mucha ventaja, incluso al modelar correctamente su probabilidad conjunta, ya que estas palabras a menudo co-ocurrirán en el documento pero no necesariamente en una frase. Además, la detección de elementos de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden depender de la agregación de evidencia de un gran número de indicadores débiles en todo el documento. A pesar de haber descartado la información del encabezado, al examinar las características mejor clasificadas a nivel de documento, se revela que muchas de las características son nombres o partes de direcciones de correo electrónico que aparecieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden a contener muchos o ningún elemento de acción. Algunos ejemplos son términos como org, bob y gov. Observamos que estas características serán sensibles a la distribución particular (emisores/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieran menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá del modelo de bolsa de palabras puede ser la metodología, y al investigar propiedades como las curvas de aprendizaje y qué tan bien un modelo se transfiere, se pueden resaltar diferencias en modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones en las que fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin necesidad de volver a entrenarlos. 5. TRABAJO FUTURO Si bien la aplicación de clasificadores de texto a nivel de documento está bastante bien entendida, existe el potencial de aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Tales métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas a tfidf, que pueden no ser apropiadas dado que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis frásico. Además, el etiquetado de entidades nombradas, las expresiones de tiempo, etc., parecen ser candidatos probables para características que pueden mejorar aún más esta tarea. Actualmente estamos explorando algunas de estas vías para ver qué beneficios adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el clasificador a nivel de oración. Un modelo así podría servir como un ejemplo general para otros problemas donde el enfoque de bolsa de palabras puede establecer un modelo base, pero se necesitan enfoques más complejos para lograr un rendimiento más allá de ese modelo base. RESUMEN Y CONCLUSIONES La efectividad de la detección a nivel de oración argumenta que etiquetar a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo esto interactúa con la cantidad de datos de entrenamiento disponibles. La detección de oraciones que luego se aglomera a nivel de documento funciona sorprendentemente mejor con un bajo índice de recuperación de lo que se esperaría con elementos a nivel de oración. Esto, a su vez, indica que métodos mejorados de segmentación de oraciones podrían generar más mejoras en la clasificación. En este trabajo, examinamos cómo se pueden detectar de manera efectiva los elementos de acción en correos electrónicos. Nuestro análisis empírico ha demostrado que los n-gramos son de vital importancia para aprovechar al máximo las evaluaciones a nivel de documento. Cuando están disponibles juicios más detallados, entonces un enfoque estándar de bolsa de palabras utilizando un tamaño de ventana pequeño (de oración) y técnicas de segmentación automática puede producir resultados casi tan buenos como los enfoques basados en n-gramos. Agradecimientos: Este material se basa en trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autor(es) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) o el Centro Nacional de Negocios del Departamento del Interior (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recolección de datos fueron esenciales para la construcción del corpus, y tanto a Jill como a Aaron Steinfeld por su dirección de los experimentos de HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de corpus y a Curtis Huttenhower por su apoyo al paquete de preprocesamiento de texto. Finalmente, agradecemos sinceramente a Scott Fahlman por su apoyo y las útiles discusiones sobre este tema. 7. REFERENCIAS [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron y Y. Yang. Estudio piloto de detección y seguimiento de temas: Informe final. En Actas del Taller de Transcripción y Comprensión de Noticias de Radiodifusión de DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automatizado de reglas de decisión para la categorización de texto. ACM Transactions on Information Systems, 12(3):233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo en tareas de clasificación: La estadística kappa. Lingüística Computacional, 22(2):249-254, 1996. [4] J. Carroll. Extracción de relaciones gramaticales de alta precisión. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (COLING), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprendiendo a clasificar correos electrónicos en actos de habla. En EMNLP-2004 (Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen centrado en la tarea del correo electrónico. En \"La ramificación de la síntesis de texto: Actas del taller ACL-04\", páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extrayendo redes sociales e información de contacto de correos electrónicos y la web. En CEAS-2004 (Conferencia sobre Correo Electrónico y Anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, Actas de la 7ª Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [11] T. Joachims. Haciendo que el aprendizaje svm a gran escala sea práctico. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en Métodos de Núcleo - Aprendizaje de Vectores de Soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. \n\nMIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia de Bibliotecas Digitales de la ACM, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto. En SIGIR 92, Actas de la 15ª Conferencia Anual Internacional de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin. Un enfoque de conjunto por pares para una clasificación precisa de géneros. En Actas de la Conferencia Europea sobre Aprendizaje Automático (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell. Un estudio comparativo de núcleos para la clasificación de texto multi-etiqueta utilizando asociación de categorías. En la Vigésimo-primera Conferencia Internacional sobre Aprendizaje Automático (ICML), 2004. [16] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En Notas de Trabajo de AAAI 98 (La 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre Aprendizaje para la Categorización de Textos, páginas 41-48, 1998. TR WS-98-05. [17] F. Sebastiani. \n\nTR WS-98-05. [17] F. Sebastiani. Aprendizaje automático en la categorización automatizada de textos. ACM Computing Surveys, 34(1):1-47, marzo de 2002. [18] C. J. van Rijsbergen. Recuperación de información. Butterworths, Londres, 1979. [19] Y. Yang. Una evaluación de enfoques estadísticos para la categorización de texto. Recuperación de información, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu. Enfoques de aprendizaje para la detección y seguimiento de temas. IEEE EXPERT, Número Especial sobre Aplicaciones de la Recuperación de Información Inteligente, 1999. [21] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de textos. En SIGIR 99, Actas de la 22ª Conferencia Internacional Anual de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada por el tema. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, julio de 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "embedded cross-validation": {
            "translated_key": "validación cruzada incrustada",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via <br>embedded cross-validation</br>.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via <br>embedded cross-validation</br>."
            ],
            "translated_annotated_samples": [
                "Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de <br>validación cruzada incrustada</br>."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de <br>validación cruzada incrustada</br>. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación. En la zona de alta precisión del gráfico, el borde consistente del clasificador a nivel de oraciones es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1. Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento. La precisión F1 de los clasificadores de nivel de oración en la detección de oraciones es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí. Esto refuerza aún más la evidencia de que un clasificador alternativo con la representación de bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador Voted Perceptron mejora cuando se aumenta el número de iteraciones de entrenamiento, pero sigue siendo inferior al clasificador SVM. Los resultados de detección de oraciones se presentan en la Tabla 6. En cuanto al problema de detección de oraciones, observamos que la medida F1 proporciona una mejor idea del espacio restante para mejorar en este problema difícil. Es decir, a diferencia de la detección de documentos donde los documentos de elementos de acción son bastante comunes, las frases de elementos de acción son muy raras. Por lo tanto, al igual que en otros problemas de texto, los números de precisión son engañosamente altos simplemente debido a la precisión predeterminada alcanzable al predecir siempre negativo. Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y el ranking. Figura 4: Los usuarios encuentran los elementos de acción más rápidamente cuando son asistidos por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso construido para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventana de tres oraciones. Había tres conjuntos distintos de correos electrónicos en los que los usuarios tenían que encontrar elementos de acción. Estos conjuntos fueron presentados en un orden aleatorio (Desordenado), ordenados por el clasificador (Ordenado), o ordenados por el clasificador y con la precisión de 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recuperación de Acciones de Detección de Elementos SVM Rendimiento (Post Selección de Modelo) Unigrama de Documento N-grama de Oración Figura 3: Tanto los n-gramas como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar. la oración central en la ventana de mayor confianza resaltada (Orden+ayuda). Para realizar comparaciones justas entre condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual; es decir, la carga cognitiva de lectura debe ser aproximadamente la misma antes de reordenar los clasificadores. Además, los usuarios suelen mostrar efectos de práctica al mejorar en la tarea general y, por lo tanto, desempeñarse mejor en los conjuntos de mensajes posteriores. Esto suele ser manejado variando el orden de los conjuntos entre usuarios para que las medias sean comparables. Sin entrar en más detalles, señalamos que los conjuntos se equilibraron en cuanto al número total de fichas y se utilizó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que en intervalos de 5, 10 y 15 minutos, los usuarios encontraron consistentemente significativamente más elementos de acción cuando fueron asistidos por el clasificador, pero fueron ayudados de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda consistentemente a los usuarios, no obtuvimos un impacto adicional en los usuarios finales al resaltar. Como se mencionó anteriormente, esto podría ser resultado del amplio margen de mejora que aún existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, resaltar la oración incorrecta cerca de una acción real perjudica la confianza de los usuarios, pero si un indicador vago (por ejemplo, una flecha) señala el área aproximada de la que el usuario no está al tanto, se evita el error por poco. Dado que el usuario estudia utilizó una ventana de tres oraciones, creemos que esto también jugó un papel en la precisión de la detección de oraciones. 4.6 Discusión En contraste con problemas donde los n-gramas han mostrado poca diferencia, creemos que su poder aquí se deriva del hecho de que muchos de los n-gramas significativos para las acciones consisten en palabras comunes, por ejemplo, házmelo saber. Por lo tanto, el enfoque de unigrama a nivel de documento no puede obtener mucha ventaja, incluso al modelar correctamente su probabilidad conjunta, ya que estas palabras a menudo co-ocurrirán en el documento pero no necesariamente en una frase. Además, la detección de elementos de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden depender de la agregación de evidencia de un gran número de indicadores débiles en todo el documento. A pesar de haber descartado la información del encabezado, al examinar las características mejor clasificadas a nivel de documento, se revela que muchas de las características son nombres o partes de direcciones de correo electrónico que aparecieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden a contener muchos o ningún elemento de acción. Algunos ejemplos son términos como org, bob y gov. Observamos que estas características serán sensibles a la distribución particular (emisores/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieran menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá del modelo de bolsa de palabras puede ser la metodología, y al investigar propiedades como las curvas de aprendizaje y qué tan bien un modelo se transfiere, se pueden resaltar diferencias en modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones en las que fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin necesidad de volver a entrenarlos. 5. TRABAJO FUTURO Si bien la aplicación de clasificadores de texto a nivel de documento está bastante bien entendida, existe el potencial de aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Tales métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas a tfidf, que pueden no ser apropiadas dado que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis frásico. Además, el etiquetado de entidades nombradas, las expresiones de tiempo, etc., parecen ser candidatos probables para características que pueden mejorar aún más esta tarea. Actualmente estamos explorando algunas de estas vías para ver qué beneficios adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el clasificador a nivel de oración. Un modelo así podría servir como un ejemplo general para otros problemas donde el enfoque de bolsa de palabras puede establecer un modelo base, pero se necesitan enfoques más complejos para lograr un rendimiento más allá de ese modelo base. RESUMEN Y CONCLUSIONES La efectividad de la detección a nivel de oración argumenta que etiquetar a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo esto interactúa con la cantidad de datos de entrenamiento disponibles. La detección de oraciones que luego se aglomera a nivel de documento funciona sorprendentemente mejor con un bajo índice de recuperación de lo que se esperaría con elementos a nivel de oración. Esto, a su vez, indica que métodos mejorados de segmentación de oraciones podrían generar más mejoras en la clasificación. En este trabajo, examinamos cómo se pueden detectar de manera efectiva los elementos de acción en correos electrónicos. Nuestro análisis empírico ha demostrado que los n-gramos son de vital importancia para aprovechar al máximo las evaluaciones a nivel de documento. Cuando están disponibles juicios más detallados, entonces un enfoque estándar de bolsa de palabras utilizando un tamaño de ventana pequeño (de oración) y técnicas de segmentación automática puede producir resultados casi tan buenos como los enfoques basados en n-gramos. Agradecimientos: Este material se basa en trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autor(es) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) o el Centro Nacional de Negocios del Departamento del Interior (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recolección de datos fueron esenciales para la construcción del corpus, y tanto a Jill como a Aaron Steinfeld por su dirección de los experimentos de HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de corpus y a Curtis Huttenhower por su apoyo al paquete de preprocesamiento de texto. Finalmente, agradecemos sinceramente a Scott Fahlman por su apoyo y las útiles discusiones sobre este tema. 7. REFERENCIAS [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron y Y. Yang. Estudio piloto de detección y seguimiento de temas: Informe final. En Actas del Taller de Transcripción y Comprensión de Noticias de Radiodifusión de DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automatizado de reglas de decisión para la categorización de texto. ACM Transactions on Information Systems, 12(3):233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo en tareas de clasificación: La estadística kappa. Lingüística Computacional, 22(2):249-254, 1996. [4] J. Carroll. Extracción de relaciones gramaticales de alta precisión. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (COLING), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprendiendo a clasificar correos electrónicos en actos de habla. En EMNLP-2004 (Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen centrado en la tarea del correo electrónico. En \"La ramificación de la síntesis de texto: Actas del taller ACL-04\", páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extrayendo redes sociales e información de contacto de correos electrónicos y la web. En CEAS-2004 (Conferencia sobre Correo Electrónico y Anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, Actas de la 7ª Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [11] T. Joachims. Haciendo que el aprendizaje svm a gran escala sea práctico. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en Métodos de Núcleo - Aprendizaje de Vectores de Soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. \n\nMIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia de Bibliotecas Digitales de la ACM, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto. En SIGIR 92, Actas de la 15ª Conferencia Anual Internacional de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin. Un enfoque de conjunto por pares para una clasificación precisa de géneros. En Actas de la Conferencia Europea sobre Aprendizaje Automático (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell. Un estudio comparativo de núcleos para la clasificación de texto multi-etiqueta utilizando asociación de categorías. En la Vigésimo-primera Conferencia Internacional sobre Aprendizaje Automático (ICML), 2004. [16] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En Notas de Trabajo de AAAI 98 (La 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre Aprendizaje para la Categorización de Textos, páginas 41-48, 1998. TR WS-98-05. [17] F. Sebastiani. \n\nTR WS-98-05. [17] F. Sebastiani. Aprendizaje automático en la categorización automatizada de textos. ACM Computing Surveys, 34(1):1-47, marzo de 2002. [18] C. J. van Rijsbergen. Recuperación de información. Butterworths, Londres, 1979. [19] Y. Yang. Una evaluación de enfoques estadísticos para la categorización de texto. Recuperación de información, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu. Enfoques de aprendizaje para la detección y seguimiento de temas. IEEE EXPERT, Número Especial sobre Aplicaciones de la Recuperación de Información Inteligente, 1999. [21] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de textos. En SIGIR 99, Actas de la 22ª Conferencia Internacional Anual de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada por el tema. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, julio de 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "text categorization": {
            "translated_key": "categorización de texto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard <br>text categorization</br> merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "<br>text categorization</br> by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for <br>text categorization</br>.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for <br>text categorization</br>.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a <br>text categorization</br> task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for <br>text categorization</br>, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated <br>text categorization</br>.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to <br>text categorization</br>.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of <br>text categorization</br> methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "In contrast, standard <br>text categorization</br> merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "<br>text categorization</br> by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "Automated learning of decision rules for <br>text categorization</br>.",
                "Inductive learning algorithms and representations for <br>text categorization</br>.",
                "An evaluation of phrasal and clustered representations on a <br>text categorization</br> task."
            ],
            "translated_annotated_samples": [
                "Por el contrario, la <br>categorización de texto</br> estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22].",
                "La <br>categorización de texto</br> por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17].",
                "Aprendizaje automatizado de reglas de decisión para la <br>categorización de texto</br>.",
                "Algoritmos de aprendizaje inductivo y representaciones para la <br>categorización de texto</br>.",
                "Una evaluación de representaciones frasales y agrupadas en una tarea de <br>categorización de texto</br>."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la <br>categorización de texto</br> estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La <br>categorización de texto</br> por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación. En la zona de alta precisión del gráfico, el borde consistente del clasificador a nivel de oraciones es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1. Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento. La precisión F1 de los clasificadores de nivel de oración en la detección de oraciones es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí. Esto refuerza aún más la evidencia de que un clasificador alternativo con la representación de bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador Voted Perceptron mejora cuando se aumenta el número de iteraciones de entrenamiento, pero sigue siendo inferior al clasificador SVM. Los resultados de detección de oraciones se presentan en la Tabla 6. En cuanto al problema de detección de oraciones, observamos que la medida F1 proporciona una mejor idea del espacio restante para mejorar en este problema difícil. Es decir, a diferencia de la detección de documentos donde los documentos de elementos de acción son bastante comunes, las frases de elementos de acción son muy raras. Por lo tanto, al igual que en otros problemas de texto, los números de precisión son engañosamente altos simplemente debido a la precisión predeterminada alcanzable al predecir siempre negativo. Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y el ranking. Figura 4: Los usuarios encuentran los elementos de acción más rápidamente cuando son asistidos por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso construido para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventana de tres oraciones. Había tres conjuntos distintos de correos electrónicos en los que los usuarios tenían que encontrar elementos de acción. Estos conjuntos fueron presentados en un orden aleatorio (Desordenado), ordenados por el clasificador (Ordenado), o ordenados por el clasificador y con la precisión de 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recuperación de Acciones de Detección de Elementos SVM Rendimiento (Post Selección de Modelo) Unigrama de Documento N-grama de Oración Figura 3: Tanto los n-gramas como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar. la oración central en la ventana de mayor confianza resaltada (Orden+ayuda). Para realizar comparaciones justas entre condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual; es decir, la carga cognitiva de lectura debe ser aproximadamente la misma antes de reordenar los clasificadores. Además, los usuarios suelen mostrar efectos de práctica al mejorar en la tarea general y, por lo tanto, desempeñarse mejor en los conjuntos de mensajes posteriores. Esto suele ser manejado variando el orden de los conjuntos entre usuarios para que las medias sean comparables. Sin entrar en más detalles, señalamos que los conjuntos se equilibraron en cuanto al número total de fichas y se utilizó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que en intervalos de 5, 10 y 15 minutos, los usuarios encontraron consistentemente significativamente más elementos de acción cuando fueron asistidos por el clasificador, pero fueron ayudados de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda consistentemente a los usuarios, no obtuvimos un impacto adicional en los usuarios finales al resaltar. Como se mencionó anteriormente, esto podría ser resultado del amplio margen de mejora que aún existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, resaltar la oración incorrecta cerca de una acción real perjudica la confianza de los usuarios, pero si un indicador vago (por ejemplo, una flecha) señala el área aproximada de la que el usuario no está al tanto, se evita el error por poco. Dado que el usuario estudia utilizó una ventana de tres oraciones, creemos que esto también jugó un papel en la precisión de la detección de oraciones. 4.6 Discusión En contraste con problemas donde los n-gramas han mostrado poca diferencia, creemos que su poder aquí se deriva del hecho de que muchos de los n-gramas significativos para las acciones consisten en palabras comunes, por ejemplo, házmelo saber. Por lo tanto, el enfoque de unigrama a nivel de documento no puede obtener mucha ventaja, incluso al modelar correctamente su probabilidad conjunta, ya que estas palabras a menudo co-ocurrirán en el documento pero no necesariamente en una frase. Además, la detección de elementos de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden depender de la agregación de evidencia de un gran número de indicadores débiles en todo el documento. A pesar de haber descartado la información del encabezado, al examinar las características mejor clasificadas a nivel de documento, se revela que muchas de las características son nombres o partes de direcciones de correo electrónico que aparecieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden a contener muchos o ningún elemento de acción. Algunos ejemplos son términos como org, bob y gov. Observamos que estas características serán sensibles a la distribución particular (emisores/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieran menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá del modelo de bolsa de palabras puede ser la metodología, y al investigar propiedades como las curvas de aprendizaje y qué tan bien un modelo se transfiere, se pueden resaltar diferencias en modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones en las que fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin necesidad de volver a entrenarlos. 5. TRABAJO FUTURO Si bien la aplicación de clasificadores de texto a nivel de documento está bastante bien entendida, existe el potencial de aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Tales métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas a tfidf, que pueden no ser apropiadas dado que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis frásico. Además, el etiquetado de entidades nombradas, las expresiones de tiempo, etc., parecen ser candidatos probables para características que pueden mejorar aún más esta tarea. Actualmente estamos explorando algunas de estas vías para ver qué beneficios adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el clasificador a nivel de oración. Un modelo así podría servir como un ejemplo general para otros problemas donde el enfoque de bolsa de palabras puede establecer un modelo base, pero se necesitan enfoques más complejos para lograr un rendimiento más allá de ese modelo base. RESUMEN Y CONCLUSIONES La efectividad de la detección a nivel de oración argumenta que etiquetar a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo esto interactúa con la cantidad de datos de entrenamiento disponibles. La detección de oraciones que luego se aglomera a nivel de documento funciona sorprendentemente mejor con un bajo índice de recuperación de lo que se esperaría con elementos a nivel de oración. Esto, a su vez, indica que métodos mejorados de segmentación de oraciones podrían generar más mejoras en la clasificación. En este trabajo, examinamos cómo se pueden detectar de manera efectiva los elementos de acción en correos electrónicos. Nuestro análisis empírico ha demostrado que los n-gramos son de vital importancia para aprovechar al máximo las evaluaciones a nivel de documento. Cuando están disponibles juicios más detallados, entonces un enfoque estándar de bolsa de palabras utilizando un tamaño de ventana pequeño (de oración) y técnicas de segmentación automática puede producir resultados casi tan buenos como los enfoques basados en n-gramos. Agradecimientos: Este material se basa en trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autor(es) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) o el Centro Nacional de Negocios del Departamento del Interior (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recolección de datos fueron esenciales para la construcción del corpus, y tanto a Jill como a Aaron Steinfeld por su dirección de los experimentos de HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de corpus y a Curtis Huttenhower por su apoyo al paquete de preprocesamiento de texto. Finalmente, agradecemos sinceramente a Scott Fahlman por su apoyo y las útiles discusiones sobre este tema. 7. REFERENCIAS [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron y Y. Yang. Estudio piloto de detección y seguimiento de temas: Informe final. En Actas del Taller de Transcripción y Comprensión de Noticias de Radiodifusión de DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automatizado de reglas de decisión para la <br>categorización de texto</br>. ACM Transactions on Information Systems, 12(3):233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo en tareas de clasificación: La estadística kappa. Lingüística Computacional, 22(2):249-254, 1996. [4] J. Carroll. Extracción de relaciones gramaticales de alta precisión. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (COLING), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprendiendo a clasificar correos electrónicos en actos de habla. En EMNLP-2004 (Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen centrado en la tarea del correo electrónico. En \"La ramificación de la síntesis de texto: Actas del taller ACL-04\", páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extrayendo redes sociales e información de contacto de correos electrónicos y la web. En CEAS-2004 (Conferencia sobre Correo Electrónico y Anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la <br>categorización de texto</br>. En CIKM 98, Actas de la 7ª Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [11] T. Joachims. Haciendo que el aprendizaje svm a gran escala sea práctico. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en Métodos de Núcleo - Aprendizaje de Vectores de Soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. \n\nMIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia de Bibliotecas Digitales de la ACM, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de <br>categorización de texto</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "genre-classification": {
            "translated_key": "clasificación de género",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, <br>genre-classification</br>, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "In fact, <br>genre-classification</br>, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14]."
            ],
            "translated_annotated_samples": [
                "De hecho, la <br>clasificación de género</br>, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la <br>clasificación de género</br>, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación. En la zona de alta precisión del gráfico, el borde consistente del clasificador a nivel de oraciones es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1. Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento. La precisión F1 de los clasificadores de nivel de oración en la detección de oraciones es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí. Esto refuerza aún más la evidencia de que un clasificador alternativo con la representación de bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador Voted Perceptron mejora cuando se aumenta el número de iteraciones de entrenamiento, pero sigue siendo inferior al clasificador SVM. Los resultados de detección de oraciones se presentan en la Tabla 6. En cuanto al problema de detección de oraciones, observamos que la medida F1 proporciona una mejor idea del espacio restante para mejorar en este problema difícil. Es decir, a diferencia de la detección de documentos donde los documentos de elementos de acción son bastante comunes, las frases de elementos de acción son muy raras. Por lo tanto, al igual que en otros problemas de texto, los números de precisión son engañosamente altos simplemente debido a la precisión predeterminada alcanzable al predecir siempre negativo. Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y el ranking. Figura 4: Los usuarios encuentran los elementos de acción más rápidamente cuando son asistidos por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso construido para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventana de tres oraciones. Había tres conjuntos distintos de correos electrónicos en los que los usuarios tenían que encontrar elementos de acción. Estos conjuntos fueron presentados en un orden aleatorio (Desordenado), ordenados por el clasificador (Ordenado), o ordenados por el clasificador y con la precisión de 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recuperación de Acciones de Detección de Elementos SVM Rendimiento (Post Selección de Modelo) Unigrama de Documento N-grama de Oración Figura 3: Tanto los n-gramas como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar. la oración central en la ventana de mayor confianza resaltada (Orden+ayuda). Para realizar comparaciones justas entre condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual; es decir, la carga cognitiva de lectura debe ser aproximadamente la misma antes de reordenar los clasificadores. Además, los usuarios suelen mostrar efectos de práctica al mejorar en la tarea general y, por lo tanto, desempeñarse mejor en los conjuntos de mensajes posteriores. Esto suele ser manejado variando el orden de los conjuntos entre usuarios para que las medias sean comparables. Sin entrar en más detalles, señalamos que los conjuntos se equilibraron en cuanto al número total de fichas y se utilizó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que en intervalos de 5, 10 y 15 minutos, los usuarios encontraron consistentemente significativamente más elementos de acción cuando fueron asistidos por el clasificador, pero fueron ayudados de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda consistentemente a los usuarios, no obtuvimos un impacto adicional en los usuarios finales al resaltar. Como se mencionó anteriormente, esto podría ser resultado del amplio margen de mejora que aún existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, resaltar la oración incorrecta cerca de una acción real perjudica la confianza de los usuarios, pero si un indicador vago (por ejemplo, una flecha) señala el área aproximada de la que el usuario no está al tanto, se evita el error por poco. Dado que el usuario estudia utilizó una ventana de tres oraciones, creemos que esto también jugó un papel en la precisión de la detección de oraciones. 4.6 Discusión En contraste con problemas donde los n-gramas han mostrado poca diferencia, creemos que su poder aquí se deriva del hecho de que muchos de los n-gramas significativos para las acciones consisten en palabras comunes, por ejemplo, házmelo saber. Por lo tanto, el enfoque de unigrama a nivel de documento no puede obtener mucha ventaja, incluso al modelar correctamente su probabilidad conjunta, ya que estas palabras a menudo co-ocurrirán en el documento pero no necesariamente en una frase. Además, la detección de elementos de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden depender de la agregación de evidencia de un gran número de indicadores débiles en todo el documento. A pesar de haber descartado la información del encabezado, al examinar las características mejor clasificadas a nivel de documento, se revela que muchas de las características son nombres o partes de direcciones de correo electrónico que aparecieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden a contener muchos o ningún elemento de acción. Algunos ejemplos son términos como org, bob y gov. Observamos que estas características serán sensibles a la distribución particular (emisores/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieran menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá del modelo de bolsa de palabras puede ser la metodología, y al investigar propiedades como las curvas de aprendizaje y qué tan bien un modelo se transfiere, se pueden resaltar diferencias en modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones en las que fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin necesidad de volver a entrenarlos. 5. TRABAJO FUTURO Si bien la aplicación de clasificadores de texto a nivel de documento está bastante bien entendida, existe el potencial de aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Tales métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas a tfidf, que pueden no ser apropiadas dado que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis frásico. Además, el etiquetado de entidades nombradas, las expresiones de tiempo, etc., parecen ser candidatos probables para características que pueden mejorar aún más esta tarea. Actualmente estamos explorando algunas de estas vías para ver qué beneficios adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el clasificador a nivel de oración. Un modelo así podría servir como un ejemplo general para otros problemas donde el enfoque de bolsa de palabras puede establecer un modelo base, pero se necesitan enfoques más complejos para lograr un rendimiento más allá de ese modelo base. RESUMEN Y CONCLUSIONES La efectividad de la detección a nivel de oración argumenta que etiquetar a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo esto interactúa con la cantidad de datos de entrenamiento disponibles. La detección de oraciones que luego se aglomera a nivel de documento funciona sorprendentemente mejor con un bajo índice de recuperación de lo que se esperaría con elementos a nivel de oración. Esto, a su vez, indica que métodos mejorados de segmentación de oraciones podrían generar más mejoras en la clasificación. En este trabajo, examinamos cómo se pueden detectar de manera efectiva los elementos de acción en correos electrónicos. Nuestro análisis empírico ha demostrado que los n-gramos son de vital importancia para aprovechar al máximo las evaluaciones a nivel de documento. Cuando están disponibles juicios más detallados, entonces un enfoque estándar de bolsa de palabras utilizando un tamaño de ventana pequeño (de oración) y técnicas de segmentación automática puede producir resultados casi tan buenos como los enfoques basados en n-gramos. Agradecimientos: Este material se basa en trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autor(es) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) o el Centro Nacional de Negocios del Departamento del Interior (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recolección de datos fueron esenciales para la construcción del corpus, y tanto a Jill como a Aaron Steinfeld por su dirección de los experimentos de HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de corpus y a Curtis Huttenhower por su apoyo al paquete de preprocesamiento de texto. Finalmente, agradecemos sinceramente a Scott Fahlman por su apoyo y las útiles discusiones sobre este tema. 7. REFERENCIAS [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron y Y. Yang. Estudio piloto de detección y seguimiento de temas: Informe final. En Actas del Taller de Transcripción y Comprensión de Noticias de Radiodifusión de DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automatizado de reglas de decisión para la categorización de texto. ACM Transactions on Information Systems, 12(3):233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo en tareas de clasificación: La estadística kappa. Lingüística Computacional, 22(2):249-254, 1996. [4] J. Carroll. Extracción de relaciones gramaticales de alta precisión. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (COLING), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprendiendo a clasificar correos electrónicos en actos de habla. En EMNLP-2004 (Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen centrado en la tarea del correo electrónico. En \"La ramificación de la síntesis de texto: Actas del taller ACL-04\", páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extrayendo redes sociales e información de contacto de correos electrónicos y la web. En CEAS-2004 (Conferencia sobre Correo Electrónico y Anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, Actas de la 7ª Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [11] T. Joachims. Haciendo que el aprendizaje svm a gran escala sea práctico. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en Métodos de Núcleo - Aprendizaje de Vectores de Soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. \n\nMIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia de Bibliotecas Digitales de la ACM, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto. En SIGIR 92, Actas de la 15ª Conferencia Anual Internacional de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin. Un enfoque de conjunto por pares para una clasificación precisa de géneros. En Actas de la Conferencia Europea sobre Aprendizaje Automático (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell. Un estudio comparativo de núcleos para la clasificación de texto multi-etiqueta utilizando asociación de categorías. En la Vigésimo-primera Conferencia Internacional sobre Aprendizaje Automático (ICML), 2004. [16] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En Notas de Trabajo de AAAI 98 (La 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre Aprendizaje para la Categorización de Textos, páginas 41-48, 1998. TR WS-98-05. [17] F. Sebastiani. \n\nTR WS-98-05. [17] F. Sebastiani. Aprendizaje automático en la categorización automatizada de textos. ACM Computing Surveys, 34(1):1-47, marzo de 2002. [18] C. J. van Rijsbergen. Recuperación de información. Butterworths, Londres, 1979. [19] Y. Yang. Una evaluación de enfoques estadísticos para la categorización de texto. Recuperación de información, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu. Enfoques de aprendizaje para la detección y seguimiento de temas. IEEE EXPERT, Número Especial sobre Aplicaciones de la Recuperación de Información Inteligente, 1999. [21] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de textos. En SIGIR 99, Actas de la 22ª Conferencia Internacional Anual de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada por el tema. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, julio de 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "e-mail priority ranking": {
            "translated_key": "clasificación de prioridad de correos electrónicos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as <br>e-mail priority ranking</br> and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "We first review related work for similar text classification problems such as <br>e-mail priority ranking</br> and speech act identification."
            ],
            "translated_annotated_samples": [
                "Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la <br>clasificación de prioridad de correos electrónicos</br> y la identificación de actos de habla."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la <br>clasificación de prioridad de correos electrónicos</br> y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación. En la zona de alta precisión del gráfico, el borde consistente del clasificador a nivel de oraciones es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1. Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento. La precisión F1 de los clasificadores de nivel de oración en la detección de oraciones es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí. Esto refuerza aún más la evidencia de que un clasificador alternativo con la representación de bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador Voted Perceptron mejora cuando se aumenta el número de iteraciones de entrenamiento, pero sigue siendo inferior al clasificador SVM. Los resultados de detección de oraciones se presentan en la Tabla 6. En cuanto al problema de detección de oraciones, observamos que la medida F1 proporciona una mejor idea del espacio restante para mejorar en este problema difícil. Es decir, a diferencia de la detección de documentos donde los documentos de elementos de acción son bastante comunes, las frases de elementos de acción son muy raras. Por lo tanto, al igual que en otros problemas de texto, los números de precisión son engañosamente altos simplemente debido a la precisión predeterminada alcanzable al predecir siempre negativo. Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y el ranking. Figura 4: Los usuarios encuentran los elementos de acción más rápidamente cuando son asistidos por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso construido para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventana de tres oraciones. Había tres conjuntos distintos de correos electrónicos en los que los usuarios tenían que encontrar elementos de acción. Estos conjuntos fueron presentados en un orden aleatorio (Desordenado), ordenados por el clasificador (Ordenado), o ordenados por el clasificador y con la precisión de 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recuperación de Acciones de Detección de Elementos SVM Rendimiento (Post Selección de Modelo) Unigrama de Documento N-grama de Oración Figura 3: Tanto los n-gramas como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar. la oración central en la ventana de mayor confianza resaltada (Orden+ayuda). Para realizar comparaciones justas entre condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual; es decir, la carga cognitiva de lectura debe ser aproximadamente la misma antes de reordenar los clasificadores. Además, los usuarios suelen mostrar efectos de práctica al mejorar en la tarea general y, por lo tanto, desempeñarse mejor en los conjuntos de mensajes posteriores. Esto suele ser manejado variando el orden de los conjuntos entre usuarios para que las medias sean comparables. Sin entrar en más detalles, señalamos que los conjuntos se equilibraron en cuanto al número total de fichas y se utilizó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que en intervalos de 5, 10 y 15 minutos, los usuarios encontraron consistentemente significativamente más elementos de acción cuando fueron asistidos por el clasificador, pero fueron ayudados de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda consistentemente a los usuarios, no obtuvimos un impacto adicional en los usuarios finales al resaltar. Como se mencionó anteriormente, esto podría ser resultado del amplio margen de mejora que aún existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, resaltar la oración incorrecta cerca de una acción real perjudica la confianza de los usuarios, pero si un indicador vago (por ejemplo, una flecha) señala el área aproximada de la que el usuario no está al tanto, se evita el error por poco. Dado que el usuario estudia utilizó una ventana de tres oraciones, creemos que esto también jugó un papel en la precisión de la detección de oraciones. 4.6 Discusión En contraste con problemas donde los n-gramas han mostrado poca diferencia, creemos que su poder aquí se deriva del hecho de que muchos de los n-gramas significativos para las acciones consisten en palabras comunes, por ejemplo, házmelo saber. Por lo tanto, el enfoque de unigrama a nivel de documento no puede obtener mucha ventaja, incluso al modelar correctamente su probabilidad conjunta, ya que estas palabras a menudo co-ocurrirán en el documento pero no necesariamente en una frase. Además, la detección de elementos de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden depender de la agregación de evidencia de un gran número de indicadores débiles en todo el documento. A pesar de haber descartado la información del encabezado, al examinar las características mejor clasificadas a nivel de documento, se revela que muchas de las características son nombres o partes de direcciones de correo electrónico que aparecieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden a contener muchos o ningún elemento de acción. Algunos ejemplos son términos como org, bob y gov. Observamos que estas características serán sensibles a la distribución particular (emisores/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieran menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá del modelo de bolsa de palabras puede ser la metodología, y al investigar propiedades como las curvas de aprendizaje y qué tan bien un modelo se transfiere, se pueden resaltar diferencias en modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones en las que fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin necesidad de volver a entrenarlos. 5. TRABAJO FUTURO Si bien la aplicación de clasificadores de texto a nivel de documento está bastante bien entendida, existe el potencial de aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Tales métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas a tfidf, que pueden no ser apropiadas dado que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis frásico. Además, el etiquetado de entidades nombradas, las expresiones de tiempo, etc., parecen ser candidatos probables para características que pueden mejorar aún más esta tarea. Actualmente estamos explorando algunas de estas vías para ver qué beneficios adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el clasificador a nivel de oración. Un modelo así podría servir como un ejemplo general para otros problemas donde el enfoque de bolsa de palabras puede establecer un modelo base, pero se necesitan enfoques más complejos para lograr un rendimiento más allá de ese modelo base. RESUMEN Y CONCLUSIONES La efectividad de la detección a nivel de oración argumenta que etiquetar a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo esto interactúa con la cantidad de datos de entrenamiento disponibles. La detección de oraciones que luego se aglomera a nivel de documento funciona sorprendentemente mejor con un bajo índice de recuperación de lo que se esperaría con elementos a nivel de oración. Esto, a su vez, indica que métodos mejorados de segmentación de oraciones podrían generar más mejoras en la clasificación. En este trabajo, examinamos cómo se pueden detectar de manera efectiva los elementos de acción en correos electrónicos. Nuestro análisis empírico ha demostrado que los n-gramos son de vital importancia para aprovechar al máximo las evaluaciones a nivel de documento. Cuando están disponibles juicios más detallados, entonces un enfoque estándar de bolsa de palabras utilizando un tamaño de ventana pequeño (de oración) y técnicas de segmentación automática puede producir resultados casi tan buenos como los enfoques basados en n-gramos. Agradecimientos: Este material se basa en trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autor(es) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) o el Centro Nacional de Negocios del Departamento del Interior (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recolección de datos fueron esenciales para la construcción del corpus, y tanto a Jill como a Aaron Steinfeld por su dirección de los experimentos de HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de corpus y a Curtis Huttenhower por su apoyo al paquete de preprocesamiento de texto. Finalmente, agradecemos sinceramente a Scott Fahlman por su apoyo y las útiles discusiones sobre este tema. 7. REFERENCIAS [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron y Y. Yang. Estudio piloto de detección y seguimiento de temas: Informe final. En Actas del Taller de Transcripción y Comprensión de Noticias de Radiodifusión de DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automatizado de reglas de decisión para la categorización de texto. ACM Transactions on Information Systems, 12(3):233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo en tareas de clasificación: La estadística kappa. Lingüística Computacional, 22(2):249-254, 1996. [4] J. Carroll. Extracción de relaciones gramaticales de alta precisión. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (COLING), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprendiendo a clasificar correos electrónicos en actos de habla. En EMNLP-2004 (Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen centrado en la tarea del correo electrónico. En \"La ramificación de la síntesis de texto: Actas del taller ACL-04\", páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extrayendo redes sociales e información de contacto de correos electrónicos y la web. En CEAS-2004 (Conferencia sobre Correo Electrónico y Anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, Actas de la 7ª Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [11] T. Joachims. Haciendo que el aprendizaje svm a gran escala sea práctico. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en Métodos de Núcleo - Aprendizaje de Vectores de Soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. \n\nMIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia de Bibliotecas Digitales de la ACM, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto. En SIGIR 92, Actas de la 15ª Conferencia Anual Internacional de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin. Un enfoque de conjunto por pares para una clasificación precisa de géneros. En Actas de la Conferencia Europea sobre Aprendizaje Automático (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell. Un estudio comparativo de núcleos para la clasificación de texto multi-etiqueta utilizando asociación de categorías. En la Vigésimo-primera Conferencia Internacional sobre Aprendizaje Automático (ICML), 2004. [16] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En Notas de Trabajo de AAAI 98 (La 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre Aprendizaje para la Categorización de Textos, páginas 41-48, 1998. TR WS-98-05. [17] F. Sebastiani. \n\nTR WS-98-05. [17] F. Sebastiani. Aprendizaje automático en la categorización automatizada de textos. ACM Computing Surveys, 34(1):1-47, marzo de 2002. [18] C. J. van Rijsbergen. Recuperación de información. Butterworths, Londres, 1979. [19] Y. Yang. Una evaluación de enfoques estadísticos para la categorización de texto. Recuperación de información, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu. Enfoques de aprendizaje para la detección y seguimiento de temas. IEEE EXPERT, Número Especial sobre Aplicaciones de la Recuperación de Información Inteligente, 1999. [21] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de textos. En SIGIR 99, Actas de la 22ª Conferencia Internacional Anual de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada por el tema. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, julio de 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "speech act identification": {
            "translated_key": "identificación de actos de habla",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and <br>speech act identification</br>.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "We first review related work for similar text classification problems such as e-mail priority ranking and <br>speech act identification</br>."
            ],
            "translated_annotated_samples": [
                "Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la <br>identificación de actos de habla</br>."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la <br>identificación de actos de habla</br>. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación. En la zona de alta precisión del gráfico, el borde consistente del clasificador a nivel de oraciones es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1. Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento. La precisión F1 de los clasificadores de nivel de oración en la detección de oraciones es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí. Esto refuerza aún más la evidencia de que un clasificador alternativo con la representación de bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador Voted Perceptron mejora cuando se aumenta el número de iteraciones de entrenamiento, pero sigue siendo inferior al clasificador SVM. Los resultados de detección de oraciones se presentan en la Tabla 6. En cuanto al problema de detección de oraciones, observamos que la medida F1 proporciona una mejor idea del espacio restante para mejorar en este problema difícil. Es decir, a diferencia de la detección de documentos donde los documentos de elementos de acción son bastante comunes, las frases de elementos de acción son muy raras. Por lo tanto, al igual que en otros problemas de texto, los números de precisión son engañosamente altos simplemente debido a la precisión predeterminada alcanzable al predecir siempre negativo. Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y el ranking. Figura 4: Los usuarios encuentran los elementos de acción más rápidamente cuando son asistidos por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso construido para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventana de tres oraciones. Había tres conjuntos distintos de correos electrónicos en los que los usuarios tenían que encontrar elementos de acción. Estos conjuntos fueron presentados en un orden aleatorio (Desordenado), ordenados por el clasificador (Ordenado), o ordenados por el clasificador y con la precisión de 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recuperación de Acciones de Detección de Elementos SVM Rendimiento (Post Selección de Modelo) Unigrama de Documento N-grama de Oración Figura 3: Tanto los n-gramas como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar. la oración central en la ventana de mayor confianza resaltada (Orden+ayuda). Para realizar comparaciones justas entre condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual; es decir, la carga cognitiva de lectura debe ser aproximadamente la misma antes de reordenar los clasificadores. Además, los usuarios suelen mostrar efectos de práctica al mejorar en la tarea general y, por lo tanto, desempeñarse mejor en los conjuntos de mensajes posteriores. Esto suele ser manejado variando el orden de los conjuntos entre usuarios para que las medias sean comparables. Sin entrar en más detalles, señalamos que los conjuntos se equilibraron en cuanto al número total de fichas y se utilizó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que en intervalos de 5, 10 y 15 minutos, los usuarios encontraron consistentemente significativamente más elementos de acción cuando fueron asistidos por el clasificador, pero fueron ayudados de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda consistentemente a los usuarios, no obtuvimos un impacto adicional en los usuarios finales al resaltar. Como se mencionó anteriormente, esto podría ser resultado del amplio margen de mejora que aún existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, resaltar la oración incorrecta cerca de una acción real perjudica la confianza de los usuarios, pero si un indicador vago (por ejemplo, una flecha) señala el área aproximada de la que el usuario no está al tanto, se evita el error por poco. Dado que el usuario estudia utilizó una ventana de tres oraciones, creemos que esto también jugó un papel en la precisión de la detección de oraciones. 4.6 Discusión En contraste con problemas donde los n-gramas han mostrado poca diferencia, creemos que su poder aquí se deriva del hecho de que muchos de los n-gramas significativos para las acciones consisten en palabras comunes, por ejemplo, házmelo saber. Por lo tanto, el enfoque de unigrama a nivel de documento no puede obtener mucha ventaja, incluso al modelar correctamente su probabilidad conjunta, ya que estas palabras a menudo co-ocurrirán en el documento pero no necesariamente en una frase. Además, la detección de elementos de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden depender de la agregación de evidencia de un gran número de indicadores débiles en todo el documento. A pesar de haber descartado la información del encabezado, al examinar las características mejor clasificadas a nivel de documento, se revela que muchas de las características son nombres o partes de direcciones de correo electrónico que aparecieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden a contener muchos o ningún elemento de acción. Algunos ejemplos son términos como org, bob y gov. Observamos que estas características serán sensibles a la distribución particular (emisores/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieran menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá del modelo de bolsa de palabras puede ser la metodología, y al investigar propiedades como las curvas de aprendizaje y qué tan bien un modelo se transfiere, se pueden resaltar diferencias en modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones en las que fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin necesidad de volver a entrenarlos. 5. TRABAJO FUTURO Si bien la aplicación de clasificadores de texto a nivel de documento está bastante bien entendida, existe el potencial de aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Tales métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas a tfidf, que pueden no ser apropiadas dado que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis frásico. Además, el etiquetado de entidades nombradas, las expresiones de tiempo, etc., parecen ser candidatos probables para características que pueden mejorar aún más esta tarea. Actualmente estamos explorando algunas de estas vías para ver qué beneficios adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el clasificador a nivel de oración. Un modelo así podría servir como un ejemplo general para otros problemas donde el enfoque de bolsa de palabras puede establecer un modelo base, pero se necesitan enfoques más complejos para lograr un rendimiento más allá de ese modelo base. RESUMEN Y CONCLUSIONES La efectividad de la detección a nivel de oración argumenta que etiquetar a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo esto interactúa con la cantidad de datos de entrenamiento disponibles. La detección de oraciones que luego se aglomera a nivel de documento funciona sorprendentemente mejor con un bajo índice de recuperación de lo que se esperaría con elementos a nivel de oración. Esto, a su vez, indica que métodos mejorados de segmentación de oraciones podrían generar más mejoras en la clasificación. En este trabajo, examinamos cómo se pueden detectar de manera efectiva los elementos de acción en correos electrónicos. Nuestro análisis empírico ha demostrado que los n-gramos son de vital importancia para aprovechar al máximo las evaluaciones a nivel de documento. Cuando están disponibles juicios más detallados, entonces un enfoque estándar de bolsa de palabras utilizando un tamaño de ventana pequeño (de oración) y técnicas de segmentación automática puede producir resultados casi tan buenos como los enfoques basados en n-gramos. Agradecimientos: Este material se basa en trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autor(es) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) o el Centro Nacional de Negocios del Departamento del Interior (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recolección de datos fueron esenciales para la construcción del corpus, y tanto a Jill como a Aaron Steinfeld por su dirección de los experimentos de HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de corpus y a Curtis Huttenhower por su apoyo al paquete de preprocesamiento de texto. Finalmente, agradecemos sinceramente a Scott Fahlman por su apoyo y las útiles discusiones sobre este tema. 7. REFERENCIAS [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron y Y. Yang. Estudio piloto de detección y seguimiento de temas: Informe final. En Actas del Taller de Transcripción y Comprensión de Noticias de Radiodifusión de DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automatizado de reglas de decisión para la categorización de texto. ACM Transactions on Information Systems, 12(3):233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo en tareas de clasificación: La estadística kappa. Lingüística Computacional, 22(2):249-254, 1996. [4] J. Carroll. Extracción de relaciones gramaticales de alta precisión. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (COLING), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprendiendo a clasificar correos electrónicos en actos de habla. En EMNLP-2004 (Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen centrado en la tarea del correo electrónico. En \"La ramificación de la síntesis de texto: Actas del taller ACL-04\", páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extrayendo redes sociales e información de contacto de correos electrónicos y la web. En CEAS-2004 (Conferencia sobre Correo Electrónico y Anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, Actas de la 7ª Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [11] T. Joachims. Haciendo que el aprendizaje svm a gran escala sea práctico. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en Métodos de Núcleo - Aprendizaje de Vectores de Soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. \n\nMIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia de Bibliotecas Digitales de la ACM, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto. En SIGIR 92, Actas de la 15ª Conferencia Anual Internacional de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin. Un enfoque de conjunto por pares para una clasificación precisa de géneros. En Actas de la Conferencia Europea sobre Aprendizaje Automático (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell. Un estudio comparativo de núcleos para la clasificación de texto multi-etiqueta utilizando asociación de categorías. En la Vigésimo-primera Conferencia Internacional sobre Aprendizaje Automático (ICML), 2004. [16] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En Notas de Trabajo de AAAI 98 (La 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre Aprendizaje para la Categorización de Textos, páginas 41-48, 1998. TR WS-98-05. [17] F. Sebastiani. \n\nTR WS-98-05. [17] F. Sebastiani. Aprendizaje automático en la categorización automatizada de textos. ACM Computing Surveys, 34(1):1-47, marzo de 2002. [18] C. J. van Rijsbergen. Recuperación de información. Butterworths, Londres, 1979. [19] Y. Yang. Una evaluación de enfoques estadísticos para la categorización de texto. Recuperación de información, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu. Enfoques de aprendizaje para la detección y seguimiento de temas. IEEE EXPERT, Número Especial sobre Aplicaciones de la Recuperación de Información Inteligente, 1999. [21] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de textos. En SIGIR 99, Actas de la 22ª Conferencia Internacional Anual de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada por el tema. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, julio de 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "simple factual question": {
            "translated_key": "preguntas simples de hecho",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider <br>simple factual question</br>s to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "This classification task is very similar to ours except they do not consider <br>simple factual question</br>s to belong to this category."
            ],
            "translated_annotated_samples": [
                "Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las <br>preguntas simples de hecho</br> pertenezcan a esta categoría."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las <br>preguntas simples de hecho</br> pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación. En la zona de alta precisión del gráfico, el borde consistente del clasificador a nivel de oraciones es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1. Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento. La precisión F1 de los clasificadores de nivel de oración en la detección de oraciones es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí. Esto refuerza aún más la evidencia de que un clasificador alternativo con la representación de bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador Voted Perceptron mejora cuando se aumenta el número de iteraciones de entrenamiento, pero sigue siendo inferior al clasificador SVM. Los resultados de detección de oraciones se presentan en la Tabla 6. En cuanto al problema de detección de oraciones, observamos que la medida F1 proporciona una mejor idea del espacio restante para mejorar en este problema difícil. Es decir, a diferencia de la detección de documentos donde los documentos de elementos de acción son bastante comunes, las frases de elementos de acción son muy raras. Por lo tanto, al igual que en otros problemas de texto, los números de precisión son engañosamente altos simplemente debido a la precisión predeterminada alcanzable al predecir siempre negativo. Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y el ranking. Figura 4: Los usuarios encuentran los elementos de acción más rápidamente cuando son asistidos por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso construido para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventana de tres oraciones. Había tres conjuntos distintos de correos electrónicos en los que los usuarios tenían que encontrar elementos de acción. Estos conjuntos fueron presentados en un orden aleatorio (Desordenado), ordenados por el clasificador (Ordenado), o ordenados por el clasificador y con la precisión de 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recuperación de Acciones de Detección de Elementos SVM Rendimiento (Post Selección de Modelo) Unigrama de Documento N-grama de Oración Figura 3: Tanto los n-gramas como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar. la oración central en la ventana de mayor confianza resaltada (Orden+ayuda). Para realizar comparaciones justas entre condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual; es decir, la carga cognitiva de lectura debe ser aproximadamente la misma antes de reordenar los clasificadores. Además, los usuarios suelen mostrar efectos de práctica al mejorar en la tarea general y, por lo tanto, desempeñarse mejor en los conjuntos de mensajes posteriores. Esto suele ser manejado variando el orden de los conjuntos entre usuarios para que las medias sean comparables. Sin entrar en más detalles, señalamos que los conjuntos se equilibraron en cuanto al número total de fichas y se utilizó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que en intervalos de 5, 10 y 15 minutos, los usuarios encontraron consistentemente significativamente más elementos de acción cuando fueron asistidos por el clasificador, pero fueron ayudados de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda consistentemente a los usuarios, no obtuvimos un impacto adicional en los usuarios finales al resaltar. Como se mencionó anteriormente, esto podría ser resultado del amplio margen de mejora que aún existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, resaltar la oración incorrecta cerca de una acción real perjudica la confianza de los usuarios, pero si un indicador vago (por ejemplo, una flecha) señala el área aproximada de la que el usuario no está al tanto, se evita el error por poco. Dado que el usuario estudia utilizó una ventana de tres oraciones, creemos que esto también jugó un papel en la precisión de la detección de oraciones. 4.6 Discusión En contraste con problemas donde los n-gramas han mostrado poca diferencia, creemos que su poder aquí se deriva del hecho de que muchos de los n-gramas significativos para las acciones consisten en palabras comunes, por ejemplo, házmelo saber. Por lo tanto, el enfoque de unigrama a nivel de documento no puede obtener mucha ventaja, incluso al modelar correctamente su probabilidad conjunta, ya que estas palabras a menudo co-ocurrirán en el documento pero no necesariamente en una frase. Además, la detección de elementos de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden depender de la agregación de evidencia de un gran número de indicadores débiles en todo el documento. A pesar de haber descartado la información del encabezado, al examinar las características mejor clasificadas a nivel de documento, se revela que muchas de las características son nombres o partes de direcciones de correo electrónico que aparecieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden a contener muchos o ningún elemento de acción. Algunos ejemplos son términos como org, bob y gov. Observamos que estas características serán sensibles a la distribución particular (emisores/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieran menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá del modelo de bolsa de palabras puede ser la metodología, y al investigar propiedades como las curvas de aprendizaje y qué tan bien un modelo se transfiere, se pueden resaltar diferencias en modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones en las que fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin necesidad de volver a entrenarlos. 5. TRABAJO FUTURO Si bien la aplicación de clasificadores de texto a nivel de documento está bastante bien entendida, existe el potencial de aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Tales métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas a tfidf, que pueden no ser apropiadas dado que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis frásico. Además, el etiquetado de entidades nombradas, las expresiones de tiempo, etc., parecen ser candidatos probables para características que pueden mejorar aún más esta tarea. Actualmente estamos explorando algunas de estas vías para ver qué beneficios adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el clasificador a nivel de oración. Un modelo así podría servir como un ejemplo general para otros problemas donde el enfoque de bolsa de palabras puede establecer un modelo base, pero se necesitan enfoques más complejos para lograr un rendimiento más allá de ese modelo base. RESUMEN Y CONCLUSIONES La efectividad de la detección a nivel de oración argumenta que etiquetar a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo esto interactúa con la cantidad de datos de entrenamiento disponibles. La detección de oraciones que luego se aglomera a nivel de documento funciona sorprendentemente mejor con un bajo índice de recuperación de lo que se esperaría con elementos a nivel de oración. Esto, a su vez, indica que métodos mejorados de segmentación de oraciones podrían generar más mejoras en la clasificación. En este trabajo, examinamos cómo se pueden detectar de manera efectiva los elementos de acción en correos electrónicos. Nuestro análisis empírico ha demostrado que los n-gramos son de vital importancia para aprovechar al máximo las evaluaciones a nivel de documento. Cuando están disponibles juicios más detallados, entonces un enfoque estándar de bolsa de palabras utilizando un tamaño de ventana pequeño (de oración) y técnicas de segmentación automática puede producir resultados casi tan buenos como los enfoques basados en n-gramos. Agradecimientos: Este material se basa en trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autor(es) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) o el Centro Nacional de Negocios del Departamento del Interior (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recolección de datos fueron esenciales para la construcción del corpus, y tanto a Jill como a Aaron Steinfeld por su dirección de los experimentos de HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de corpus y a Curtis Huttenhower por su apoyo al paquete de preprocesamiento de texto. Finalmente, agradecemos sinceramente a Scott Fahlman por su apoyo y las útiles discusiones sobre este tema. 7. REFERENCIAS [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron y Y. Yang. Estudio piloto de detección y seguimiento de temas: Informe final. En Actas del Taller de Transcripción y Comprensión de Noticias de Radiodifusión de DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automatizado de reglas de decisión para la categorización de texto. ACM Transactions on Information Systems, 12(3):233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo en tareas de clasificación: La estadística kappa. Lingüística Computacional, 22(2):249-254, 1996. [4] J. Carroll. Extracción de relaciones gramaticales de alta precisión. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (COLING), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprendiendo a clasificar correos electrónicos en actos de habla. En EMNLP-2004 (Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen centrado en la tarea del correo electrónico. En \"La ramificación de la síntesis de texto: Actas del taller ACL-04\", páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extrayendo redes sociales e información de contacto de correos electrónicos y la web. En CEAS-2004 (Conferencia sobre Correo Electrónico y Anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, Actas de la 7ª Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [11] T. Joachims. Haciendo que el aprendizaje svm a gran escala sea práctico. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en Métodos de Núcleo - Aprendizaje de Vectores de Soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. \n\nMIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia de Bibliotecas Digitales de la ACM, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto. En SIGIR 92, Actas de la 15ª Conferencia Anual Internacional de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin. Un enfoque de conjunto por pares para una clasificación precisa de géneros. En Actas de la Conferencia Europea sobre Aprendizaje Automático (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell. Un estudio comparativo de núcleos para la clasificación de texto multi-etiqueta utilizando asociación de categorías. En la Vigésimo-primera Conferencia Internacional sobre Aprendizaje Automático (ICML), 2004. [16] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En Notas de Trabajo de AAAI 98 (La 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre Aprendizaje para la Categorización de Textos, páginas 41-48, 1998. TR WS-98-05. [17] F. Sebastiani. \n\nTR WS-98-05. [17] F. Sebastiani. Aprendizaje automático en la categorización automatizada de textos. ACM Computing Surveys, 34(1):1-47, marzo de 2002. [18] C. J. van Rijsbergen. Recuperación de información. Butterworths, Londres, 1979. [19] Y. Yang. Una evaluación de enfoques estadísticos para la categorización de texto. Recuperación de información, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu. Enfoques de aprendizaje para la detección y seguimiento de temas. IEEE EXPERT, Número Especial sobre Aplicaciones de la Recuperación de Información Inteligente, 1999. [21] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de textos. En SIGIR 99, Actas de la 22ª Conferencia Internacional Anual de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada por el tema. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, julio de 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "document detection": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "<br>document detection</br>: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to <br>document detection</br> and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to <br>document detection</br> and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for <br>document detection</br> using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the <br>document detection</br> problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike <br>document detection</br> where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "<br>document detection</br>: Classify a document as to whether or not it contains an action-item. 2.",
                "Applying a document-level classifier to <br>document detection</br> and ranking is straightforward.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to <br>document detection</br> and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for <br>document detection</br> using document-level and sentence-level classifiers.",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the <br>document detection</br> problem."
            ],
            "translated_annotated_samples": [
                "Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2.",
                "Aplicar un clasificador a nivel de documento para la <br>detección y clasificación de documentos</br> es sencillo.",
                "Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la <br>detección de documentos</br> y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento.",
                "Tabla 4: Resultados de significancia para n-gramas versus unigramas para la <br>detección de documentos</br> utilizando clasificadores a nivel de documento y a nivel de oración.",
                "Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de <br>detección de documentos</br>."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la <br>detección y clasificación de documentos</br> es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la <br>detección de documentos</br> y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la <br>detección de documentos</br> utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de <br>detección de documentos</br>. ",
            "candidates": [],
            "error": [
                [
                    "detección y clasificación de documentos",
                    "detección de documentos",
                    "detección de documentos",
                    "detección de documentos"
                ]
            ]
        },
        "document ranking": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "<br>document ranking</br>: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and <br>document ranking</br>, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to <br>document ranking</br> and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "<br>document ranking</br>: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and <br>document ranking</br>, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to <br>document ranking</br> and classification."
            ],
            "translated_annotated_samples": [
                "Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3.",
                "Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la <br>clasificación de documentos</br>, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento.",
                "Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para <br>documentar la clasificación</br> y el ranking."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la <br>clasificación de documentos</br>, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación. En la zona de alta precisión del gráfico, el borde consistente del clasificador a nivel de oraciones es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1. Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento. La precisión F1 de los clasificadores de nivel de oración en la detección de oraciones es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí. Esto refuerza aún más la evidencia de que un clasificador alternativo con la representación de bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador Voted Perceptron mejora cuando se aumenta el número de iteraciones de entrenamiento, pero sigue siendo inferior al clasificador SVM. Los resultados de detección de oraciones se presentan en la Tabla 6. En cuanto al problema de detección de oraciones, observamos que la medida F1 proporciona una mejor idea del espacio restante para mejorar en este problema difícil. Es decir, a diferencia de la detección de documentos donde los documentos de elementos de acción son bastante comunes, las frases de elementos de acción son muy raras. Por lo tanto, al igual que en otros problemas de texto, los números de precisión son engañosamente altos simplemente debido a la precisión predeterminada alcanzable al predecir siempre negativo. Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para <br>documentar la clasificación</br> y el ranking. Figura 4: Los usuarios encuentran los elementos de acción más rápidamente cuando son asistidos por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso construido para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventana de tres oraciones. Había tres conjuntos distintos de correos electrónicos en los que los usuarios tenían que encontrar elementos de acción. Estos conjuntos fueron presentados en un orden aleatorio (Desordenado), ordenados por el clasificador (Ordenado), o ordenados por el clasificador y con la precisión de 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recuperación de Acciones de Detección de Elementos SVM Rendimiento (Post Selección de Modelo) Unigrama de Documento N-grama de Oración Figura 3: Tanto los n-gramas como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar. la oración central en la ventana de mayor confianza resaltada (Orden+ayuda). Para realizar comparaciones justas entre condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual; es decir, la carga cognitiva de lectura debe ser aproximadamente la misma antes de reordenar los clasificadores. Además, los usuarios suelen mostrar efectos de práctica al mejorar en la tarea general y, por lo tanto, desempeñarse mejor en los conjuntos de mensajes posteriores. Esto suele ser manejado variando el orden de los conjuntos entre usuarios para que las medias sean comparables. Sin entrar en más detalles, señalamos que los conjuntos se equilibraron en cuanto al número total de fichas y se utilizó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que en intervalos de 5, 10 y 15 minutos, los usuarios encontraron consistentemente significativamente más elementos de acción cuando fueron asistidos por el clasificador, pero fueron ayudados de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda consistentemente a los usuarios, no obtuvimos un impacto adicional en los usuarios finales al resaltar. Como se mencionó anteriormente, esto podría ser resultado del amplio margen de mejora que aún existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, resaltar la oración incorrecta cerca de una acción real perjudica la confianza de los usuarios, pero si un indicador vago (por ejemplo, una flecha) señala el área aproximada de la que el usuario no está al tanto, se evita el error por poco. Dado que el usuario estudia utilizó una ventana de tres oraciones, creemos que esto también jugó un papel en la precisión de la detección de oraciones. 4.6 Discusión En contraste con problemas donde los n-gramas han mostrado poca diferencia, creemos que su poder aquí se deriva del hecho de que muchos de los n-gramas significativos para las acciones consisten en palabras comunes, por ejemplo, házmelo saber. Por lo tanto, el enfoque de unigrama a nivel de documento no puede obtener mucha ventaja, incluso al modelar correctamente su probabilidad conjunta, ya que estas palabras a menudo co-ocurrirán en el documento pero no necesariamente en una frase. Además, la detección de elementos de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden depender de la agregación de evidencia de un gran número de indicadores débiles en todo el documento. A pesar de haber descartado la información del encabezado, al examinar las características mejor clasificadas a nivel de documento, se revela que muchas de las características son nombres o partes de direcciones de correo electrónico que aparecieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden a contener muchos o ningún elemento de acción. Algunos ejemplos son términos como org, bob y gov. Observamos que estas características serán sensibles a la distribución particular (emisores/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieran menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá del modelo de bolsa de palabras puede ser la metodología, y al investigar propiedades como las curvas de aprendizaje y qué tan bien un modelo se transfiere, se pueden resaltar diferencias en modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones en las que fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin necesidad de volver a entrenarlos. 5. TRABAJO FUTURO Si bien la aplicación de clasificadores de texto a nivel de documento está bastante bien entendida, existe el potencial de aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Tales métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas a tfidf, que pueden no ser apropiadas dado que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis frásico. Además, el etiquetado de entidades nombradas, las expresiones de tiempo, etc., parecen ser candidatos probables para características que pueden mejorar aún más esta tarea. Actualmente estamos explorando algunas de estas vías para ver qué beneficios adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el clasificador a nivel de oración. Un modelo así podría servir como un ejemplo general para otros problemas donde el enfoque de bolsa de palabras puede establecer un modelo base, pero se necesitan enfoques más complejos para lograr un rendimiento más allá de ese modelo base. RESUMEN Y CONCLUSIONES La efectividad de la detección a nivel de oración argumenta que etiquetar a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo esto interactúa con la cantidad de datos de entrenamiento disponibles. La detección de oraciones que luego se aglomera a nivel de documento funciona sorprendentemente mejor con un bajo índice de recuperación de lo que se esperaría con elementos a nivel de oración. Esto, a su vez, indica que métodos mejorados de segmentación de oraciones podrían generar más mejoras en la clasificación. En este trabajo, examinamos cómo se pueden detectar de manera efectiva los elementos de acción en correos electrónicos. Nuestro análisis empírico ha demostrado que los n-gramos son de vital importancia para aprovechar al máximo las evaluaciones a nivel de documento. Cuando están disponibles juicios más detallados, entonces un enfoque estándar de bolsa de palabras utilizando un tamaño de ventana pequeño (de oración) y técnicas de segmentación automática puede producir resultados casi tan buenos como los enfoques basados en n-gramos. Agradecimientos: Este material se basa en trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autor(es) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) o el Centro Nacional de Negocios del Departamento del Interior (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recolección de datos fueron esenciales para la construcción del corpus, y tanto a Jill como a Aaron Steinfeld por su dirección de los experimentos de HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de corpus y a Curtis Huttenhower por su apoyo al paquete de preprocesamiento de texto. Finalmente, agradecemos sinceramente a Scott Fahlman por su apoyo y las útiles discusiones sobre este tema. 7. REFERENCIAS [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron y Y. Yang. Estudio piloto de detección y seguimiento de temas: Informe final. En Actas del Taller de Transcripción y Comprensión de Noticias de Radiodifusión de DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automatizado de reglas de decisión para la categorización de texto. ACM Transactions on Information Systems, 12(3):233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo en tareas de clasificación: La estadística kappa. Lingüística Computacional, 22(2):249-254, 1996. [4] J. Carroll. Extracción de relaciones gramaticales de alta precisión. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (COLING), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprendiendo a clasificar correos electrónicos en actos de habla. En EMNLP-2004 (Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen centrado en la tarea del correo electrónico. En \"La ramificación de la síntesis de texto: Actas del taller ACL-04\", páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extrayendo redes sociales e información de contacto de correos electrónicos y la web. En CEAS-2004 (Conferencia sobre Correo Electrónico y Anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, Actas de la 7ª Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [11] T. Joachims. Haciendo que el aprendizaje svm a gran escala sea práctico. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en Métodos de Núcleo - Aprendizaje de Vectores de Soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. \n\nMIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia de Bibliotecas Digitales de la ACM, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto. En SIGIR 92, Actas de la 15ª Conferencia Anual Internacional de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin. Un enfoque de conjunto por pares para una clasificación precisa de géneros. En Actas de la Conferencia Europea sobre Aprendizaje Automático (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell. Un estudio comparativo de núcleos para la clasificación de texto multi-etiqueta utilizando asociación de categorías. En la Vigésimo-primera Conferencia Internacional sobre Aprendizaje Automático (ICML), 2004. [16] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En Notas de Trabajo de AAAI 98 (La 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre Aprendizaje para la Categorización de Textos, páginas 41-48, 1998. TR WS-98-05. [17] F. Sebastiani. \n\nTR WS-98-05. [17] F. Sebastiani. Aprendizaje automático en la categorización automatizada de textos. ACM Computing Surveys, 34(1):1-47, marzo de 2002. [18] C. J. van Rijsbergen. Recuperación de información. Butterworths, Londres, 1979. [19] Y. Yang. Una evaluación de enfoques estadísticos para la categorización de texto. Recuperación de información, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu. Enfoques de aprendizaje para la detección y seguimiento de temas. IEEE EXPERT, Número Especial sobre Aplicaciones de la Recuperación de Información Inteligente, 1999. [21] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de textos. En SIGIR 99, Actas de la 22ª Conferencia Internacional Anual de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada por el tema. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, julio de 2002. ",
            "candidates": [],
            "error": [
                [
                    "clasificación de documentos",
                    "documentar la clasificación"
                ]
            ]
        },
        "sentence detection": {
            "translated_key": "detección de oraciones",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "<br>sentence detection</br>: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, <br>sentence detection</br> plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to <br>sentence detection</br>, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to <br>sentence detection</br> is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at <br>sentence detection</br> Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "<br>sentence detection</br> results are presented in Table 6.",
                "With regard to the <br>sentence detection</br> problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for <br>sentence detection</br> to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for <br>sentence detection</br>, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of <br>sentence detection</br>.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as <br>sentence detection</br> accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "<br>sentence detection</br> that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "<br>sentence detection</br>: Classify each sentence in a document as to whether or not it is an action-item.",
                "Finally, <br>sentence detection</br> plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "In order to apply it to <br>sentence detection</br>, one must make additional steps.",
                "Once trained, applying the resulting classifiers to <br>sentence detection</br> is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at <br>sentence detection</br> Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here."
            ],
            "translated_annotated_samples": [
                "Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no.",
                "Finalmente, la <br>detección de oraciones</br> juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés.",
                "Para aplicarlo a la <br>detección de oraciones</br>, se deben seguir pasos adicionales.",
                "Una vez entrenados, aplicar los clasificadores resultantes a la <br>detección de oraciones</br> es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento.",
                "La precisión F1 de los clasificadores de nivel de oración en la <br>detección de oraciones</br> es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la <br>detección de oraciones</br> juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la <br>detección de oraciones</br>, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la <br>detección de oraciones</br> es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación. En la zona de alta precisión del gráfico, el borde consistente del clasificador a nivel de oraciones es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1. Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento. La precisión F1 de los clasificadores de nivel de oración en la <br>detección de oraciones</br> es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "sentence-level classifier": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a <br>sentence-level classifier</br> has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM <br>sentence-level classifier</br> has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the <br>sentence-level classifier</br> is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the <br>sentence-level classifier</br>.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "Once a <br>sentence-level classifier</br> has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM <br>sentence-level classifier</br> has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the <br>sentence-level classifier</br> is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the <br>sentence-level classifier</br>."
            ],
            "translated_annotated_samples": [
                "Una vez que un <br>clasificador a nivel de oración</br> ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento.",
                "Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación.",
                "En la zona de alta precisión del gráfico, el borde consistente del <br>clasificador a nivel de oraciones</br> es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1.",
                "Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el <br>clasificador a nivel de oración</br>."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un <br>clasificador a nivel de oración</br> ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación. En la zona de alta precisión del gráfico, el borde consistente del <br>clasificador a nivel de oraciones</br> es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1. Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento. La precisión F1 de los clasificadores de nivel de oración en la detección de oraciones es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí. Esto refuerza aún más la evidencia de que un clasificador alternativo con la representación de bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador Voted Perceptron mejora cuando se aumenta el número de iteraciones de entrenamiento, pero sigue siendo inferior al clasificador SVM. Los resultados de detección de oraciones se presentan en la Tabla 6. En cuanto al problema de detección de oraciones, observamos que la medida F1 proporciona una mejor idea del espacio restante para mejorar en este problema difícil. Es decir, a diferencia de la detección de documentos donde los documentos de elementos de acción son bastante comunes, las frases de elementos de acción son muy raras. Por lo tanto, al igual que en otros problemas de texto, los números de precisión son engañosamente altos simplemente debido a la precisión predeterminada alcanzable al predecir siempre negativo. Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y el ranking. Figura 4: Los usuarios encuentran los elementos de acción más rápidamente cuando son asistidos por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso construido para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventana de tres oraciones. Había tres conjuntos distintos de correos electrónicos en los que los usuarios tenían que encontrar elementos de acción. Estos conjuntos fueron presentados en un orden aleatorio (Desordenado), ordenados por el clasificador (Ordenado), o ordenados por el clasificador y con la precisión de 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recuperación de Acciones de Detección de Elementos SVM Rendimiento (Post Selección de Modelo) Unigrama de Documento N-grama de Oración Figura 3: Tanto los n-gramas como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar. la oración central en la ventana de mayor confianza resaltada (Orden+ayuda). Para realizar comparaciones justas entre condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual; es decir, la carga cognitiva de lectura debe ser aproximadamente la misma antes de reordenar los clasificadores. Además, los usuarios suelen mostrar efectos de práctica al mejorar en la tarea general y, por lo tanto, desempeñarse mejor en los conjuntos de mensajes posteriores. Esto suele ser manejado variando el orden de los conjuntos entre usuarios para que las medias sean comparables. Sin entrar en más detalles, señalamos que los conjuntos se equilibraron en cuanto al número total de fichas y se utilizó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que en intervalos de 5, 10 y 15 minutos, los usuarios encontraron consistentemente significativamente más elementos de acción cuando fueron asistidos por el clasificador, pero fueron ayudados de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda consistentemente a los usuarios, no obtuvimos un impacto adicional en los usuarios finales al resaltar. Como se mencionó anteriormente, esto podría ser resultado del amplio margen de mejora que aún existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, resaltar la oración incorrecta cerca de una acción real perjudica la confianza de los usuarios, pero si un indicador vago (por ejemplo, una flecha) señala el área aproximada de la que el usuario no está al tanto, se evita el error por poco. Dado que el usuario estudia utilizó una ventana de tres oraciones, creemos que esto también jugó un papel en la precisión de la detección de oraciones. 4.6 Discusión En contraste con problemas donde los n-gramas han mostrado poca diferencia, creemos que su poder aquí se deriva del hecho de que muchos de los n-gramas significativos para las acciones consisten en palabras comunes, por ejemplo, házmelo saber. Por lo tanto, el enfoque de unigrama a nivel de documento no puede obtener mucha ventaja, incluso al modelar correctamente su probabilidad conjunta, ya que estas palabras a menudo co-ocurrirán en el documento pero no necesariamente en una frase. Además, la detección de elementos de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden depender de la agregación de evidencia de un gran número de indicadores débiles en todo el documento. A pesar de haber descartado la información del encabezado, al examinar las características mejor clasificadas a nivel de documento, se revela que muchas de las características son nombres o partes de direcciones de correo electrónico que aparecieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden a contener muchos o ningún elemento de acción. Algunos ejemplos son términos como org, bob y gov. Observamos que estas características serán sensibles a la distribución particular (emisores/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieran menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá del modelo de bolsa de palabras puede ser la metodología, y al investigar propiedades como las curvas de aprendizaje y qué tan bien un modelo se transfiere, se pueden resaltar diferencias en modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones en las que fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin necesidad de volver a entrenarlos. 5. TRABAJO FUTURO Si bien la aplicación de clasificadores de texto a nivel de documento está bastante bien entendida, existe el potencial de aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Tales métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas a tfidf, que pueden no ser apropiadas dado que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis frásico. Además, el etiquetado de entidades nombradas, las expresiones de tiempo, etc., parecen ser candidatos probables para características que pueden mejorar aún más esta tarea. Actualmente estamos explorando algunas de estas vías para ver qué beneficios adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el <br>clasificador a nivel de oración</br>. Un modelo así podría servir como un ejemplo general para otros problemas donde el enfoque de bolsa de palabras puede establecer un modelo base, pero se necesitan enfoques más complejos para lograr un rendimiento más allá de ese modelo base. RESUMEN Y CONCLUSIONES La efectividad de la detección a nivel de oración argumenta que etiquetar a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo esto interactúa con la cantidad de datos de entrenamiento disponibles. La detección de oraciones que luego se aglomera a nivel de documento funciona sorprendentemente mejor con un bajo índice de recuperación de lo que se esperaría con elementos a nivel de oración. Esto, a su vez, indica que métodos mejorados de segmentación de oraciones podrían generar más mejoras en la clasificación. En este trabajo, examinamos cómo se pueden detectar de manera efectiva los elementos de acción en correos electrónicos. Nuestro análisis empírico ha demostrado que los n-gramos son de vital importancia para aprovechar al máximo las evaluaciones a nivel de documento. Cuando están disponibles juicios más detallados, entonces un enfoque estándar de bolsa de palabras utilizando un tamaño de ventana pequeño (de oración) y técnicas de segmentación automática puede producir resultados casi tan buenos como los enfoques basados en n-gramos. Agradecimientos: Este material se basa en trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autor(es) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) o el Centro Nacional de Negocios del Departamento del Interior (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recolección de datos fueron esenciales para la construcción del corpus, y tanto a Jill como a Aaron Steinfeld por su dirección de los experimentos de HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de corpus y a Curtis Huttenhower por su apoyo al paquete de preprocesamiento de texto. Finalmente, agradecemos sinceramente a Scott Fahlman por su apoyo y las útiles discusiones sobre este tema. 7. REFERENCIAS [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron y Y. Yang. Estudio piloto de detección y seguimiento de temas: Informe final. En Actas del Taller de Transcripción y Comprensión de Noticias de Radiodifusión de DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automatizado de reglas de decisión para la categorización de texto. ACM Transactions on Information Systems, 12(3):233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo en tareas de clasificación: La estadística kappa. Lingüística Computacional, 22(2):249-254, 1996. [4] J. Carroll. Extracción de relaciones gramaticales de alta precisión. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (COLING), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprendiendo a clasificar correos electrónicos en actos de habla. En EMNLP-2004 (Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen centrado en la tarea del correo electrónico. En \"La ramificación de la síntesis de texto: Actas del taller ACL-04\", páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extrayendo redes sociales e información de contacto de correos electrónicos y la web. En CEAS-2004 (Conferencia sobre Correo Electrónico y Anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, Actas de la 7ª Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [11] T. Joachims. Haciendo que el aprendizaje svm a gran escala sea práctico. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en Métodos de Núcleo - Aprendizaje de Vectores de Soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. \n\nMIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia de Bibliotecas Digitales de la ACM, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto. En SIGIR 92, Actas de la 15ª Conferencia Anual Internacional de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin. Un enfoque de conjunto por pares para una clasificación precisa de géneros. En Actas de la Conferencia Europea sobre Aprendizaje Automático (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell. Un estudio comparativo de núcleos para la clasificación de texto multi-etiqueta utilizando asociación de categorías. En la Vigésimo-primera Conferencia Internacional sobre Aprendizaje Automático (ICML), 2004. [16] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En Notas de Trabajo de AAAI 98 (La 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre Aprendizaje para la Categorización de Textos, páginas 41-48, 1998. TR WS-98-05. [17] F. Sebastiani. \n\nTR WS-98-05. [17] F. Sebastiani. Aprendizaje automático en la categorización automatizada de textos. ACM Computing Surveys, 34(1):1-47, marzo de 2002. [18] C. J. van Rijsbergen. Recuperación de información. Butterworths, Londres, 1979. [19] Y. Yang. Una evaluación de enfoques estadísticos para la categorización de texto. Recuperación de información, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu. Enfoques de aprendizaje para la detección y seguimiento de temas. IEEE EXPERT, Número Especial sobre Aplicaciones de la Recuperación de Información Inteligente, 1999. [21] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de textos. En SIGIR 99, Actas de la 22ª Conferencia Internacional Anual de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada por el tema. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, julio de 2002. ",
            "candidates": [],
            "error": [
                [
                    "clasificador a nivel de oración",
                    "clasificador a nivel de oraciones",
                    "clasificador a nivel de oración"
                ]
            ]
        },
        "text classification": {
            "translated_key": "clasificación de texto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven <br>text classification</br>, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard <br>text classification</br> in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar <br>text classification</br> problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard <br>text classification</br> approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar <br>text classification</br> tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard <br>text classification</br> approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard <br>text classification</br> algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in <br>text classification</br>, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many <br>text classification</br> tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label <br>text classification</br> using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes <br>text classification</br>.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "Unlike standard topic-driven <br>text classification</br>, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "Action-item detection differs from standard <br>text classification</br> in two important ways.",
                "We first review related work for similar <br>text classification</br> problems such as e-mail priority ranking and speech act identification.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard <br>text classification</br> approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "RELATED WORK Several other researchers have considered very similar <br>text classification</br> tasks."
            ],
            "translated_annotated_samples": [
                "A diferencia de la <br>clasificación de texto</br> estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras.",
                "La detección de elementos de acción difiere de la <br>clasificación de texto</br> estándar en dos formas importantes.",
                "Primero revisamos trabajos relacionados para problemas de <br>clasificación de texto</br> similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla.",
                "A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de <br>clasificación de texto</br> pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento.",
                "TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de <br>clasificación de texto</br> muy similares."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la <br>clasificación de texto</br> estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la <br>clasificación de texto</br> estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de <br>clasificación de texto</br> similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de <br>clasificación de texto</br> pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de <br>clasificación de texto</br> muy similares. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "speech act": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and <br>speech act</br> identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of <br>speech act</br> that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "We first review related work for similar text classification problems such as e-mail priority ranking and <br>speech act</br> identification.",
                "We consider action-items to be an important specific type of <br>speech act</br> that falls within their more general classification."
            ],
            "translated_annotated_samples": [
                "Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de <br>actos de habla</br>.",
                "Consideramos que los elementos de acción son un tipo específico importante de <br>acto de habla</br> que se encuentra dentro de su clasificación más general."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de <br>actos de habla</br>. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de <br>acto de habla</br> que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación. En la zona de alta precisión del gráfico, el borde consistente del clasificador a nivel de oraciones es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1. Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento. La precisión F1 de los clasificadores de nivel de oración en la detección de oraciones es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí. Esto refuerza aún más la evidencia de que un clasificador alternativo con la representación de bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador Voted Perceptron mejora cuando se aumenta el número de iteraciones de entrenamiento, pero sigue siendo inferior al clasificador SVM. Los resultados de detección de oraciones se presentan en la Tabla 6. En cuanto al problema de detección de oraciones, observamos que la medida F1 proporciona una mejor idea del espacio restante para mejorar en este problema difícil. Es decir, a diferencia de la detección de documentos donde los documentos de elementos de acción son bastante comunes, las frases de elementos de acción son muy raras. Por lo tanto, al igual que en otros problemas de texto, los números de precisión son engañosamente altos simplemente debido a la precisión predeterminada alcanzable al predecir siempre negativo. Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y el ranking. Figura 4: Los usuarios encuentran los elementos de acción más rápidamente cuando son asistidos por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso construido para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventana de tres oraciones. Había tres conjuntos distintos de correos electrónicos en los que los usuarios tenían que encontrar elementos de acción. Estos conjuntos fueron presentados en un orden aleatorio (Desordenado), ordenados por el clasificador (Ordenado), o ordenados por el clasificador y con la precisión de 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recuperación de Acciones de Detección de Elementos SVM Rendimiento (Post Selección de Modelo) Unigrama de Documento N-grama de Oración Figura 3: Tanto los n-gramas como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar. la oración central en la ventana de mayor confianza resaltada (Orden+ayuda). Para realizar comparaciones justas entre condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual; es decir, la carga cognitiva de lectura debe ser aproximadamente la misma antes de reordenar los clasificadores. Además, los usuarios suelen mostrar efectos de práctica al mejorar en la tarea general y, por lo tanto, desempeñarse mejor en los conjuntos de mensajes posteriores. Esto suele ser manejado variando el orden de los conjuntos entre usuarios para que las medias sean comparables. Sin entrar en más detalles, señalamos que los conjuntos se equilibraron en cuanto al número total de fichas y se utilizó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que en intervalos de 5, 10 y 15 minutos, los usuarios encontraron consistentemente significativamente más elementos de acción cuando fueron asistidos por el clasificador, pero fueron ayudados de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda consistentemente a los usuarios, no obtuvimos un impacto adicional en los usuarios finales al resaltar. Como se mencionó anteriormente, esto podría ser resultado del amplio margen de mejora que aún existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, resaltar la oración incorrecta cerca de una acción real perjudica la confianza de los usuarios, pero si un indicador vago (por ejemplo, una flecha) señala el área aproximada de la que el usuario no está al tanto, se evita el error por poco. Dado que el usuario estudia utilizó una ventana de tres oraciones, creemos que esto también jugó un papel en la precisión de la detección de oraciones. 4.6 Discusión En contraste con problemas donde los n-gramas han mostrado poca diferencia, creemos que su poder aquí se deriva del hecho de que muchos de los n-gramas significativos para las acciones consisten en palabras comunes, por ejemplo, házmelo saber. Por lo tanto, el enfoque de unigrama a nivel de documento no puede obtener mucha ventaja, incluso al modelar correctamente su probabilidad conjunta, ya que estas palabras a menudo co-ocurrirán en el documento pero no necesariamente en una frase. Además, la detección de elementos de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden depender de la agregación de evidencia de un gran número de indicadores débiles en todo el documento. A pesar de haber descartado la información del encabezado, al examinar las características mejor clasificadas a nivel de documento, se revela que muchas de las características son nombres o partes de direcciones de correo electrónico que aparecieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden a contener muchos o ningún elemento de acción. Algunos ejemplos son términos como org, bob y gov. Observamos que estas características serán sensibles a la distribución particular (emisores/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieran menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá del modelo de bolsa de palabras puede ser la metodología, y al investigar propiedades como las curvas de aprendizaje y qué tan bien un modelo se transfiere, se pueden resaltar diferencias en modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones en las que fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin necesidad de volver a entrenarlos. 5. TRABAJO FUTURO Si bien la aplicación de clasificadores de texto a nivel de documento está bastante bien entendida, existe el potencial de aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Tales métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas a tfidf, que pueden no ser apropiadas dado que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis frásico. Además, el etiquetado de entidades nombradas, las expresiones de tiempo, etc., parecen ser candidatos probables para características que pueden mejorar aún más esta tarea. Actualmente estamos explorando algunas de estas vías para ver qué beneficios adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el clasificador a nivel de oración. Un modelo así podría servir como un ejemplo general para otros problemas donde el enfoque de bolsa de palabras puede establecer un modelo base, pero se necesitan enfoques más complejos para lograr un rendimiento más allá de ese modelo base. RESUMEN Y CONCLUSIONES La efectividad de la detección a nivel de oración argumenta que etiquetar a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo esto interactúa con la cantidad de datos de entrenamiento disponibles. La detección de oraciones que luego se aglomera a nivel de documento funciona sorprendentemente mejor con un bajo índice de recuperación de lo que se esperaría con elementos a nivel de oración. Esto, a su vez, indica que métodos mejorados de segmentación de oraciones podrían generar más mejoras en la clasificación. En este trabajo, examinamos cómo se pueden detectar de manera efectiva los elementos de acción en correos electrónicos. Nuestro análisis empírico ha demostrado que los n-gramos son de vital importancia para aprovechar al máximo las evaluaciones a nivel de documento. Cuando están disponibles juicios más detallados, entonces un enfoque estándar de bolsa de palabras utilizando un tamaño de ventana pequeño (de oración) y técnicas de segmentación automática puede producir resultados casi tan buenos como los enfoques basados en n-gramos. Agradecimientos: Este material se basa en trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autor(es) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) o el Centro Nacional de Negocios del Departamento del Interior (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recolección de datos fueron esenciales para la construcción del corpus, y tanto a Jill como a Aaron Steinfeld por su dirección de los experimentos de HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de corpus y a Curtis Huttenhower por su apoyo al paquete de preprocesamiento de texto. Finalmente, agradecemos sinceramente a Scott Fahlman por su apoyo y las útiles discusiones sobre este tema. 7. REFERENCIAS [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron y Y. Yang. Estudio piloto de detección y seguimiento de temas: Informe final. En Actas del Taller de Transcripción y Comprensión de Noticias de Radiodifusión de DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automatizado de reglas de decisión para la categorización de texto. ACM Transactions on Information Systems, 12(3):233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo en tareas de clasificación: La estadística kappa. Lingüística Computacional, 22(2):249-254, 1996. [4] J. Carroll. Extracción de relaciones gramaticales de alta precisión. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (COLING), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprendiendo a clasificar correos electrónicos en actos de habla. En EMNLP-2004 (Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen centrado en la tarea del correo electrónico. En \"La ramificación de la síntesis de texto: Actas del taller ACL-04\", páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extrayendo redes sociales e información de contacto de correos electrónicos y la web. En CEAS-2004 (Conferencia sobre Correo Electrónico y Anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, Actas de la 7ª Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [11] T. Joachims. Haciendo que el aprendizaje svm a gran escala sea práctico. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en Métodos de Núcleo - Aprendizaje de Vectores de Soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. \n\nMIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia de Bibliotecas Digitales de la ACM, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto. En SIGIR 92, Actas de la 15ª Conferencia Anual Internacional de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin. Un enfoque de conjunto por pares para una clasificación precisa de géneros. En Actas de la Conferencia Europea sobre Aprendizaje Automático (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell. Un estudio comparativo de núcleos para la clasificación de texto multi-etiqueta utilizando asociación de categorías. En la Vigésimo-primera Conferencia Internacional sobre Aprendizaje Automático (ICML), 2004. [16] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En Notas de Trabajo de AAAI 98 (La 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre Aprendizaje para la Categorización de Textos, páginas 41-48, 1998. TR WS-98-05. [17] F. Sebastiani. \n\nTR WS-98-05. [17] F. Sebastiani. Aprendizaje automático en la categorización automatizada de textos. ACM Computing Surveys, 34(1):1-47, marzo de 2002. [18] C. J. van Rijsbergen. Recuperación de información. Butterworths, Londres, 1979. [19] Y. Yang. Una evaluación de enfoques estadísticos para la categorización de texto. Recuperación de información, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu. Enfoques de aprendizaje para la detección y seguimiento de temas. IEEE EXPERT, Número Especial sobre Aplicaciones de la Recuperación de Información Inteligente, 1999. [21] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de textos. En SIGIR 99, Actas de la 22ª Conferencia Internacional Anual de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada por el tema. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, julio de 2002. ",
            "candidates": [],
            "error": [
                [
                    "actos de habla",
                    "acto de habla"
                ]
            ]
        },
        "feature selection": {
            "translated_key": "selección de características",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared <br>feature selection</br>, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "<br>feature selection</br> was performed using the chi-squared statistic.",
                "Different levels of <br>feature selection</br> were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without <br>feature selection</br>.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "<br>feature selection</br> is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the <br>feature selection</br> method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared <br>feature selection</br>, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "<br>feature selection</br> was performed using the chi-squared statistic.",
                "Different levels of <br>feature selection</br> were considered for each classifier.",
                "There are approximately 4700 unigram tokens without <br>feature selection</br>.",
                "<br>feature selection</br> is always applied to all candidate features."
            ],
            "translated_annotated_samples": [
                "Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con <br>selección de características</br> chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada.",
                "La <br>selección de características</br> se realizó utilizando la estadística de chi-cuadrado.",
                "Se consideraron diferentes niveles de <br>selección de características</br> para cada clasificador.",
                "Hay aproximadamente 4700 tokens unigram sin <br>selección de características</br>.",
                "La <br>selección de características</br> siempre se aplica a todas las características candidatas."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con <br>selección de características</br> chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La <br>selección de características</br> se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de <br>selección de características</br> para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin <br>selección de características</br>. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La <br>selección de características</br> siempre se aplica a todas las características candidatas. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "e-mail": {
            "translated_key": "correo electrónico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT <br>e-mail</br> users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION <br>e-mail</br> users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising <br>e-mail</br> usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk <br>e-mail</br>, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An <br>e-mail</br> with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an <br>e-mail</br> agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful <br>e-mail</br> agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an <br>e-mail</br> folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as <br>e-mail</br> priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an <br>e-mail</br> contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in <br>e-mail</br> to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to <br>e-mail</br> has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to <br>e-mail</br> learning tasks, Culotta et al. [7] presented methods for learning social networks from <br>e-mail</br>.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the <br>e-mail</br> which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each <br>e-mail</br> as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an <br>e-mail</br> contains an action-item and not the specific sentences.",
                "In the sentence-level view, each <br>e-mail</br> is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the <br>e-mail</br> provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often <br>e-mail</br> lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the <br>e-mail</br>, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 <br>e-mail</br> messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous <br>e-mail</br> that an author often leaves in an <br>e-mail</br> message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and <br>e-mail</br> signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the <br>e-mail</br> which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for <br>e-mail</br>, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each <br>e-mail</br> message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on <br>e-mail</br> users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of <br>e-mail</br> in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of <br>e-mail</br> addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT <br>e-mail</br> users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "INTRODUCTION <br>e-mail</br> users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising <br>e-mail</br> usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk <br>e-mail</br>, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "Keep up the good work! -Henry Figure 1: An <br>e-mail</br> with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Such a detection system can be used as one part of an <br>e-mail</br> agent which would assist a user in processing important e-mails quicker than would have been possible without the agent."
            ],
            "translated_annotated_samples": [
                "Los usuarios de <br>correo electrónico</br> enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del <br>correo electrónico</br> en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información.",
                "Los usuarios de <br>correo electrónico</br> se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del <br>correo electrónico</br>.",
                "Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m.",
                "¡Sigue con el buen trabajo! -Henry Figura 1: Un <br>correo electrónico</br> con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios.",
                "Un sistema de detección como este puede ser utilizado como una parte de un agente de <br>correo electrónico</br> que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de <br>correo electrónico</br> enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del <br>correo electrónico</br> en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de <br>correo electrónico</br> se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del <br>correo electrónico</br>. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un <br>correo electrónico</br> con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de <br>correo electrónico</br> que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "n-gram": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an <br>n-gram</br> of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the <br>n-gram</br> and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the <br>n-gram</br> representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the <br>n-gram</br> representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the <br>n-gram</br> representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the <br>n-gram</br> hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level <br>n-gram</br> representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an <br>n-gram</br> representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the <br>n-gram</br> based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "When using n-grams, if we find an <br>n-gram</br> of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the <br>n-gram</br> and an occurrence of each smaller n-gram contained by it.",
                "That is, for the <br>n-gram</br> representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "This difference in performance produced by the <br>n-gram</br> representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the <br>n-gram</br> representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "More importantly for the <br>n-gram</br> hypothesis, the n-grams lead to the best document-level classifier performance as well."
            ],
            "translated_annotated_samples": [
                "Al utilizar <br>n-grama</br>s, si encontramos un <br>n-grama</br> de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él.",
                "Es decir, para la representación de <br>n-gramos</br>, los <br>n-gramos</br> y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3.",
                "Esta diferencia en el rendimiento producida por la <br>representación de n-gramos</br> es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4).",
                "El bajo rendimiento del Naïve Bayes con la representación de <br>n-gramas</br> no es sorprendente, ya que la bolsa de <br>n-gramas</br> causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble.",
                "Más importante para la <br>hipótesis de los n-gramos</br>, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar <br>n-grama</br>s, si encontramos un <br>n-grama</br> de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de <br>n-gramos</br>, los <br>n-gramos</br> y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la <br>representación de n-gramos</br> es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de <br>n-gramas</br> no es sorprendente, ya que la bolsa de <br>n-gramas</br> causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la <br>hipótesis de los n-gramos</br>, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. ",
            "candidates": [],
            "error": [
                [
                    "n-grama",
                    "n-grama",
                    "n-gramos",
                    "n-gramos",
                    "representación de n-gramos",
                    "n-gramas",
                    "n-gramas",
                    "hipótesis de los n-gramos"
                ]
            ]
        },
        "svm": {
            "translated_key": "SVM",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard <br>svm</br> at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 <br>svm</br> 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 <br>svm</br> 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 <br>svm</br> We have used a linear <br>svm</br> with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the <br>svm</br>, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the <br>svm</br>, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the <br>svm</br> give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an <br>svm</br> than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the <br>svm</br> for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram <br>svm</br> Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence <br>svm</br> Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the <br>svm</br> sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 <br>svm</br> 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed <br>svm</br> in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the <br>svm</br> classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection <br>svm</br> Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale <br>svm</br> learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [
                "Instead, they simply apply a standard <br>svm</br> at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 <br>svm</br> 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 <br>svm</br> 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 <br>svm</br> We have used a linear <br>svm</br> with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the <br>svm</br>, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the <br>svm</br>, a linear kernel with tfidf-weighting and an L2-norm."
            ],
            "translated_annotated_samples": [
                "En cambio, simplemente aplican un <br>SVM</br> estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas.",
                "Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 <br>SVM</br> 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 <br>SVM</br> 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva.",
                "El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 <br>SVM</br> Hemos utilizado un <br>SVM</br> lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11].",
                "Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el <br>SVM</br>, el Perceptrón Votado es un método de aprendizaje basado en núcleos.",
                "Utilizamos la misma representación de características y núcleo que tenemos para la <br>SVM</br>, un núcleo lineal con ponderación tfidf y una norma L2."
            ],
            "translated_text": "Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un <br>SVM</br> estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas \"etiquetado de frases\" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 <br>SVM</br> 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 <br>SVM</br> 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 <br>SVM</br> Hemos utilizado un <br>SVM</br> lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el <br>SVM</br>, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la <br>SVM</br>, un núcleo lineal con ponderación tfidf y una norma L2. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}