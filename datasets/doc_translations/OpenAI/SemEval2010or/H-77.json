{
    "id": "H-77",
    "original_text": "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents. By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters. Previously, methods have been proposed mainly for title extraction from research papers. It has not been clear whether it could be possible to conduct automatic title extraction from general documents. As a case study, we consider extraction from Office including Word and PowerPoint. In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models. Our method is unique in that we mainly utilize formatting information such as font size as features in the models. It turns out that the use of formatting information can lead to quite accurate extraction from general documents. Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data. Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language. Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1. INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering. Ideally, metadata is defined by the authors of documents and is then used by various systems. However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26]. Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue. Methods for performing the task have been proposed. However, the focus was mainly on extraction from research papers. For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers. They formalized the problem as that of classification and employed Support Vector Machines as the classifier. They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents. By general documents, we mean documents that may belong to any one of a number of specific genres. General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed. Research papers usually have well-formed styles and noticeable characteristics. In contrast, the styles of general documents can vary greatly. It has not been clarified whether a machine learning based approach can work well for this task. There are many types of metadata: title, author, date of creation, etc. As a case study, we consider title extraction in this paper. General documents can be in many different file formats: Microsoft Office, PDF (PS), etc. As a case study, we consider extraction from Office including Word and PowerPoint. We take a machine learning approach. We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models. In the models, we mainly utilize formatting information such as font size as features. We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron. In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search. Experimental results indicate that our approach works well for title extraction from general documents. Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles. Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively. It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%). We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications. The rest of the paper is organized as follows. In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work. In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles. Section 6 gives our experimental results. We make concluding remarks in section 7. 2. RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers. The proposed methods fall into two categories: the rule based approach and the machine learning based approach. Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript. They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes. Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies. Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information. The rule-based approach can achieve high performance. However, it also has disadvantages. It is less adaptive and robust when compared with the machine learning approach. Han et al. [10], for instance, conducted metadata extraction with the machine learning approach. They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier. They mainly used linguistic information as features. They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested. Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models. Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval. In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8]. In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5]. Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22]. Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24]. To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3. MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents. By general documents, we mean documents that belong to one of any number of specific genres. The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes. General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed. Figure 1 shows an estimate on distributions of file formats on intranet and internet [15]. Office and PDF are the main file formats on the intranet. Even on the internet, the documents in the formats are still not negligible, given its extremely large size. In this paper, without loss of generality, we take Office documents as an example. Figure 1. Distributions of file formats in internet and intranet. For Office documents, users can define titles as file properties using a feature provided by Office. We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate. That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers. We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct. We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details). A number of reasons can be considered. For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file. In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate. We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google. We found that nearly all the titles presented in the search results were from the file properties of the documents. However, only 0.272 of them were correct. Actually, true titles usually exist at the beginnings of the bodies of documents. If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing. This is exactly the problem we address in this paper. More specifically, given a Word document, we are to extract the title from the top region of the first page. Given a PowerPoint document, we are to extract the title from the first slide. A title sometimes consists of a main title and one or two subtitles. We only consider extraction of the main title. As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles. Figure 2. Title extraction from Word document. Figure 3. Title extraction from PowerPoint document. Next, we define a specification for human judgments in title data annotation. The annotated data will be used in training and testing of the title extraction methods. Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification. However, there are many cases in which the identification is not easy. There are some rules defined in the specification that guide identification for such cases. The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.) Figures 2 and 3 show examples of Office documents from which we conduct title extraction. In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document. Microsoft Windows on the top of this page is a picture and thus is ignored. In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document. We have developed a tool for annotation of titles by human annotators. Figure 4 shows a snapshot of the tool. Figure 4. Title annotation tool. 4. TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction. The same pre-processing step occurs before training and extraction. During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted. If a line (lines are separated by return symbols) only has a single format, then the line will become a unit. If a line has several parts and each of them has its own format, then each part will become a unit. Each unit will be treated as an instance in learning. A unit contains not only content information (linguistic information) but also formatting information. The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances). Figure 5 shows the units obtained from the document in Figure 2. Figure 5. Example of units. In learning, the input is sequences of units where each sequence corresponds to a document. We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other. We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM). In extraction, the input is a sequence of units from one document. We employ one type of model to identify whether a unit is title_begin, title_end, or other. We then extract units from the unit labeled with title_begin to the unit labeled with title_end. The result is the extracted title of the document. The unique characteristic of our approach is that we mainly utilize formatting information for title extraction. Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction. This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework. That is why we apply them together to our current problem. Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ). Recall that an instance here represents a unit. A label represents title_begin, title_end, or other. Here, k is the number of units in a document. In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ). Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6. Metadata extraction model. We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L . Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers. We train the classifiers locally using the labeled data. As the classifier, we employ the Perceptron or Maximum Entropy model. We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L . Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers. However, the classifiers are conditioned on the previous label. When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively. That is to say, the two models are more precise. In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction. For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics. Specifically, we first identify the most likely title_begin. Then we find the most likely title_end within three units after the title_begin. Finally, we extract as a title the units between the title_begin and the title_end. For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence. In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13]. This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem. We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2]. In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features. We mainly use the former. The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font). If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0. If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0. If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0. If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0. It is necessary to conduct normalization on font sizes. For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt. Boldface: This binary feature represents whether or not the current unit is in boldface. Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment. The following format features with respect to context play an important role in title extraction. Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines. Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit. Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one. Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words. Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words. The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats. However, if lines begin with one of the positive words, then it is likely that they are title lines. Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words. The negative words include To, By, created by, updated by, etc. There are more negative words than positive words. The above linguistic features are language dependent. Word Count: A title should not be too long. We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval. If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0. Ending Character: This feature represents whether the unit ends with :, -, or other special characters. A title usually does not end with such a character. 5. DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles. Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text. A ranking function in search can use different weights for different fields of 149 the document. Also, titles are typically assigned high weights, indicating that they are important for document retrieval. As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document. By doing this, we attempt to improve the overall precision. In this paper, we employ a modification of BM25 that allows field weighting [21]. As fields, we make use of body, title, extracted title and anchor. First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field. Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk . Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6. EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments. First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft. We call it MS hereafter. Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively. Figure 7 shows the distributions of the genres of the documents. We see that the documents are indeed general documents as we define them. Figure 7. Distributions of document genres. Third, a data set in Chinese was also downloaded from the internet. It includes 500 Word documents and 500 PowerPoint documents in Chinese. We manually labeled the titles of all the documents, on the basis of our specification. Not all the documents in the two data sets have titles. Table 1 shows the percentages of the documents having titles. We see that DotCom and DotGov have more PowerPoint documents with titles than MS. This might be because PowerPoint documents published on the internet are more formal than those on the intranet. Table 1. The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure. The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2. Table 2. Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2. They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable. We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles. We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation). This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces). Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3. Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS). As the model, we used Perceptron. We conduct 4-fold cross validation. Thus, all the results reported here are those averaged over 4 trials. Tables 4 and 5 show the results. We see that Perceptron significantly outperforms the baselines. In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles. Table 4. Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5. Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction. For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines. For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines. We conduct significance tests. The results are shown in Table 6. Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line. The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6. Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction. However, it is also obvious that using only these two features is not enough. There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc. For those cases, the largest font size method cannot work well. For similar reasons, the first line method alone cannot work well, either. With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First. We investigate the performance of solely using linguistic features. We found that it does not work well. It seems that the format features play important roles and the linguistic features are supplements.. Figure 8. An example Word document. Figure 9. An example PowerPoint document. We conducted an error analysis on the results of Perceptron. We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases. In these documents, the layouts of the first pages were difficult to understand, even for humans. Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets. Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3). Confusions between main titles and subtitles were another type of error. Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect. This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment. Again, we perform 4-fold cross 151 validation on the first data set (MS). Table 7, 8 shows the results of all the four models. It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst. In general, the Markovian models perform better than or as well as their classifier counterparts. This seems to be because the Markovian models are trained globally, while the classifiers are trained locally. The Perceptron based models perform better than the ME based counterparts. This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction. Table 7. Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8. Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov). Tables 9-12 show the results. Table 9. Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10. Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11. Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12. Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well. There is almost no drop in accuracy. The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese. Tables 13-14 show the results. Table 13. Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14. Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language. There are only small drops in accuracy. Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible. The results indicate that the patterns of title formats exist across different languages. From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval. As a baseline, we employed BM25 without using extracted titles. The ranking mechanism was as described in Section 5. The weights were heuristically set. We did not conduct optimization on the weights. The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly. Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results. In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10. Search ranking results. Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title. With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%. Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7. CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents. We have tried using a machine learning approach to address the problem. Previous work showed that the machine learning approach can work well for metadata extraction from research papers. In this paper, we showed that the approach can work for extraction from general documents as well. Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents. Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information. It appeared that using formatting information is a key for successfully conducting title extraction from general documents. We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron. We found that the performance of the Perceptorn models was the best. We applied models constructed in one domain to another domain and applied models trained in one language to another language. We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic. We also attempted to use the extracted titles in document retrieval. We observed a significant improvement in document ranking performance for search when using extracted title information. All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8. ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation. We acknowledge Jinzhu Li for his assistance in conducting the experiments. We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9. REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J. A maximum entropy approach to natural language processing. Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text. In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web. In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models. Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files. In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A. Automatic document metadata extraction using support vector machines. In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins. In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J. Automatic Metadata generation & evaluation. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A. Effective enterprise information retrieval across new content formats. In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials. In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation. In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions. In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B. Table extraction using conditional random fields. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment. In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata. In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation. Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154",
    "original_translation": "Extracción automática de títulos de documentos generales utilizando aprendizaje automático. En este documento, proponemos un enfoque de aprendizaje automático para la extracción de títulos de documentos generales. Por documentos generales nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos, incluyendo presentaciones, capítulos de libros, artículos técnicos, folletos, informes y cartas. Anteriormente, se han propuesto métodos principalmente para la extracción de títulos de artículos de investigación. No ha quedado claro si sería posible realizar la extracción automática de títulos de documentos generales. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que principalmente utilizamos información de formato como el tamaño de fuente como características en los modelos. Resulta que el uso de información de formato puede llevar a una extracción bastante precisa de documentos generales. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente en un experimento con datos de intranet. Otros hallazgos importantes en este trabajo incluyen que podemos entrenar modelos en un dominio y aplicarlos a otro dominio, y más sorprendentemente, incluso podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente los resultados de clasificación de búsqueda en la recuperación de documentos utilizando los títulos extraídos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de Búsqueda; H.4.1 [Aplicaciones de Sistemas de Información]: Automatización de Oficinas - Procesamiento de Texto; D.2.8 [Ingeniería de Software]: Métricas - medidas de complejidad, medidas de rendimiento Términos Generales Algoritmos, Experimentación, Rendimiento. 1. METADATA La información de metadatos de los documentos es útil para muchos tipos de procesamiento de documentos, como la búsqueda, la navegación y el filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen los metadatos de los documentos por sí mismas, incluso cuando tienen herramientas convenientes para la definición de metadatos [26]. Por lo tanto, la extracción automática de metadatos de los cuerpos de los documentos resulta ser un tema de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, el enfoque estaba principalmente en la extracción de los documentos de investigación. Por ejemplo, Han et al. [10] propusieron un método basado en aprendizaje automático para realizar extracciones de artículos de investigación. Formalizaron el problema como el de clasificación y emplearon Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron características lingüísticas en el modelo. En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigación sobre la extracción de información de ellos. Los artículos de investigación suelen tener estilos bien formados y características notables. Por el contrario, los estilos de los documentos generales pueden variar considerablemente. No se ha aclarado si un enfoque basado en aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como estudio de caso, consideramos la extracción de títulos en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizamos la extracción de títulos utilizando uno de los tipos de modelos entrenados. En los modelos, principalmente utilizamos información de formato como el tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con Márgenes Desiguales, Modelo de Entropía Máxima de Markov y Perceptrón Votado. En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, ¿cuál modelo funciona mejor para la extracción de títulos?; (2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a los baselines: uno que siempre utiliza las primeras líneas como títulos y el otro que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente. Resulta que el uso de características de formato es la clave para la exitosa extracción de títulos. (1) Hemos observado que los modelos basados en Perceptrón tienen un mejor rendimiento en términos de precisión de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados en otro, y pueden ser entrenados en un idioma y aplicados en otro. (3) Hemos descubierto que al utilizar los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que podemos realizar de manera fiable la extracción de títulos de documentos generales y utilizar los resultados extraídos para mejorar aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, presentamos el trabajo relacionado, y en la sección 3, explicamos la motivación y el contexto del problema de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando los títulos extraídos. La sección 6 presenta nuestros resultados experimentales. Hacemos observaciones finales en la sección 7.2. TRABAJO RELACIONADO 2.1 Se han propuesto métodos de extracción de metadatos de documentos para realizar extracción automática de metadatos de documentos; sin embargo, el enfoque principal ha sido la extracción de artículos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en reglas y el enfoque basado en aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrollaron un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como que los títulos suelen ubicarse en las partes superiores de las primeras páginas y suelen estar en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel et al. [23] realizaron extracción de metadatos de materiales educativos utilizando tecnologías de procesamiento del lenguaje natural basadas en reglas. Mao et al. [16] también llevaron a cabo la extracción automática de metadatos de artículos de investigación utilizando reglas sobre la información de formato. El enfoque basado en reglas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, realizaron extracción de metadatos con enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron información lingüística como características. Informaron de una alta precisión de extracción de información de artículos de investigación en términos de precisión y recuperación. 2.2 Extracción de Información La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la cual, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Los Modelos Ocultos de Markov [6], el Modelo de Entropía Máxima [1, 4], el Modelo de Entropía Máxima de Markov [17], las Máquinas de Vectores de Soporte [3], el Campo Aleatorio Condicional [12] y el Perceptrón Votado [2] son modelos ampliamente utilizados para la extracción de información. La extracción de información se ha aplicado, por ejemplo, al etiquetado de partes del discurso [20], al reconocimiento de entidades nombradas [25] y a la extracción de tablas [19]. 2.3 Búsqueda utilizando la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. lograron extraer títulos de artículos de investigación y utilizar los títulos extraídos en la búsqueda de metadatos de los artículos [8]. En la búsqueda web, los campos de título (es decir, propiedades de archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al. encontraron que las páginas web con metadatos bien definidos son más fáciles de recuperar que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de Office) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y DEFINICIÓN DEL PROBLEMA Consideramos el problema de extraer automáticamente títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de varios géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, artículos técnicos, folletos, informes, memorandos, especificaciones, cartas, anuncios o currículums. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigar la extracción de títulos de los mismos. La Figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e internet [15]. Office y PDF son los principales formatos de archivo en la intranet. Incluso en internet, los documentos en los formatos siguen siendo significativos, dada su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de Office. Figura 1. Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir títulos como propiedades de archivo utilizando una función proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la función y, por lo tanto, los títulos en las propiedades de los archivos suelen ser muy inexactos. Es decir, los títulos en las propiedades del archivo suelen ser inconsistentes con los verdaderos títulos en los cuerpos de los archivos que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 documentos de Word y 6,000 documentos de PowerPoint de una intranet y de internet, y examinamos cuántos títulos en las propiedades de los archivos son correctos. Descubrimos que sorprendentemente la precisión fue solo de 0.265 (cf., Sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, entonces la propiedad de archivo del nuevo archivo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en las propiedades de archivo de documentos de Office en la búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devueltos por Google. Descubrimos que casi todos los títulos presentados en los resultados de búsqueda eran de las propiedades de archivo de los documentos. Sin embargo, solo 0.272 de ellos fueron correctos. De hecho, los títulos verdaderos suelen encontrarse al principio de los cuerpos de los documentos. Si podemos extraer con precisión los títulos de los cuerpos de los documentos, entonces podemos aprovechar la información de título confiable en el procesamiento de documentos. Este es exactamente el problema que abordamos en este artículo. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, debemos extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Solo consideramos la extracción del título principal. Como líneas base para la extracción de títulos, utilizamos la de siempre usar las primeras líneas como títulos y la de siempre usar las líneas con tamaños de fuente más grandes como títulos. Figura 2. Extracción de título de documento de Word. Figura 3. Extracción de títulos de un documento de PowerPoint. A continuación, definimos una especificación para los juicios humanos en la anotación de datos de títulos. Los datos anotados se utilizarán en el entrenamiento y prueba de los métodos de extracción de títulos. Resumen de la especificación: El título de un documento debe ser identificado en base al sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en los que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación para tales casos. Las reglas incluyen que un título suele estar en líneas consecutivas en el mismo formato, un documento puede no tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como borrador, 147 whitepaper, etc., si es difícil determinar cuál es el título, seleccionar el que tenga el tamaño de fuente más grande y, si aún es difícil determinar cuál es el título, seleccionar el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos). Las figuras 2 y 3 muestran ejemplos de documentos de Office de los cuales realizamos la extracción de títulos. En la Figura 2, Diferencias en las Implementaciones de la API de Win32 entre los Sistemas Operativos de Windows es el título del documento de Word. En la parte superior de esta página hay una imagen de Microsoft Windows que por lo tanto es ignorada. En la Figura 3, \"Construyendo Ventajas Competitivas a través de una Infraestructura Ágil\" es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotadores humanos. La Figura 4 muestra una captura de pantalla de la herramienta. Figura 4. Herramienta de anotación de títulos. 4. MÉTODO DE EXTRACCIÓN DE TÍTULOS 4.1 El método de extracción de títulos basado en aprendizaje automático consta de entrenamiento y extracción. El mismo paso de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el preprocesamiento, se extraen una serie de unidades para el procesamiento de la región superior de la primera página de un documento de Word o de la primera diapositiva de un documento de PowerPoint. Si una línea (las líneas están separadas por símbolos de retorno) solo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no solo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Figura 5. Ejemplo de unidades. En el aprendizaje, la entrada son secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end u otras) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin, title_end u otra. Empleamos cuatro tipos de modelos: Perceptrón, Entropía Máxima (ME), Modelo de Perceptrón de Markov (PMM) y Modelo de Entropía Máxima de Markov (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es título_inicio, título_fin u otro. Luego extraemos las unidades desde la unidad etiquetada con title_begin hasta la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que principalmente utilizamos información de formato para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar esos patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el cual solo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos De hecho, los cuatro modelos pueden considerarse dentro del mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias kxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki ,,2,1 L=). Recuerda que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end, u otro. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )|( 11 kk XXYYP LL donde iX e iY denotan variables aleatorias que toman instancias ix e etiquetas iy como valores, respectivamente ( ki ,,2,1 L= ). Herramienta de extracción de herramientas de aprendizaje 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general para simplificarlo lo suficiente para el entrenamiento. Por ejemplo, podemos asumir que kYY ,,1 L son independientes entre sí dado kXX ,,1 L. De esta manera, descomponemos el modelo en varios clasificadores. Entrenamos los clasificadores localmente utilizando los datos etiquetados. Como clasificador, empleamos el modelo Perceptrón o de Máxima Entropía. También podemos asumir que la propiedad de Markov de primer orden se cumple para kYY ,,1 L dado kXX ,,1 L. Por lo tanto, obtenemos una serie de clasificadores. Sin embargo, los clasificadores están condicionados por la etiqueta previa. Cuando empleamos el modelo Perceptrón o de Entropía Máxima como clasificador, los modelos se convierten en un Modelo Markov Perceptrón o un Modelo Markov de Entropía Máxima, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptrón y ME, asignamos etiquetas localmente y luego combinamos los resultados globalmente utilizando heurísticas. Específicamente, primero identificamos el título más probable. Entonces encontramos el título más probable dentro de tres unidades después del título inicial. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo de Viterbi para encontrar la secuencia de etiquetas globalmente óptima. En este artículo, para el Perceptrón, en realidad empleamos una variante mejorada de él, llamada Perceptrón con Margen Desigual [13]. Esta versión de Perceptrón puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren considerablemente, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del Modelo Perceptrón Markov en la cual el modelo Perceptrón es el llamado Perceptrón Votado [2]. Además, en el entrenamiento, los parámetros del modelo se actualizan de forma global en lugar de localmente. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Principalmente usamos el primero. Las características se utilizan tanto para los clasificadores de inicio de título como para los de fin de título. 4.3.1 Características de formato Tamaño de fuente: Hay cuatro características binarias que representan el tamaño de fuente normalizado de la unidad (recordemos que una unidad tiene solo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario, 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario, 0. Si el tamaño de la fuente es mayor que el tamaño de fuente promedio y no es el más grande en el documento, entonces la segunda característica será 1, de lo contrario, 0. Si el tamaño de la fuente es inferior al tamaño promedio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario, 0. Es necesario realizar la normalización de los tamaños de fuente. Por ejemplo, en un documento el tamaño de fuente más grande podría ser de 12pt, mientras que en otro el más pequeño podría ser de 18pt. Negrita: Esta característica binaria representa si la unidad actual está en negrita o no. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. El siguiente formato de características con respecto al contexto juega un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la siguiente unidad difieren del de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la siguiente difieren de la de la actual. Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad siguiente están en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, asunto:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, es probable que sean líneas de título. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Recuento de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, ∞) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario, 0. Carácter de finalización: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título generalmente no termina con un carácter como ese. 5. MÉTODO DE RECUPERACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Normalmente, en la recuperación de información, un documento se divide en varios campos que incluyen cuerpo, título y texto de anclaje. Una función de clasificación en la búsqueda puede utilizar diferentes pesos para diferentes campos del documento. Además, los títulos suelen recibir un peso alto, lo que indica que son importantes para la recuperación de documentos. Como se explicó anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite la ponderación de campos [21]. Como campos, hacemos uso del cuerpo, título, título extraído y ancla. Primero, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; luego, cada frecuencia de campo se pondera según el parámetro de peso correspondiente: ∑= f tfft tfwwtf De manera similar, calculamos la longitud del documento como una suma ponderada de las longitudes de cada campo. La longitud promedio del documento en el corpus se convierte en el promedio de todas las longitudes de documentos ponderadas. ∑= f ff dlwwdl En nuestros experimentos usamos 75.0,8.11 == bk. El peso para el contenido fue de 1.0, para el título fue de 10.0, para el enlace fue de 10.0 y para el título extraído fue de 5.0. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación. Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos aleatoriamente 5,000 documentos de Word y 5,000 documentos de PowerPoint de una intranet de Microsoft. Lo llamamos MS de ahora en adelante. Segundo, descargamos y seleccionamos aleatoriamente 500 documentos de Word y 500 documentos de PowerPoint de los dominios DotGov y DotCom en internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, de hecho, documentos generales tal como los definimos. Figura 7. Distribuciones de géneros de documentos. Tercero, también se descargó un conjunto de datos en chino de internet. Incluye 500 documentos de Word y 500 documentos de PowerPoint en chino. Etiquetamos manualmente los títulos de todos los documentos, basándonos en nuestra especificación. No todos los documentos en los dos conjuntos de datos tienen títulos. La Tabla 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría deberse a que los documentos de PowerPoint publicados en internet son más formales que los de la intranet. Tabla 1. En nuestros experimentos, realizamos evaluaciones sobre la extracción de títulos en términos de precisión, recuperación y medida F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / (A + B) Recall: R = A / (A + C) F-measure: F1 = 2PR / (P + R) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Tabla 2. Tabla de contingencia con respecto a la extracción de títulos. ¿Es título? ¿No es título? Extraído A B No extraído C D 6.2 Baselines Probamos las precisiones de los dos baselines descritos en la sección 4.2. Se denominan como el tamaño de fuente más grande y la primera línea respectivamente. 6.3 Precisión de los títulos en las propiedades del archivo Investigamos cuántos títulos en las propiedades del archivo de los documentos son confiables. Consideramos los títulos anotados por humanos como títulos verdaderos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos la Distancia de Edición para realizar la coincidencia aproximada. (La coincidencia aproximada solo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados por humanos pueden ser ligeramente diferentes de los títulos en las propiedades del archivo en la superficie, por ejemplo, contener espacios adicionales. Dadas las cadenas A y B: si ((D == 0) o (D / (La + Lb) < θ)) entonces la cadena A = cadena B D: Distancia de edición entre la cadena A y la cadena B La: longitud de la cadena A Lb: longitud de la cadena B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Tabla 3. Precisión de los títulos en las propiedades del archivo Tipo de archivo Dominio Precisión Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186 PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparación con Baselines Realizamos la extracción de títulos del primer conjunto de datos (Word y PowerPoint en MS). Como modelo, utilizamos el Perceptrón. Realizamos validación cruzada de 4 pliegues. Por lo tanto, todos los resultados reportados aquí son aquellos promediados en 4 ensayos. Las tablas 4 y 5 muestran los resultados. Vemos que el Perceptrón supera significativamente a los baselines. En la evaluación, utilizamos coincidencia exacta entre los títulos reales anotados por humanos y los títulos extraídos. Tabla 4. Precisión de la extracción de títulos con el Modelo Perceptrón de Palabra Precisión Recall F1 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Líneas base Primera línea 0.707 0.767 0.736 Tabla 5. Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como la exhaustividad del enfoque son un 8 por ciento más altas que las de las líneas de base. Para PowerPoint, tanto la precisión como la exhaustividad del enfoque son un 2 por ciento más altas que las de las líneas de base. Realizamos pruebas de significancia. Los resultados se muestran en la Tabla 6. Aquí, \"Largest\" denota la línea base de usar el tamaño de fuente más grande, \"First\" denota la línea base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático sobre las líneas de base son estadísticamente significativas (en el sentido de que el valor p < 0.05) Tabla 6. Vemos, a partir de los resultados, que los dos baselines pueden funcionar bien para la extracción de títulos, lo que sugiere que el tamaño de fuente y la información de posición son las características más útiles para la extracción de títulos. Sin embargo, también es evidente que utilizar solo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande solo contienen descripciones generales como Confidencial, Documento blanco, etc. Para esos casos, el método del tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio del título), el Perceptrón puede superar a Largest y First. Investigamos el rendimiento de utilizar únicamente características lingüísticas. Descubrimos que no funciona bien. Parece que las características del formato desempeñan roles importantes y las características lingüísticas son complementos. Figura 8. Un documento de Word de ejemplo. Figura 9. Un documento de PowerPoint de ejemplo. Realizamos un análisis de errores en los resultados del Perceptrón. Encontramos que los errores caían en tres categorías. (1) Aproximadamente un tercio de los errores estaban relacionados con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores provienen de los documentos que no tienen títulos verdaderos, sino que solo contienen viñetas. Dado que realizamos la extracción desde las regiones superiores, es difícil deshacernos de estos errores con el enfoque actual. Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que solo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error no afecta mucho al procesamiento de documentos como la búsqueda. 6.5 Comparación entre Modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Nuevamente, realizamos una validación cruzada de 4 pliegues en el primer conjunto de datos (MS). La tabla 7 y 8 muestran los resultados de los cuatro modelos. Resulta que Perceptrón y PMM tienen el mejor rendimiento, seguidos por MEMM, y ME tiene el peor rendimiento. En general, los modelos markovianos tienen un mejor rendimiento que sus contrapartes clasificadoras o igual de bueno. Esto parece ser porque los modelos markovianos se entrenan de forma global, mientras que los clasificadores se entrenan de forma local. Los modelos basados en el Perceptrón tienen un mejor rendimiento que sus contrapartes basadas en el ME. Esto parece ser porque los modelos basados en Perceptrón están creados para realizar mejores clasificaciones, mientras que los modelos de ME se construyen para una mejor predicción. Tabla 7. Comparación entre diferentes modelos de aprendizaje para la extracción de títulos con Modelo de Palabra Precisión Recall F1 Perceptrón 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de títulos con el Modelo de PowerPoint Precisión Recall F1 Perceptrón 0.875 0.895 0.885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). Las tablas 9-12 muestran los resultados. Tabla 9. Precisión de la extracción de títulos con Word en DotGov Precisión Recall F1 Modelo Perceptrón 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582 Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Precisión de la extracción de títulos con PowerPoint en DotGov Precision Recall F1 Modelo Perceptrón 0.900 0.906 0.903 Tamaño de fuente más grande 0.871 0.888 0.879 Baselines Primera línea 0.554 0.564 0.559 Tabla 11. Precisión de la extracción de títulos con Word en DotCom Precisión Recall F1 Modelo Perceptrón 0.832 0.880 0.855 Tamaño de fuente más grande 0.676 0.753 0.712 Baselines Primera línea 0.577 0.643 0.608 Tabla 12. Rendimiento de la extracción de títulos de documentos de PowerPoint en el modelo de Precisión, Recall y F1 de Perceptrón DotCom 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875 Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos pueden adaptarse bien a diferentes dominios. Casi no hay disminución en la precisión. Los resultados indican que los patrones de formatos de títulos existen en diferentes dominios, y es posible construir un modelo independiente del dominio utilizando principalmente información de formato. 6.7 Adaptación de idioma Aplicamos el modelo entrenado con los datos en inglés (MS) al conjunto de datos en chino. Las tablas 13-14 muestran los resultados. Tabla 13. Precisión de la extracción de títulos con Word en chino: Precisión Recall F1 Modelo Perceptrón 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738 Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Vemos que los modelos se pueden adaptar a un idioma diferente. Solo hay pequeñas caídas en la precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no usarlas es insignificante. Los resultados indican que los patrones de formatos de títulos existen en diferentes idiomas. A partir de los resultados de adaptación de dominio y adaptación de lenguaje, concluimos que el uso de información de formato es clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con Títulos Extraídos Realizamos experimentos utilizando la extracción de títulos para la recuperación de documentos. Como línea base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación fue como se describe en la Sección 5. Los pesos fueron establecidos heurísticamente. No realizamos la optimización de los pesos. La evaluación se llevó a cabo en un corpus de 1.3 millones de documentos extraídos de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de los registros de consultas del motor de búsqueda de esta intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 consultas fueron elegidas al azar. A los usuarios se les pidió que proporcionaran juicios sobre el grado de relevancia del documento en una escala del 1 al 5 (1 significando perjudicial, 2 - malo, 3 - regular, 4 - bueno y 5 - excelente). La figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión al considerar documentos buenos o excelentes como relevantes (tres barras izquierdas con umbral de relevancia 0.5), o al considerar solo documentos excelentes como relevantes (tres barras derechas con umbral de relevancia 1.0). Resultados de clasificación de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo los campos cuerpo, título (propiedad de archivo) y texto de anclaje. • Barra morada - BM25 incluyendo los campos cuerpo, título (propiedad de archivo), texto de anclaje y título extraído. Con el campo adicional de título extraído incluido en BM25, la precisión @10 aumentó de 0.132 a 0.145, o aproximadamente un 10%. Por lo tanto, se puede afirmar que el uso del título extraído puede mejorar efectivamente la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este documento, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. Trabajos anteriores mostraron que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de artículos de investigación. En este artículo, demostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas base en la extracción de títulos de documentos de Office. Trabajos anteriores sobre extracción de metadatos principalmente utilizaron características lingüísticas en los documentos, mientras que nosotros principalmente utilizamos información de formato. Parecía que el uso de información de formato es clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Probamos diferentes modelos de aprendizaje automático incluyendo Perceptrón, Entropía Máxima, Modelo de Markov de Entropía Máxima y Perceptrón Votado. Descubrimos que el rendimiento de los modelos Perceptrón fue el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos entrenados en un idioma a otro idioma. Encontramos que las precisiones no disminuyeron sustancialmente en diferentes dominios y en diferentes idiomas, lo que indica que los modelos eran genéricos. También intentamos utilizar los títulos extraídos en la recuperación de documentos. Observamos una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda al utilizar la información del título extraída. Todas las investigaciones anteriores no se llevaron a cabo, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción de títulos. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu y a los revisores anónimos de JCDL05 por sus valiosos comentarios sobre este artículo. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de entropía máxima para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminativo para modelos ocultos de Markov: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en Procesamiento de Lenguaje Natural, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de vectores de soporte. Aprendizaje automático, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información de texto semiestructurado y libre. En Actas de la Decimoctava Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia newsblaster: resumen multilingüe de noticias en la web. En Actas de la conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos ocultos de Markov factorial. Aprendizaje automático, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de Visualización de Información de 1999, 446-451, 1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., y Pal, N. eBizSearch: un motor de búsqueda de nicho para e-Business. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 413-414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. Extracción de metadatos basada en conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de Bibliotecas Digitales de la ACM, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos utilizando máquinas de vectores de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y etiquetar datos de secuencias. En Actas de la Decimoctava Conferencia Internacional sobre Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo del perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial en nuevos formatos de contenido. En Actas de la Séptima Conferencia de Motores de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., y Thoma, G. R. Un sistema de generación de características dinámicas para la extracción automatizada de metadatos en la preservación de materiales digitales. En Actas del Primer Taller Internacional sobre Análisis de Imágenes de Documentos para Bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de máxima entropía de Markov para extracción de información y segmentación. En Actas de la Decimoséptima Conferencia Internacional sobre Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésimo-Primera Conferencia Internacional Anual de Ciencias de Sistemas de Hawái, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas utilizando campos aleatorios condicionales. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para la unión de frases preposicionales. En Actas de la Decimoséptima Conferencia Internacional de Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Extensión simple de BM25 a múltiples campos ponderados, En Actas de la Decimotercera Conferencia de ACM sobre Información y Gestión del Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Minería web basada en metadatos para relevancia, En Actas del Simposio Internacional de Ingeniería y Aplicaciones de Bases de Datos 2000, 113-121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema de procesamiento de lenguaje natural para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE sobre Bibliotecas Digitales de 2004, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda de Internet a la implementación de metadatos Dublin Core. Revista de Ciencia de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y., y Zhang, T. Reconocimiento y uso de entidades nombradas: reconocimiento de entidades nombradas enfocado utilizando aprendizaje automático. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154",
    "original_sentences": [
        "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
        "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
        "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
        "Previously, methods have been proposed mainly for title extraction from research papers.",
        "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
        "As a case study, we consider extraction from Office including Word and PowerPoint.",
        "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
        "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
        "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
        "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
        "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
        "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
        "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
        "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
        "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
        "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
        "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
        "Methods for performing the task have been proposed.",
        "However, the focus was mainly on extraction from research papers.",
        "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
        "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
        "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
        "By general documents, we mean documents that may belong to any one of a number of specific genres.",
        "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
        "Research papers usually have well-formed styles and noticeable characteristics.",
        "In contrast, the styles of general documents can vary greatly.",
        "It has not been clarified whether a machine learning based approach can work well for this task.",
        "There are many types of metadata: title, author, date of creation, etc.",
        "As a case study, we consider title extraction in this paper.",
        "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
        "As a case study, we consider extraction from Office including Word and PowerPoint.",
        "We take a machine learning approach.",
        "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
        "In the models, we mainly utilize formatting information such as font size as features.",
        "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
        "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
        "Experimental results indicate that our approach works well for title extraction from general documents.",
        "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
        "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
        "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
        "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
        "The rest of the paper is organized as follows.",
        "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
        "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
        "Section 6 gives our experimental results.",
        "We make concluding remarks in section 7. 2.",
        "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
        "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
        "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
        "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
        "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
        "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
        "The rule-based approach can achieve high performance.",
        "However, it also has disadvantages.",
        "It is less adaptive and robust when compared with the machine learning approach.",
        "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
        "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
        "They mainly used linguistic information as features.",
        "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
        "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
        "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
        "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
        "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
        "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
        "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
        "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
        "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
        "By general documents, we mean documents that belong to one of any number of specific genres.",
        "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
        "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
        "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
        "Office and PDF are the main file formats on the intranet.",
        "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
        "In this paper, without loss of generality, we take Office documents as an example.",
        "Figure 1.",
        "Distributions of file formats in internet and intranet.",
        "For Office documents, users can define titles as file properties using a feature provided by Office.",
        "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
        "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
        "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
        "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
        "A number of reasons can be considered.",
        "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
        "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
        "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
        "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
        "However, only 0.272 of them were correct.",
        "Actually, true titles usually exist at the beginnings of the bodies of documents.",
        "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
        "This is exactly the problem we address in this paper.",
        "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
        "Given a PowerPoint document, we are to extract the title from the first slide.",
        "A title sometimes consists of a main title and one or two subtitles.",
        "We only consider extraction of the main title.",
        "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
        "Figure 2.",
        "Title extraction from Word document.",
        "Figure 3.",
        "Title extraction from PowerPoint document.",
        "Next, we define a specification for human judgments in title data annotation.",
        "The annotated data will be used in training and testing of the title extraction methods.",
        "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
        "However, there are many cases in which the identification is not easy.",
        "There are some rules defined in the specification that guide identification for such cases.",
        "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
        "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
        "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
        "Microsoft Windows on the top of this page is a picture and thus is ignored.",
        "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
        "We have developed a tool for annotation of titles by human annotators.",
        "Figure 4 shows a snapshot of the tool.",
        "Figure 4.",
        "Title annotation tool. 4.",
        "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
        "The same pre-processing step occurs before training and extraction.",
        "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
        "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
        "If a line has several parts and each of them has its own format, then each part will become a unit.",
        "Each unit will be treated as an instance in learning.",
        "A unit contains not only content information (linguistic information) but also formatting information.",
        "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
        "Figure 5 shows the units obtained from the document in Figure 2.",
        "Figure 5.",
        "Example of units.",
        "In learning, the input is sequences of units where each sequence corresponds to a document.",
        "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
        "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
        "In extraction, the input is a sequence of units from one document.",
        "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
        "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
        "The result is the extracted title of the document.",
        "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
        "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
        "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
        "That is why we apply them together to our current problem.",
        "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
        "Recall that an instance here represents a unit.",
        "A label represents title_begin, title_end, or other.",
        "Here, k is the number of units in a document.",
        "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
        "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
        "Metadata extraction model.",
        "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
        "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
        "We train the classifiers locally using the labeled data.",
        "As the classifier, we employ the Perceptron or Maximum Entropy model.",
        "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
        "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
        "However, the classifiers are conditioned on the previous label.",
        "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
        "That is to say, the two models are more precise.",
        "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
        "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
        "Specifically, we first identify the most likely title_begin.",
        "Then we find the most likely title_end within three units after the title_begin.",
        "Finally, we extract as a title the units between the title_begin and the title_end.",
        "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
        "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
        "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
        "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
        "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
        "We mainly use the former.",
        "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
        "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
        "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
        "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
        "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
        "It is necessary to conduct normalization on font sizes.",
        "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
        "Boldface: This binary feature represents whether or not the current unit is in boldface.",
        "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
        "The following format features with respect to context play an important role in title extraction.",
        "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
        "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
        "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
        "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
        "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
        "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
        "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
        "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
        "The negative words include To, By, created by, updated by, etc.",
        "There are more negative words than positive words.",
        "The above linguistic features are language dependent.",
        "Word Count: A title should not be too long.",
        "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
        "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
        "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
        "A title usually does not end with such a character. 5.",
        "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
        "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
        "A ranking function in search can use different weights for different fields of 149 the document.",
        "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
        "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
        "By doing this, we attempt to improve the overall precision.",
        "In this paper, we employ a modification of BM25 that allows field weighting [21].",
        "As fields, we make use of body, title, extracted title and anchor.",
        "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
        "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
        "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
        "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
        "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
        "We call it MS hereafter.",
        "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
        "Figure 7 shows the distributions of the genres of the documents.",
        "We see that the documents are indeed general documents as we define them.",
        "Figure 7.",
        "Distributions of document genres.",
        "Third, a data set in Chinese was also downloaded from the internet.",
        "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
        "We manually labeled the titles of all the documents, on the basis of our specification.",
        "Not all the documents in the two data sets have titles.",
        "Table 1 shows the percentages of the documents having titles.",
        "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
        "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
        "Table 1.",
        "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
        "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
        "Table 2.",
        "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
        "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
        "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
        "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
        "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
        "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
        "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
        "As the model, we used Perceptron.",
        "We conduct 4-fold cross validation.",
        "Thus, all the results reported here are those averaged over 4 trials.",
        "Tables 4 and 5 show the results.",
        "We see that Perceptron significantly outperforms the baselines.",
        "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
        "Table 4.",
        "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
        "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
        "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
        "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
        "We conduct significance tests.",
        "The results are shown in Table 6.",
        "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
        "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
        "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
        "However, it is also obvious that using only these two features is not enough.",
        "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
        "For those cases, the largest font size method cannot work well.",
        "For similar reasons, the first line method alone cannot work well, either.",
        "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
        "We investigate the performance of solely using linguistic features.",
        "We found that it does not work well.",
        "It seems that the format features play important roles and the linguistic features are supplements..",
        "Figure 8.",
        "An example Word document.",
        "Figure 9.",
        "An example PowerPoint document.",
        "We conducted an error analysis on the results of Perceptron.",
        "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
        "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
        "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
        "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
        "Confusions between main titles and subtitles were another type of error.",
        "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
        "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
        "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
        "Table 7, 8 shows the results of all the four models.",
        "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
        "In general, the Markovian models perform better than or as well as their classifier counterparts.",
        "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
        "The Perceptron based models perform better than the ME based counterparts.",
        "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
        "Table 7.",
        "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
        "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
        "Tables 9-12 show the results.",
        "Table 9.",
        "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
        "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
        "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
        "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
        "There is almost no drop in accuracy.",
        "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
        "Tables 13-14 show the results.",
        "Table 13.",
        "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
        "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
        "There are only small drops in accuracy.",
        "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
        "The results indicate that the patterns of title formats exist across different languages.",
        "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
        "As a baseline, we employed BM25 without using extracted titles.",
        "The ranking mechanism was as described in Section 5.",
        "The weights were heuristically set.",
        "We did not conduct optimization on the weights.",
        "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
        "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
        "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
        "Search ranking results.",
        "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
        "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
        "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
        "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
        "We have tried using a machine learning approach to address the problem.",
        "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
        "In this paper, we showed that the approach can work for extraction from general documents as well.",
        "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
        "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
        "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
        "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
        "We found that the performance of the Perceptorn models was the best.",
        "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
        "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
        "We also attempted to use the extracted titles in document retrieval.",
        "We observed a significant improvement in document ranking performance for search when using extracted title information.",
        "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
        "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
        "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
        "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
        "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
        "A maximum entropy approach to natural language processing.",
        "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
        "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
        "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
        "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
        "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
        "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
        "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
        "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
        "Automatic document metadata extraction using support vector machines.",
        "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
        "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
        "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
        "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
        "Automatic Metadata generation & evaluation.",
        "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
        "Effective enterprise information retrieval across new content formats.",
        "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
        "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
        "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
        "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
        "Table extraction using conditional random fields.",
        "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
        "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
        "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
        "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
        "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
    ],
    "translated_text_sentences": [
        "Extracción automática de títulos de documentos generales utilizando aprendizaje automático.",
        "En este documento, proponemos un enfoque de aprendizaje automático para la extracción de títulos de documentos generales.",
        "Por documentos generales nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos, incluyendo presentaciones, capítulos de libros, artículos técnicos, folletos, informes y cartas.",
        "Anteriormente, se han propuesto métodos principalmente para la extracción de títulos de artículos de investigación.",
        "No ha quedado claro si sería posible realizar la extracción automática de títulos de documentos generales.",
        "Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint.",
        "En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados.",
        "Nuestro método es único en que principalmente utilizamos información de formato como el tamaño de fuente como características en los modelos.",
        "Resulta que el uso de información de formato puede llevar a una extracción bastante precisa de documentos generales.",
        "La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente en un experimento con datos de intranet.",
        "Otros hallazgos importantes en este trabajo incluyen que podemos entrenar modelos en un dominio y aplicarlos a otro dominio, y más sorprendentemente, incluso podemos entrenar modelos en un idioma y aplicarlos a otro idioma.",
        "Además, podemos mejorar significativamente los resultados de clasificación de búsqueda en la recuperación de documentos utilizando los títulos extraídos.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de Búsqueda; H.4.1 [Aplicaciones de Sistemas de Información]: Automatización de Oficinas - Procesamiento de Texto; D.2.8 [Ingeniería de Software]: Métricas - medidas de complejidad, medidas de rendimiento Términos Generales Algoritmos, Experimentación, Rendimiento. 1.",
        "METADATA La información de metadatos de los documentos es útil para muchos tipos de procesamiento de documentos, como la búsqueda, la navegación y el filtrado.",
        "Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas.",
        "Sin embargo, las personas rara vez definen los metadatos de los documentos por sí mismas, incluso cuando tienen herramientas convenientes para la definición de metadatos [26].",
        "Por lo tanto, la extracción automática de metadatos de los cuerpos de los documentos resulta ser un tema de investigación importante.",
        "Se han propuesto métodos para realizar la tarea.",
        "Sin embargo, el enfoque estaba principalmente en la extracción de los documentos de investigación.",
        "Por ejemplo, Han et al. [10] propusieron un método basado en aprendizaje automático para realizar extracciones de artículos de investigación.",
        "Formalizaron el problema como el de clasificación y emplearon Máquinas de Vectores de Soporte como clasificador.",
        "Principalmente utilizaron características lingüísticas en el modelo. En este artículo, consideramos la extracción de metadatos de documentos generales.",
        "Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos.",
        "Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigación sobre la extracción de información de ellos.",
        "Los artículos de investigación suelen tener estilos bien formados y características notables.",
        "Por el contrario, los estilos de los documentos generales pueden variar considerablemente.",
        "No se ha aclarado si un enfoque basado en aprendizaje automático puede funcionar bien para esta tarea.",
        "Hay muchos tipos de metadatos: título, autor, fecha de creación, etc.",
        "Como estudio de caso, consideramos la extracción de títulos en este artículo.",
        "Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc.",
        "Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint.",
        "Tomamos un enfoque de aprendizaje automático.",
        "Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizamos la extracción de títulos utilizando uno de los tipos de modelos entrenados.",
        "En los modelos, principalmente utilizamos información de formato como el tamaño de fuente como características.",
        "Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con Márgenes Desiguales, Modelo de Entropía Máxima de Markov y Perceptrón Votado.",
        "En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, ¿cuál modelo funciona mejor para la extracción de títulos?; (2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda.",
        "Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales.",
        "Nuestro método puede superar significativamente a los baselines: uno que siempre utiliza las primeras líneas como títulos y el otro que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos.",
        "La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente.",
        "Resulta que el uso de características de formato es la clave para la exitosa extracción de títulos. (1) Hemos observado que los modelos basados en Perceptrón tienen un mejor rendimiento en términos de precisión de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados en otro, y pueden ser entrenados en un idioma y aplicados en otro. (3) Hemos descubierto que al utilizar los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%).",
        "Concluimos que podemos realizar de manera fiable la extracción de títulos de documentos generales y utilizar los resultados extraídos para mejorar aplicaciones reales.",
        "El resto del documento está organizado de la siguiente manera.",
        "En la sección 2, presentamos el trabajo relacionado, y en la sección 3, explicamos la motivación y el contexto del problema de nuestro trabajo.",
        "En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando los títulos extraídos.",
        "La sección 6 presenta nuestros resultados experimentales.",
        "Hacemos observaciones finales en la sección 7.2.",
        "TRABAJO RELACIONADO 2.1 Se han propuesto métodos de extracción de metadatos de documentos para realizar extracción automática de metadatos de documentos; sin embargo, el enfoque principal ha sido la extracción de artículos de investigación.",
        "Los métodos propuestos se dividen en dos categorías: el enfoque basado en reglas y el enfoque basado en aprendizaje automático.",
        "Giuffrida et al. [9], por ejemplo, desarrollaron un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript.",
        "Utilizaron reglas como que los títulos suelen ubicarse en las partes superiores de las primeras páginas y suelen estar en los tamaños de fuente más grandes.",
        "Liddy et al. [14] y Yilmazel et al. [23] realizaron extracción de metadatos de materiales educativos utilizando tecnologías de procesamiento del lenguaje natural basadas en reglas.",
        "Mao et al. [16] también llevaron a cabo la extracción automática de metadatos de artículos de investigación utilizando reglas sobre la información de formato.",
        "El enfoque basado en reglas puede lograr un alto rendimiento.",
        "Sin embargo, también tiene desventajas.",
        "Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático.",
        "Han et al. [10], por ejemplo, realizaron extracción de metadatos con enfoque de aprendizaje automático.",
        "Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar Máquinas de Vectores de Soporte como clasificador.",
        "Principalmente utilizaron información lingüística como características.",
        "Informaron de una alta precisión de extracción de información de artículos de investigación en términos de precisión y recuperación. 2.2 Extracción de Información La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la cual, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados.",
        "Los Modelos Ocultos de Markov [6], el Modelo de Entropía Máxima [1, 4], el Modelo de Entropía Máxima de Markov [17], las Máquinas de Vectores de Soporte [3], el Campo Aleatorio Condicional [12] y el Perceptrón Votado [2] son modelos ampliamente utilizados para la extracción de información.",
        "La extracción de información se ha aplicado, por ejemplo, al etiquetado de partes del discurso [20], al reconocimiento de entidades nombradas [25] y a la extracción de tablas [19]. 2.3 Búsqueda utilizando la información del título La información del título es útil para la recuperación de documentos.",
        "En el sistema Citeseer, por ejemplo, Giles et al. lograron extraer títulos de artículos de investigación y utilizar los títulos extraídos en la búsqueda de metadatos de los artículos [8].",
        "En la búsqueda web, los campos de título (es decir, propiedades de archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5].",
        "Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22].",
        "Zhang et al. encontraron que las páginas web con metadatos bien definidos son más fáciles de recuperar que aquellas sin metadatos bien definidos [24].",
        "Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de Office) para la búsqueda de los documentos. 146 3.",
        "MOTIVACIÓN Y DEFINICIÓN DEL PROBLEMA Consideramos el problema de extraer automáticamente títulos de documentos generales.",
        "Por documentos generales, nos referimos a documentos que pertenecen a uno de varios géneros específicos.",
        "Los documentos pueden ser presentaciones, libros, capítulos de libros, artículos técnicos, folletos, informes, memorandos, especificaciones, cartas, anuncios o currículums.",
        "Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigar la extracción de títulos de los mismos.",
        "La Figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e internet [15].",
        "Office y PDF son los principales formatos de archivo en la intranet.",
        "Incluso en internet, los documentos en los formatos siguen siendo significativos, dada su tamaño extremadamente grande.",
        "En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de Office.",
        "Figura 1.",
        "Distribuciones de formatos de archivo en internet e intranet.",
        "Para los documentos de Office, los usuarios pueden definir títulos como propiedades de archivo utilizando una función proporcionada por Office.",
        "Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la función y, por lo tanto, los títulos en las propiedades de los archivos suelen ser muy inexactos.",
        "Es decir, los títulos en las propiedades del archivo suelen ser inconsistentes con los verdaderos títulos en los cuerpos de los archivos que son creados por los autores y son visibles para los lectores.",
        "Recopilamos 6,000 documentos de Word y 6,000 documentos de PowerPoint de una intranet y de internet, y examinamos cuántos títulos en las propiedades de los archivos son correctos.",
        "Descubrimos que sorprendentemente la precisión fue solo de 0.265 (cf., Sección 6.3 para más detalles).",
        "Se pueden considerar varias razones.",
        "Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, entonces la propiedad de archivo del nuevo archivo también se copiará del archivo antiguo.",
        "En otro experimento, descubrimos que Google utiliza los títulos en las propiedades de archivo de documentos de Office en la búsqueda y navegación, pero los títulos no son muy precisos.",
        "Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devueltos por Google.",
        "Descubrimos que casi todos los títulos presentados en los resultados de búsqueda eran de las propiedades de archivo de los documentos.",
        "Sin embargo, solo 0.272 de ellos fueron correctos.",
        "De hecho, los títulos verdaderos suelen encontrarse al principio de los cuerpos de los documentos.",
        "Si podemos extraer con precisión los títulos de los cuerpos de los documentos, entonces podemos aprovechar la información de título confiable en el procesamiento de documentos.",
        "Este es exactamente el problema que abordamos en este artículo.",
        "Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página.",
        "Dado un documento de PowerPoint, debemos extraer el título de la primera diapositiva.",
        "Un título a veces consiste en un título principal y uno o dos subtítulos.",
        "Solo consideramos la extracción del título principal.",
        "Como líneas base para la extracción de títulos, utilizamos la de siempre usar las primeras líneas como títulos y la de siempre usar las líneas con tamaños de fuente más grandes como títulos.",
        "Figura 2.",
        "Extracción de título de documento de Word.",
        "Figura 3.",
        "Extracción de títulos de un documento de PowerPoint.",
        "A continuación, definimos una especificación para los juicios humanos en la anotación de datos de títulos.",
        "Los datos anotados se utilizarán en el entrenamiento y prueba de los métodos de extracción de títulos.",
        "Resumen de la especificación: El título de un documento debe ser identificado en base al sentido común, si no hay dificultad en la identificación.",
        "Sin embargo, hay muchos casos en los que la identificación no es fácil.",
        "Hay algunas reglas definidas en la especificación que guían la identificación para tales casos.",
        "Las reglas incluyen que un título suele estar en líneas consecutivas en el mismo formato, un documento puede no tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como borrador, 147 whitepaper, etc., si es difícil determinar cuál es el título, seleccionar el que tenga el tamaño de fuente más grande y, si aún es difícil determinar cuál es el título, seleccionar el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos).",
        "Las figuras 2 y 3 muestran ejemplos de documentos de Office de los cuales realizamos la extracción de títulos.",
        "En la Figura 2, Diferencias en las Implementaciones de la API de Win32 entre los Sistemas Operativos de Windows es el título del documento de Word.",
        "En la parte superior de esta página hay una imagen de Microsoft Windows que por lo tanto es ignorada.",
        "En la Figura 3, \"Construyendo Ventajas Competitivas a través de una Infraestructura Ágil\" es el título del documento de PowerPoint.",
        "Hemos desarrollado una herramienta para la anotación de títulos por anotadores humanos.",
        "La Figura 4 muestra una captura de pantalla de la herramienta.",
        "Figura 4.",
        "Herramienta de anotación de títulos. 4.",
        "MÉTODO DE EXTRACCIÓN DE TÍTULOS 4.1 El método de extracción de títulos basado en aprendizaje automático consta de entrenamiento y extracción.",
        "El mismo paso de preprocesamiento ocurre antes del entrenamiento y la extracción.",
        "Durante el preprocesamiento, se extraen una serie de unidades para el procesamiento de la región superior de la primera página de un documento de Word o de la primera diapositiva de un documento de PowerPoint.",
        "Si una línea (las líneas están separadas por símbolos de retorno) solo tiene un único formato, entonces la línea se convertirá en una unidad.",
        "Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad.",
        "Cada unidad será tratada como una instancia en el aprendizaje.",
        "Una unidad contiene no solo información de contenido (información lingüística) sino también información de formato.",
        "La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias).",
        "La Figura 5 muestra las unidades obtenidas del documento en la Figura 2.",
        "Figura 5.",
        "Ejemplo de unidades.",
        "En el aprendizaje, la entrada son secuencias de unidades donde cada secuencia corresponde a un documento.",
        "Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end u otras) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin, title_end u otra.",
        "Empleamos cuatro tipos de modelos: Perceptrón, Entropía Máxima (ME), Modelo de Perceptrón de Markov (PMM) y Modelo de Entropía Máxima de Markov (MEMM).",
        "En la extracción, la entrada es una secuencia de unidades de un documento.",
        "Empleamos un tipo de modelo para identificar si una unidad es título_inicio, título_fin u otro.",
        "Luego extraemos las unidades desde la unidad etiquetada con title_begin hasta la unidad etiquetada con title_end.",
        "El resultado es el título extraído del documento.",
        "La característica única de nuestro enfoque es que principalmente utilizamos información de formato para la extracción de títulos.",
        "Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar esos patrones para la extracción de títulos.",
        "Esto contrasta con el trabajo de Han et al., en el cual solo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos De hecho, los cuatro modelos pueden considerarse dentro del mismo marco de extracción de metadatos.",
        "Por eso los aplicamos juntos a nuestro problema actual.",
        "Cada entrada es una secuencia de instancias kxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki ,,2,1 L=).",
        "Recuerda que una instancia aquí representa una unidad.",
        "Una etiqueta representa title_begin, title_end, u otro.",
        "Aquí, k es el número de unidades en un documento.",
        "En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )|( 11 kk XXYYP LL donde iX e iY denotan variables aleatorias que toman instancias ix e etiquetas iy como valores, respectivamente ( ki ,,2,1 L= ).",
        "Herramienta de extracción de herramientas de aprendizaje 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Distribución condicional mkmm xxx L21 Figura 6.",
        "Modelo de extracción de metadatos.",
        "Podemos hacer suposiciones sobre el modelo general para simplificarlo lo suficiente para el entrenamiento. Por ejemplo, podemos asumir que kYY ,,1 L son independientes entre sí dado kXX ,,1 L.",
        "De esta manera, descomponemos el modelo en varios clasificadores.",
        "Entrenamos los clasificadores localmente utilizando los datos etiquetados.",
        "Como clasificador, empleamos el modelo Perceptrón o de Máxima Entropía.",
        "También podemos asumir que la propiedad de Markov de primer orden se cumple para kYY ,,1 L dado kXX ,,1 L.",
        "Por lo tanto, obtenemos una serie de clasificadores.",
        "Sin embargo, los clasificadores están condicionados por la etiqueta previa.",
        "Cuando empleamos el modelo Perceptrón o de Entropía Máxima como clasificador, los modelos se convierten en un Modelo Markov Perceptrón o un Modelo Markov de Entropía Máxima, respectivamente.",
        "Es decir, los dos modelos son más precisos.",
        "En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción.",
        "Para Perceptrón y ME, asignamos etiquetas localmente y luego combinamos los resultados globalmente utilizando heurísticas.",
        "Específicamente, primero identificamos el título más probable.",
        "Entonces encontramos el título más probable dentro de tres unidades después del título inicial.",
        "Finalmente, extraemos como título las unidades entre title_begin y title_end.",
        "Para PMM y MEMM, empleamos el algoritmo de Viterbi para encontrar la secuencia de etiquetas globalmente óptima.",
        "En este artículo, para el Perceptrón, en realidad empleamos una variante mejorada de él, llamada Perceptrón con Margen Desigual [13].",
        "Esta versión de Perceptrón puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren considerablemente, que es exactamente el caso en nuestro problema.",
        "También empleamos una versión mejorada del Modelo Perceptrón Markov en la cual el modelo Perceptrón es el llamado Perceptrón Votado [2].",
        "Además, en el entrenamiento, los parámetros del modelo se actualizan de forma global en lugar de localmente. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas.",
        "Principalmente usamos el primero.",
        "Las características se utilizan tanto para los clasificadores de inicio de título como para los de fin de título. 4.3.1 Características de formato Tamaño de fuente: Hay cuatro características binarias que representan el tamaño de fuente normalizado de la unidad (recordemos que una unidad tiene solo un tipo de fuente).",
        "Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario, 0.",
        "Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario, 0.",
        "Si el tamaño de la fuente es mayor que el tamaño de fuente promedio y no es el más grande en el documento, entonces la segunda característica será 1, de lo contrario, 0.",
        "Si el tamaño de la fuente es inferior al tamaño promedio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario, 0.",
        "Es necesario realizar la normalización de los tamaños de fuente.",
        "Por ejemplo, en un documento el tamaño de fuente más grande podría ser de 12pt, mientras que en otro el más pequeño podría ser de 18pt.",
        "Negrita: Esta característica binaria representa si la unidad actual está en negrita o no.",
        "Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida.",
        "El siguiente formato de características con respecto al contexto juega un papel importante en la extracción de títulos.",
        "Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco.",
        "Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la siguiente unidad difieren del de la unidad actual.",
        "Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la siguiente difieren de la de la actual.",
        "Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad siguiente están en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave.",
        "Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas.",
        "Las palabras positivas incluyen título:, asunto:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos.",
        "Sin embargo, si las líneas comienzan con una de las palabras positivas, es probable que sean líneas de título.",
        "Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas.",
        "Las palabras negativas incluyen To, By, creado por, actualizado por, etc.",
        "Hay más palabras negativas que positivas.",
        "Las características lingüísticas mencionadas anteriormente dependen del idioma.",
        "Recuento de palabras: Un título no debe ser demasiado largo.",
        "Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, ∞) y definimos una característica para cada intervalo.",
        "Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario, 0.",
        "Carácter de finalización: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales.",
        "Un título generalmente no termina con un carácter como ese. 5.",
        "MÉTODO DE RECUPERACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos.",
        "Normalmente, en la recuperación de información, un documento se divide en varios campos que incluyen cuerpo, título y texto de anclaje.",
        "Una función de clasificación en la búsqueda puede utilizar diferentes pesos para diferentes campos del documento.",
        "Además, los títulos suelen recibir un peso alto, lo que indica que son importantes para la recuperación de documentos.",
        "Como se explicó anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento.",
        "Al hacer esto, intentamos mejorar la precisión general.",
        "En este artículo, empleamos una modificación de BM25 que permite la ponderación de campos [21].",
        "Como campos, hacemos uso del cuerpo, título, título extraído y ancla.",
        "Primero, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; luego, cada frecuencia de campo se pondera según el parámetro de peso correspondiente: ∑= f tfft tfwwtf De manera similar, calculamos la longitud del documento como una suma ponderada de las longitudes de cada campo.",
        "La longitud promedio del documento en el corpus se convierte en el promedio de todas las longitudes de documentos ponderadas. ∑= f ff dlwwdl En nuestros experimentos usamos 75.0,8.11 == bk.",
        "El peso para el contenido fue de 1.0, para el título fue de 10.0, para el enlace fue de 10.0 y para el título extraído fue de 5.0.",
        "RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación. Utilizamos dos conjuntos de datos en nuestros experimentos.",
        "Primero, descargamos y seleccionamos aleatoriamente 5,000 documentos de Word y 5,000 documentos de PowerPoint de una intranet de Microsoft.",
        "Lo llamamos MS de ahora en adelante.",
        "Segundo, descargamos y seleccionamos aleatoriamente 500 documentos de Word y 500 documentos de PowerPoint de los dominios DotGov y DotCom en internet, respectivamente.",
        "La Figura 7 muestra las distribuciones de los géneros de los documentos.",
        "Vemos que los documentos son, de hecho, documentos generales tal como los definimos.",
        "Figura 7.",
        "Distribuciones de géneros de documentos.",
        "Tercero, también se descargó un conjunto de datos en chino de internet.",
        "Incluye 500 documentos de Word y 500 documentos de PowerPoint en chino.",
        "Etiquetamos manualmente los títulos de todos los documentos, basándonos en nuestra especificación.",
        "No todos los documentos en los dos conjuntos de datos tienen títulos.",
        "La Tabla 1 muestra los porcentajes de los documentos que tienen títulos.",
        "Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS.",
        "Esto podría deberse a que los documentos de PowerPoint publicados en internet son más formales que los de la intranet.",
        "Tabla 1.",
        "En nuestros experimentos, realizamos evaluaciones sobre la extracción de títulos en términos de precisión, recuperación y medida F.",
        "Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / (A + B) Recall: R = A / (A + C) F-measure: F1 = 2PR / (P + R) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2.",
        "Tabla 2.",
        "Tabla de contingencia con respecto a la extracción de títulos. ¿Es título? ¿No es título? Extraído A B No extraído C D 6.2 Baselines Probamos las precisiones de los dos baselines descritos en la sección 4.2.",
        "Se denominan como el tamaño de fuente más grande y la primera línea respectivamente. 6.3 Precisión de los títulos en las propiedades del archivo Investigamos cuántos títulos en las propiedades del archivo de los documentos son confiables.",
        "Consideramos los títulos anotados por humanos como títulos verdaderos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos.",
        "Utilizamos la Distancia de Edición para realizar la coincidencia aproximada. (La coincidencia aproximada solo se utiliza en esta evaluación).",
        "Esto se debe a que a veces los títulos anotados por humanos pueden ser ligeramente diferentes de los títulos en las propiedades del archivo en la superficie, por ejemplo, contener espacios adicionales.",
        "Dadas las cadenas A y B: si ((D == 0) o (D / (La + Lb) < θ)) entonces la cadena A = cadena B D: Distancia de edición entre la cadena A y la cadena B La: longitud de la cadena A Lb: longitud de la cadena B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Tabla 3.",
        "Precisión de los títulos en las propiedades del archivo Tipo de archivo Dominio Precisión Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186 PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparación con Baselines Realizamos la extracción de títulos del primer conjunto de datos (Word y PowerPoint en MS).",
        "Como modelo, utilizamos el Perceptrón.",
        "Realizamos validación cruzada de 4 pliegues.",
        "Por lo tanto, todos los resultados reportados aquí son aquellos promediados en 4 ensayos.",
        "Las tablas 4 y 5 muestran los resultados.",
        "Vemos que el Perceptrón supera significativamente a los baselines.",
        "En la evaluación, utilizamos coincidencia exacta entre los títulos reales anotados por humanos y los títulos extraídos.",
        "Tabla 4.",
        "Precisión de la extracción de títulos con el Modelo Perceptrón de Palabra Precisión Recall F1 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Líneas base Primera línea 0.707 0.767 0.736 Tabla 5.",
        "Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos.",
        "Para los documentos de Word, tanto la precisión como la exhaustividad del enfoque son un 8 por ciento más altas que las de las líneas de base.",
        "Para PowerPoint, tanto la precisión como la exhaustividad del enfoque son un 2 por ciento más altas que las de las líneas de base.",
        "Realizamos pruebas de significancia.",
        "Los resultados se muestran en la Tabla 6.",
        "Aquí, \"Largest\" denota la línea base de usar el tamaño de fuente más grande, \"First\" denota la línea base de usar la primera línea.",
        "Los resultados indican que las mejoras del aprendizaje automático sobre las líneas de base son estadísticamente significativas (en el sentido de que el valor p < 0.05) Tabla 6.",
        "Vemos, a partir de los resultados, que los dos baselines pueden funcionar bien para la extracción de títulos, lo que sugiere que el tamaño de fuente y la información de posición son las características más útiles para la extracción de títulos.",
        "Sin embargo, también es evidente que utilizar solo estas dos características no es suficiente.",
        "Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande solo contienen descripciones generales como Confidencial, Documento blanco, etc.",
        "Para esos casos, el método del tamaño de fuente más grande no puede funcionar bien.",
        "Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien.",
        "Con la combinación de diferentes características (evidencia en el juicio del título), el Perceptrón puede superar a Largest y First.",
        "Investigamos el rendimiento de utilizar únicamente características lingüísticas.",
        "Descubrimos que no funciona bien.",
        "Parece que las características del formato desempeñan roles importantes y las características lingüísticas son complementos.",
        "Figura 8.",
        "Un documento de Word de ejemplo.",
        "Figura 9.",
        "Un documento de PowerPoint de ejemplo.",
        "Realizamos un análisis de errores en los resultados del Perceptrón.",
        "Encontramos que los errores caían en tres categorías. (1) Aproximadamente un tercio de los errores estaban relacionados con casos difíciles.",
        "En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos.",
        "Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores provienen de los documentos que no tienen títulos verdaderos, sino que solo contienen viñetas.",
        "Dado que realizamos la extracción desde las regiones superiores, es difícil deshacernos de estos errores con el enfoque actual.",
        "Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error.",
        "Dado que solo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas.",
        "Este tipo de error no afecta mucho al procesamiento de documentos como la búsqueda. 6.5 Comparación entre Modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento.",
        "Nuevamente, realizamos una validación cruzada de 4 pliegues en el primer conjunto de datos (MS).",
        "La tabla 7 y 8 muestran los resultados de los cuatro modelos.",
        "Resulta que Perceptrón y PMM tienen el mejor rendimiento, seguidos por MEMM, y ME tiene el peor rendimiento.",
        "En general, los modelos markovianos tienen un mejor rendimiento que sus contrapartes clasificadoras o igual de bueno.",
        "Esto parece ser porque los modelos markovianos se entrenan de forma global, mientras que los clasificadores se entrenan de forma local.",
        "Los modelos basados en el Perceptrón tienen un mejor rendimiento que sus contrapartes basadas en el ME.",
        "Esto parece ser porque los modelos basados en Perceptrón están creados para realizar mejores clasificaciones, mientras que los modelos de ME se construyen para una mejor predicción.",
        "Tabla 7.",
        "Comparación entre diferentes modelos de aprendizaje para la extracción de títulos con Modelo de Palabra Precisión Recall F1 Perceptrón 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Tabla 8.",
        "Comparación entre diferentes modelos de aprendizaje para la extracción de títulos con el Modelo de PowerPoint Precisión Recall F1 Perceptrón 0.875 0.895 0.885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov).",
        "Las tablas 9-12 muestran los resultados.",
        "Tabla 9.",
        "Precisión de la extracción de títulos con Word en DotGov Precisión Recall F1 Modelo Perceptrón 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582 Baselines Primera línea 0.462 0.521 0.490 Tabla 10.",
        "Precisión de la extracción de títulos con PowerPoint en DotGov Precision Recall F1 Modelo Perceptrón 0.900 0.906 0.903 Tamaño de fuente más grande 0.871 0.888 0.879 Baselines Primera línea 0.554 0.564 0.559 Tabla 11.",
        "Precisión de la extracción de títulos con Word en DotCom Precisión Recall F1 Modelo Perceptrón 0.832 0.880 0.855 Tamaño de fuente más grande 0.676 0.753 0.712 Baselines Primera línea 0.577 0.643 0.608 Tabla 12.",
        "Rendimiento de la extracción de títulos de documentos de PowerPoint en el modelo de Precisión, Recall y F1 de Perceptrón DotCom 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875 Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos pueden adaptarse bien a diferentes dominios.",
        "Casi no hay disminución en la precisión.",
        "Los resultados indican que los patrones de formatos de títulos existen en diferentes dominios, y es posible construir un modelo independiente del dominio utilizando principalmente información de formato. 6.7 Adaptación de idioma Aplicamos el modelo entrenado con los datos en inglés (MS) al conjunto de datos en chino.",
        "Las tablas 13-14 muestran los resultados.",
        "Tabla 13.",
        "Precisión de la extracción de títulos con Word en chino: Precisión Recall F1 Modelo Perceptrón 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738 Baselines Primera línea 0.743 0.777 0.760 Tabla 14.",
        "Vemos que los modelos se pueden adaptar a un idioma diferente.",
        "Solo hay pequeñas caídas en la precisión.",
        "Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no usarlas es insignificante.",
        "Los resultados indican que los patrones de formatos de títulos existen en diferentes idiomas.",
        "A partir de los resultados de adaptación de dominio y adaptación de lenguaje, concluimos que el uso de información de formato es clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con Títulos Extraídos Realizamos experimentos utilizando la extracción de títulos para la recuperación de documentos.",
        "Como línea base, empleamos BM25 sin utilizar títulos extraídos.",
        "El mecanismo de clasificación fue como se describe en la Sección 5.",
        "Los pesos fueron establecidos heurísticamente.",
        "No realizamos la optimización de los pesos.",
        "La evaluación se llevó a cabo en un corpus de 1.3 millones de documentos extraídos de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de los registros de consultas del motor de búsqueda de esta intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 consultas fueron elegidas al azar.",
        "A los usuarios se les pidió que proporcionaran juicios sobre el grado de relevancia del documento en una escala del 1 al 5 (1 significando perjudicial, 2 - malo, 3 - regular, 4 - bueno y 5 - excelente). La figura 10 muestra los resultados.",
        "En el gráfico se obtuvieron dos conjuntos de resultados de precisión al considerar documentos buenos o excelentes como relevantes (tres barras izquierdas con umbral de relevancia 0.5), o al considerar solo documentos excelentes como relevantes (tres barras derechas con umbral de relevancia 1.0).",
        "Resultados de clasificación de búsqueda.",
        "La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo los campos cuerpo, título (propiedad de archivo) y texto de anclaje. • Barra morada - BM25 incluyendo los campos cuerpo, título (propiedad de archivo), texto de anclaje y título extraído.",
        "Con el campo adicional de título extraído incluido en BM25, la precisión @10 aumentó de 0.132 a 0.145, o aproximadamente un 10%.",
        "Por lo tanto, se puede afirmar que el uso del título extraído puede mejorar efectivamente la precisión de la recuperación de documentos. 7.",
        "CONCLUSIÓN En este documento, hemos investigado el problema de extraer automáticamente títulos de documentos generales.",
        "Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema.",
        "Trabajos anteriores mostraron que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de artículos de investigación.",
        "En este artículo, demostramos que el enfoque también puede funcionar para la extracción de documentos generales.",
        "Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas base en la extracción de títulos de documentos de Office.",
        "Trabajos anteriores sobre extracción de metadatos principalmente utilizaron características lingüísticas en los documentos, mientras que nosotros principalmente utilizamos información de formato.",
        "Parecía que el uso de información de formato es clave para llevar a cabo con éxito la extracción de títulos de documentos generales.",
        "Probamos diferentes modelos de aprendizaje automático incluyendo Perceptrón, Entropía Máxima, Modelo de Markov de Entropía Máxima y Perceptrón Votado.",
        "Descubrimos que el rendimiento de los modelos Perceptrón fue el mejor.",
        "Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos entrenados en un idioma a otro idioma.",
        "Encontramos que las precisiones no disminuyeron sustancialmente en diferentes dominios y en diferentes idiomas, lo que indica que los modelos eran genéricos.",
        "También intentamos utilizar los títulos extraídos en la recuperación de documentos.",
        "Observamos una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda al utilizar la información del título extraída.",
        "Todas las investigaciones anteriores no se llevaron a cabo, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción de títulos. 8.",
        "AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos.",
        "Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos.",
        "Agradecemos a Ming Zhou, John Chen, Jun Xu y a los revisores anónimos de JCDL05 por sus valiosos comentarios sobre este artículo. 9.",
        "REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J.",
        "Un enfoque de entropía máxima para el procesamiento del lenguaje natural.",
        "Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminativo para modelos ocultos de Markov: teoría y experimentos con algoritmos de perceptrón.",
        "En Actas de la Conferencia sobre Métodos Empíricos en Procesamiento de Lenguaje Natural, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de vectores de soporte.",
        "Aprendizaje automático, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información de texto semiestructurado y libre.",
        "En Actas de la Decimoctava Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia newsblaster: resumen multilingüe de noticias en la web.",
        "En Actas de la conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos ocultos de Markov factorial.",
        "Aprendizaje automático, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de Visualización de Información de 1999, 446-451, 1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., y Pal, N. eBizSearch: un motor de búsqueda de nicho para e-Business.",
        "En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 413-414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. Extracción de metadatos basada en conocimiento de archivos PostScript.",
        "En Actas de la Quinta Conferencia de Bibliotecas Digitales de la ACM, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A.",
        "Extracción automática de metadatos de documentos utilizando máquinas de vectores de soporte.",
        "En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web.",
        "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y etiquetar datos de secuencias.",
        "En Actas de la Decimoctava Conferencia Internacional sobre Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo del perceptrón con márgenes desiguales.",
        "En Actas de la Decimonovena Conferencia Internacional sobre Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J.",
        "Generación y evaluación automática de metadatos.",
        "En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 401-402, 2002. [15] Littlefield, A.",
        "Recuperación efectiva de información empresarial en nuevos formatos de contenido.",
        "En Actas de la Séptima Conferencia de Motores de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., y Thoma, G. R. Un sistema de generación de características dinámicas para la extracción automatizada de metadatos en la preservación de materiales digitales.",
        "En Actas del Primer Taller Internacional sobre Análisis de Imágenes de Documentos para Bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de máxima entropía de Markov para extracción de información y segmentación.",
        "En Actas de la Decimoséptima Conferencia Internacional sobre Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación.",
        "En Actas de la Trigésimo-Primera Conferencia Internacional Anual de Ciencias de Sistemas de Hawái, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B.",
        "Extracción de tablas utilizando campos aleatorios condicionales.",
        "En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para la unión de frases preposicionales.",
        "En Actas de la Decimoséptima Conferencia Internacional de Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Extensión simple de BM25 a múltiples campos ponderados, En Actas de la Decimotercera Conferencia de ACM sobre Información y Gestión del Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Minería web basada en metadatos para relevancia, En Actas del Simposio Internacional de Ingeniería y Aplicaciones de Bases de Datos 2000, 113-121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema de procesamiento de lenguaje natural para asignar automáticamente metadatos.",
        "En Actas de la Conferencia Conjunta ACM/IEEE sobre Bibliotecas Digitales de 2004, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda de Internet a la implementación de metadatos Dublin Core.",
        "Revista de Ciencia de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y., y Zhang, T. Reconocimiento y uso de entidades nombradas: reconocimiento de entidades nombradas enfocado utilizando aprendizaje automático.",
        "En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
    ],
    "error_count": 10,
    "keys": {
        "title extraction": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to <br>title extraction</br> from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for <br>title extraction</br> from research papers.",
                "It has not been clear whether it could be possible to conduct automatic <br>title extraction</br> from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform <br>title extraction</br> using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for <br>title extraction</br> from Word is 0.810 and 0.837 respectively, and precision and recall for <br>title extraction</br> from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider <br>title extraction</br> in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform <br>title extraction</br> using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for <br>title extraction</br>; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for <br>title extraction</br> from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for <br>title extraction</br> from Word are 0.810 and 0.837 respectively, and precision and recall for <br>title extraction</br> from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful <br>title extraction</br>. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable <br>title extraction</br> from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of <br>title extraction</br>, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on <br>title extraction</br> from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for <br>title extraction</br>, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "<br>title extraction</br> from Word document.",
                "Figure 3.",
                "<br>title extraction</br> from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the <br>title extraction</br> methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct <br>title extraction</br>.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "<br>title extraction</br> METHOD 4.1 Outline <br>title extraction</br> based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for <br>title extraction</br>.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for <br>title extraction</br>.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in <br>title extraction</br>.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on <br>title extraction</br> in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to <br>title extraction</br> Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted <br>title extraction</br> from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of <br>title extraction</br> with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of <br>title extraction</br> with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in <br>title extraction</br>.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for <br>title extraction</br>, suggesting that font size and position information are most useful features for <br>title extraction</br>.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for <br>title extraction</br> with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for <br>title extraction</br> with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of <br>title extraction</br> with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of <br>title extraction</br> with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of <br>title extraction</br> with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document <br>title extraction</br> in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of <br>title extraction</br> with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of <br>title extraction</br> with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using <br>title extraction</br> for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in <br>title extraction</br> from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting <br>title extraction</br> from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the <br>title extraction</br> approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to <br>title extraction</br> from general documents.",
                "Previously, methods have been proposed mainly for <br>title extraction</br> from research papers.",
                "It has not been clear whether it could be possible to conduct automatic <br>title extraction</br> from general documents.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform <br>title extraction</br> using the trained models.",
                "Precision and recall for <br>title extraction</br> from Word is 0.810 and 0.837 respectively, and precision and recall for <br>title extraction</br> from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data."
            ],
            "translated_annotated_samples": [
                "En este documento, proponemos un enfoque de aprendizaje automático para la <br>extracción de títulos</br> de documentos generales.",
                "Anteriormente, se han propuesto métodos principalmente para la <br>extracción de títulos</br> de artículos de investigación.",
                "No ha quedado claro si sería posible realizar la <br>extracción automática de títulos</br> de documentos generales.",
                "En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenamos modelos de aprendizaje automático y realizamos la <br>extracción de títulos</br> utilizando los modelos entrenados.",
                "La precisión y la exhaustividad para la <br>extracción de títulos</br> de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la <br>extracción de títulos</br> de PowerPoint son 0.875 y 0.895 respectivamente en un experimento con datos de intranet."
            ],
            "translated_text": "Extracción automática de títulos de documentos generales utilizando aprendizaje automático. En este documento, proponemos un enfoque de aprendizaje automático para la <br>extracción de títulos</br> de documentos generales. Por documentos generales nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos, incluyendo presentaciones, capítulos de libros, artículos técnicos, folletos, informes y cartas. Anteriormente, se han propuesto métodos principalmente para la <br>extracción de títulos</br> de artículos de investigación. No ha quedado claro si sería posible realizar la <br>extracción automática de títulos</br> de documentos generales. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenamos modelos de aprendizaje automático y realizamos la <br>extracción de títulos</br> utilizando los modelos entrenados. Nuestro método es único en que principalmente utilizamos información de formato como el tamaño de fuente como características en los modelos. Resulta que el uso de información de formato puede llevar a una extracción bastante precisa de documentos generales. La precisión y la exhaustividad para la <br>extracción de títulos</br> de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la <br>extracción de títulos</br> de PowerPoint son 0.875 y 0.895 respectivamente en un experimento con datos de intranet. ",
            "candidates": [],
            "error": [
                [
                    "extracción de títulos",
                    "extracción de títulos",
                    "extracción automática de títulos",
                    "extracción de títulos",
                    "extracción de títulos",
                    "extracción de títulos"
                ]
            ]
        },
        "formatting information": {
            "translated_key": "información de formato",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize <br>formatting information</br> such as font size as features in the models.",
                "It turns out that the use of <br>formatting information</br> can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize <br>formatting information</br> such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on <br>formatting information</br>.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also <br>formatting information</br>.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize <br>formatting information</br> for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using <br>formatting information</br>. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of <br>formatting information</br> is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used <br>formatting information</br>.",
                "It appeared that using <br>formatting information</br> is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [
                "Our method is unique in that we mainly utilize <br>formatting information</br> such as font size as features in the models.",
                "It turns out that the use of <br>formatting information</br> can lead to quite accurate extraction from general documents.",
                "In the models, we mainly utilize <br>formatting information</br> such as font size as features.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on <br>formatting information</br>.",
                "A unit contains not only content information (linguistic information) but also <br>formatting information</br>."
            ],
            "translated_annotated_samples": [
                "Nuestro método es único en que principalmente utilizamos <br>información de formato</br> como el tamaño de fuente como características en los modelos.",
                "Resulta que el uso de <br>información de formato</br> puede llevar a una extracción bastante precisa de documentos generales.",
                "En los modelos, principalmente utilizamos <br>información de formato</br> como el tamaño de fuente como características.",
                "Mao et al. [16] también llevaron a cabo la extracción automática de metadatos de artículos de investigación utilizando reglas sobre la <br>información de formato</br>.",
                "Una unidad contiene no solo información de contenido (información lingüística) sino también <br>información de formato</br>."
            ],
            "translated_text": "Extracción automática de títulos de documentos generales utilizando aprendizaje automático. En este documento, proponemos un enfoque de aprendizaje automático para la extracción de títulos de documentos generales. Por documentos generales nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos, incluyendo presentaciones, capítulos de libros, artículos técnicos, folletos, informes y cartas. Anteriormente, se han propuesto métodos principalmente para la extracción de títulos de artículos de investigación. No ha quedado claro si sería posible realizar la extracción automática de títulos de documentos generales. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que principalmente utilizamos <br>información de formato</br> como el tamaño de fuente como características en los modelos. Resulta que el uso de <br>información de formato</br> puede llevar a una extracción bastante precisa de documentos generales. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente en un experimento con datos de intranet. Otros hallazgos importantes en este trabajo incluyen que podemos entrenar modelos en un dominio y aplicarlos a otro dominio, y más sorprendentemente, incluso podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente los resultados de clasificación de búsqueda en la recuperación de documentos utilizando los títulos extraídos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de Búsqueda; H.4.1 [Aplicaciones de Sistemas de Información]: Automatización de Oficinas - Procesamiento de Texto; D.2.8 [Ingeniería de Software]: Métricas - medidas de complejidad, medidas de rendimiento Términos Generales Algoritmos, Experimentación, Rendimiento. 1. METADATA La información de metadatos de los documentos es útil para muchos tipos de procesamiento de documentos, como la búsqueda, la navegación y el filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen los metadatos de los documentos por sí mismas, incluso cuando tienen herramientas convenientes para la definición de metadatos [26]. Por lo tanto, la extracción automática de metadatos de los cuerpos de los documentos resulta ser un tema de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, el enfoque estaba principalmente en la extracción de los documentos de investigación. Por ejemplo, Han et al. [10] propusieron un método basado en aprendizaje automático para realizar extracciones de artículos de investigación. Formalizaron el problema como el de clasificación y emplearon Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron características lingüísticas en el modelo. En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigación sobre la extracción de información de ellos. Los artículos de investigación suelen tener estilos bien formados y características notables. Por el contrario, los estilos de los documentos generales pueden variar considerablemente. No se ha aclarado si un enfoque basado en aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como estudio de caso, consideramos la extracción de títulos en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizamos la extracción de títulos utilizando uno de los tipos de modelos entrenados. En los modelos, principalmente utilizamos <br>información de formato</br> como el tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con Márgenes Desiguales, Modelo de Entropía Máxima de Markov y Perceptrón Votado. En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, ¿cuál modelo funciona mejor para la extracción de títulos?; (2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a los baselines: uno que siempre utiliza las primeras líneas como títulos y el otro que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente. Resulta que el uso de características de formato es la clave para la exitosa extracción de títulos. (1) Hemos observado que los modelos basados en Perceptrón tienen un mejor rendimiento en términos de precisión de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados en otro, y pueden ser entrenados en un idioma y aplicados en otro. (3) Hemos descubierto que al utilizar los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que podemos realizar de manera fiable la extracción de títulos de documentos generales y utilizar los resultados extraídos para mejorar aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, presentamos el trabajo relacionado, y en la sección 3, explicamos la motivación y el contexto del problema de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando los títulos extraídos. La sección 6 presenta nuestros resultados experimentales. Hacemos observaciones finales en la sección 7.2. TRABAJO RELACIONADO 2.1 Se han propuesto métodos de extracción de metadatos de documentos para realizar extracción automática de metadatos de documentos; sin embargo, el enfoque principal ha sido la extracción de artículos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en reglas y el enfoque basado en aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrollaron un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como que los títulos suelen ubicarse en las partes superiores de las primeras páginas y suelen estar en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel et al. [23] realizaron extracción de metadatos de materiales educativos utilizando tecnologías de procesamiento del lenguaje natural basadas en reglas. Mao et al. [16] también llevaron a cabo la extracción automática de metadatos de artículos de investigación utilizando reglas sobre la <br>información de formato</br>. El enfoque basado en reglas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, realizaron extracción de metadatos con enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron información lingüística como características. Informaron de una alta precisión de extracción de información de artículos de investigación en términos de precisión y recuperación. 2.2 Extracción de Información La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la cual, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Los Modelos Ocultos de Markov [6], el Modelo de Entropía Máxima [1, 4], el Modelo de Entropía Máxima de Markov [17], las Máquinas de Vectores de Soporte [3], el Campo Aleatorio Condicional [12] y el Perceptrón Votado [2] son modelos ampliamente utilizados para la extracción de información. La extracción de información se ha aplicado, por ejemplo, al etiquetado de partes del discurso [20], al reconocimiento de entidades nombradas [25] y a la extracción de tablas [19]. 2.3 Búsqueda utilizando la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. lograron extraer títulos de artículos de investigación y utilizar los títulos extraídos en la búsqueda de metadatos de los artículos [8]. En la búsqueda web, los campos de título (es decir, propiedades de archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al. encontraron que las páginas web con metadatos bien definidos son más fáciles de recuperar que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de Office) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y DEFINICIÓN DEL PROBLEMA Consideramos el problema de extraer automáticamente títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de varios géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, artículos técnicos, folletos, informes, memorandos, especificaciones, cartas, anuncios o currículums. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigar la extracción de títulos de los mismos. La Figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e internet [15]. Office y PDF son los principales formatos de archivo en la intranet. Incluso en internet, los documentos en los formatos siguen siendo significativos, dada su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de Office. Figura 1. Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir títulos como propiedades de archivo utilizando una función proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la función y, por lo tanto, los títulos en las propiedades de los archivos suelen ser muy inexactos. Es decir, los títulos en las propiedades del archivo suelen ser inconsistentes con los verdaderos títulos en los cuerpos de los archivos que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 documentos de Word y 6,000 documentos de PowerPoint de una intranet y de internet, y examinamos cuántos títulos en las propiedades de los archivos son correctos. Descubrimos que sorprendentemente la precisión fue solo de 0.265 (cf., Sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, entonces la propiedad de archivo del nuevo archivo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en las propiedades de archivo de documentos de Office en la búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devueltos por Google. Descubrimos que casi todos los títulos presentados en los resultados de búsqueda eran de las propiedades de archivo de los documentos. Sin embargo, solo 0.272 de ellos fueron correctos. De hecho, los títulos verdaderos suelen encontrarse al principio de los cuerpos de los documentos. Si podemos extraer con precisión los títulos de los cuerpos de los documentos, entonces podemos aprovechar la información de título confiable en el procesamiento de documentos. Este es exactamente el problema que abordamos en este artículo. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, debemos extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Solo consideramos la extracción del título principal. Como líneas base para la extracción de títulos, utilizamos la de siempre usar las primeras líneas como títulos y la de siempre usar las líneas con tamaños de fuente más grandes como títulos. Figura 2. Extracción de título de documento de Word. Figura 3. Extracción de títulos de un documento de PowerPoint. A continuación, definimos una especificación para los juicios humanos en la anotación de datos de títulos. Los datos anotados se utilizarán en el entrenamiento y prueba de los métodos de extracción de títulos. Resumen de la especificación: El título de un documento debe ser identificado en base al sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en los que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación para tales casos. Las reglas incluyen que un título suele estar en líneas consecutivas en el mismo formato, un documento puede no tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como borrador, 147 whitepaper, etc., si es difícil determinar cuál es el título, seleccionar el que tenga el tamaño de fuente más grande y, si aún es difícil determinar cuál es el título, seleccionar el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos). Las figuras 2 y 3 muestran ejemplos de documentos de Office de los cuales realizamos la extracción de títulos. En la Figura 2, Diferencias en las Implementaciones de la API de Win32 entre los Sistemas Operativos de Windows es el título del documento de Word. En la parte superior de esta página hay una imagen de Microsoft Windows que por lo tanto es ignorada. En la Figura 3, \"Construyendo Ventajas Competitivas a través de una Infraestructura Ágil\" es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotadores humanos. La Figura 4 muestra una captura de pantalla de la herramienta. Figura 4. Herramienta de anotación de títulos. 4. MÉTODO DE EXTRACCIÓN DE TÍTULOS 4.1 El método de extracción de títulos basado en aprendizaje automático consta de entrenamiento y extracción. El mismo paso de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el preprocesamiento, se extraen una serie de unidades para el procesamiento de la región superior de la primera página de un documento de Word o de la primera diapositiva de un documento de PowerPoint. Si una línea (las líneas están separadas por símbolos de retorno) solo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no solo información de contenido (información lingüística) sino también <br>información de formato</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "language independence": {
            "translated_key": "independencia del lenguaje",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "metada of document": {
            "translated_key": "metadatos del documento",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "linguistic feature": {
            "translated_key": "características lingüísticas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used <br>linguistic feature</br>s in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only <br>linguistic feature</br>s are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and <br>linguistic feature</br>s.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The <br>linguistic feature</br>s are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above <br>linguistic feature</br>s are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using <br>linguistic feature</br>s.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the <br>linguistic feature</br>s are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the <br>linguistic feature</br>s do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used <br>linguistic feature</br>s in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [
                "They mainly used <br>linguistic feature</br>s in the model.1 In this paper, we consider metadata extraction from general documents.",
                "This is in contrast to the work by Han et al., in which only <br>linguistic feature</br>s are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and <br>linguistic feature</br>s.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The <br>linguistic feature</br>s are based on key words.",
                "The above <br>linguistic feature</br>s are language dependent."
            ],
            "translated_annotated_samples": [
                "Principalmente utilizaron <br>características lingüísticas</br> en el modelo. En este artículo, consideramos la extracción de metadatos de documentos generales.",
                "Esto contrasta con el trabajo de Han et al., en el cual solo se utilizan <br>características lingüísticas</br> para la extracción de artículos de investigación. 4.2 Modelos De hecho, los cuatro modelos pueden considerarse dentro del mismo marco de extracción de metadatos.",
                "Además, en el entrenamiento, los parámetros del modelo se actualizan de forma global en lugar de localmente. 4.3 Características Hay dos tipos de características: características de formato y <br>características lingüísticas</br>.",
                "Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad siguiente están en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las <br>características lingüísticas</br> se basan en palabras clave.",
                "Las <br>características lingüísticas</br> mencionadas anteriormente dependen del idioma."
            ],
            "translated_text": "Extracción automática de títulos de documentos generales utilizando aprendizaje automático. En este documento, proponemos un enfoque de aprendizaje automático para la extracción de títulos de documentos generales. Por documentos generales nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos, incluyendo presentaciones, capítulos de libros, artículos técnicos, folletos, informes y cartas. Anteriormente, se han propuesto métodos principalmente para la extracción de títulos de artículos de investigación. No ha quedado claro si sería posible realizar la extracción automática de títulos de documentos generales. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que principalmente utilizamos información de formato como el tamaño de fuente como características en los modelos. Resulta que el uso de información de formato puede llevar a una extracción bastante precisa de documentos generales. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente en un experimento con datos de intranet. Otros hallazgos importantes en este trabajo incluyen que podemos entrenar modelos en un dominio y aplicarlos a otro dominio, y más sorprendentemente, incluso podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente los resultados de clasificación de búsqueda en la recuperación de documentos utilizando los títulos extraídos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de Búsqueda; H.4.1 [Aplicaciones de Sistemas de Información]: Automatización de Oficinas - Procesamiento de Texto; D.2.8 [Ingeniería de Software]: Métricas - medidas de complejidad, medidas de rendimiento Términos Generales Algoritmos, Experimentación, Rendimiento. 1. METADATA La información de metadatos de los documentos es útil para muchos tipos de procesamiento de documentos, como la búsqueda, la navegación y el filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen los metadatos de los documentos por sí mismas, incluso cuando tienen herramientas convenientes para la definición de metadatos [26]. Por lo tanto, la extracción automática de metadatos de los cuerpos de los documentos resulta ser un tema de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, el enfoque estaba principalmente en la extracción de los documentos de investigación. Por ejemplo, Han et al. [10] propusieron un método basado en aprendizaje automático para realizar extracciones de artículos de investigación. Formalizaron el problema como el de clasificación y emplearon Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron <br>características lingüísticas</br> en el modelo. En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigación sobre la extracción de información de ellos. Los artículos de investigación suelen tener estilos bien formados y características notables. Por el contrario, los estilos de los documentos generales pueden variar considerablemente. No se ha aclarado si un enfoque basado en aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como estudio de caso, consideramos la extracción de títulos en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizamos la extracción de títulos utilizando uno de los tipos de modelos entrenados. En los modelos, principalmente utilizamos información de formato como el tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con Márgenes Desiguales, Modelo de Entropía Máxima de Markov y Perceptrón Votado. En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, ¿cuál modelo funciona mejor para la extracción de títulos?; (2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a los baselines: uno que siempre utiliza las primeras líneas como títulos y el otro que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente. Resulta que el uso de características de formato es la clave para la exitosa extracción de títulos. (1) Hemos observado que los modelos basados en Perceptrón tienen un mejor rendimiento en términos de precisión de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados en otro, y pueden ser entrenados en un idioma y aplicados en otro. (3) Hemos descubierto que al utilizar los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que podemos realizar de manera fiable la extracción de títulos de documentos generales y utilizar los resultados extraídos para mejorar aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, presentamos el trabajo relacionado, y en la sección 3, explicamos la motivación y el contexto del problema de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando los títulos extraídos. La sección 6 presenta nuestros resultados experimentales. Hacemos observaciones finales en la sección 7.2. TRABAJO RELACIONADO 2.1 Se han propuesto métodos de extracción de metadatos de documentos para realizar extracción automática de metadatos de documentos; sin embargo, el enfoque principal ha sido la extracción de artículos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en reglas y el enfoque basado en aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrollaron un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como que los títulos suelen ubicarse en las partes superiores de las primeras páginas y suelen estar en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel et al. [23] realizaron extracción de metadatos de materiales educativos utilizando tecnologías de procesamiento del lenguaje natural basadas en reglas. Mao et al. [16] también llevaron a cabo la extracción automática de metadatos de artículos de investigación utilizando reglas sobre la información de formato. El enfoque basado en reglas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, realizaron extracción de metadatos con enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron información lingüística como características. Informaron de una alta precisión de extracción de información de artículos de investigación en términos de precisión y recuperación. 2.2 Extracción de Información La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la cual, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Los Modelos Ocultos de Markov [6], el Modelo de Entropía Máxima [1, 4], el Modelo de Entropía Máxima de Markov [17], las Máquinas de Vectores de Soporte [3], el Campo Aleatorio Condicional [12] y el Perceptrón Votado [2] son modelos ampliamente utilizados para la extracción de información. La extracción de información se ha aplicado, por ejemplo, al etiquetado de partes del discurso [20], al reconocimiento de entidades nombradas [25] y a la extracción de tablas [19]. 2.3 Búsqueda utilizando la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. lograron extraer títulos de artículos de investigación y utilizar los títulos extraídos en la búsqueda de metadatos de los artículos [8]. En la búsqueda web, los campos de título (es decir, propiedades de archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al. encontraron que las páginas web con metadatos bien definidos son más fáciles de recuperar que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de Office) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y DEFINICIÓN DEL PROBLEMA Consideramos el problema de extraer automáticamente títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de varios géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, artículos técnicos, folletos, informes, memorandos, especificaciones, cartas, anuncios o currículums. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigar la extracción de títulos de los mismos. La Figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e internet [15]. Office y PDF son los principales formatos de archivo en la intranet. Incluso en internet, los documentos en los formatos siguen siendo significativos, dada su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de Office. Figura 1. Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir títulos como propiedades de archivo utilizando una función proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la función y, por lo tanto, los títulos en las propiedades de los archivos suelen ser muy inexactos. Es decir, los títulos en las propiedades del archivo suelen ser inconsistentes con los verdaderos títulos en los cuerpos de los archivos que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 documentos de Word y 6,000 documentos de PowerPoint de una intranet y de internet, y examinamos cuántos títulos en las propiedades de los archivos son correctos. Descubrimos que sorprendentemente la precisión fue solo de 0.265 (cf., Sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, entonces la propiedad de archivo del nuevo archivo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en las propiedades de archivo de documentos de Office en la búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devueltos por Google. Descubrimos que casi todos los títulos presentados en los resultados de búsqueda eran de las propiedades de archivo de los documentos. Sin embargo, solo 0.272 de ellos fueron correctos. De hecho, los títulos verdaderos suelen encontrarse al principio de los cuerpos de los documentos. Si podemos extraer con precisión los títulos de los cuerpos de los documentos, entonces podemos aprovechar la información de título confiable en el procesamiento de documentos. Este es exactamente el problema que abordamos en este artículo. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, debemos extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Solo consideramos la extracción del título principal. Como líneas base para la extracción de títulos, utilizamos la de siempre usar las primeras líneas como títulos y la de siempre usar las líneas con tamaños de fuente más grandes como títulos. Figura 2. Extracción de título de documento de Word. Figura 3. Extracción de títulos de un documento de PowerPoint. A continuación, definimos una especificación para los juicios humanos en la anotación de datos de títulos. Los datos anotados se utilizarán en el entrenamiento y prueba de los métodos de extracción de títulos. Resumen de la especificación: El título de un documento debe ser identificado en base al sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en los que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación para tales casos. Las reglas incluyen que un título suele estar en líneas consecutivas en el mismo formato, un documento puede no tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como borrador, 147 whitepaper, etc., si es difícil determinar cuál es el título, seleccionar el que tenga el tamaño de fuente más grande y, si aún es difícil determinar cuál es el título, seleccionar el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos). Las figuras 2 y 3 muestran ejemplos de documentos de Office de los cuales realizamos la extracción de títulos. En la Figura 2, Diferencias en las Implementaciones de la API de Win32 entre los Sistemas Operativos de Windows es el título del documento de Word. En la parte superior de esta página hay una imagen de Microsoft Windows que por lo tanto es ignorada. En la Figura 3, \"Construyendo Ventajas Competitivas a través de una Infraestructura Ágil\" es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotadores humanos. La Figura 4 muestra una captura de pantalla de la herramienta. Figura 4. Herramienta de anotación de títulos. 4. MÉTODO DE EXTRACCIÓN DE TÍTULOS 4.1 El método de extracción de títulos basado en aprendizaje automático consta de entrenamiento y extracción. El mismo paso de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el preprocesamiento, se extraen una serie de unidades para el procesamiento de la región superior de la primera página de un documento de Word o de la primera diapositiva de un documento de PowerPoint. Si una línea (las líneas están separadas por símbolos de retorno) solo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no solo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Figura 5. Ejemplo de unidades. En el aprendizaje, la entrada son secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end u otras) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin, title_end u otra. Empleamos cuatro tipos de modelos: Perceptrón, Entropía Máxima (ME), Modelo de Perceptrón de Markov (PMM) y Modelo de Entropía Máxima de Markov (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es título_inicio, título_fin u otro. Luego extraemos las unidades desde la unidad etiquetada con title_begin hasta la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que principalmente utilizamos información de formato para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar esos patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el cual solo se utilizan <br>características lingüísticas</br> para la extracción de artículos de investigación. 4.2 Modelos De hecho, los cuatro modelos pueden considerarse dentro del mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias kxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki ,,2,1 L=). Recuerda que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end, u otro. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )|( 11 kk XXYYP LL donde iX e iY denotan variables aleatorias que toman instancias ix e etiquetas iy como valores, respectivamente ( ki ,,2,1 L= ). Herramienta de extracción de herramientas de aprendizaje 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general para simplificarlo lo suficiente para el entrenamiento. Por ejemplo, podemos asumir que kYY ,,1 L son independientes entre sí dado kXX ,,1 L. De esta manera, descomponemos el modelo en varios clasificadores. Entrenamos los clasificadores localmente utilizando los datos etiquetados. Como clasificador, empleamos el modelo Perceptrón o de Máxima Entropía. También podemos asumir que la propiedad de Markov de primer orden se cumple para kYY ,,1 L dado kXX ,,1 L. Por lo tanto, obtenemos una serie de clasificadores. Sin embargo, los clasificadores están condicionados por la etiqueta previa. Cuando empleamos el modelo Perceptrón o de Entropía Máxima como clasificador, los modelos se convierten en un Modelo Markov Perceptrón o un Modelo Markov de Entropía Máxima, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptrón y ME, asignamos etiquetas localmente y luego combinamos los resultados globalmente utilizando heurísticas. Específicamente, primero identificamos el título más probable. Entonces encontramos el título más probable dentro de tres unidades después del título inicial. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo de Viterbi para encontrar la secuencia de etiquetas globalmente óptima. En este artículo, para el Perceptrón, en realidad empleamos una variante mejorada de él, llamada Perceptrón con Margen Desigual [13]. Esta versión de Perceptrón puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren considerablemente, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del Modelo Perceptrón Markov en la cual el modelo Perceptrón es el llamado Perceptrón Votado [2]. Además, en el entrenamiento, los parámetros del modelo se actualizan de forma global en lugar de localmente. 4.3 Características Hay dos tipos de características: características de formato y <br>características lingüísticas</br>. Principalmente usamos el primero. Las características se utilizan tanto para los clasificadores de inicio de título como para los de fin de título. 4.3.1 Características de formato Tamaño de fuente: Hay cuatro características binarias que representan el tamaño de fuente normalizado de la unidad (recordemos que una unidad tiene solo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario, 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario, 0. Si el tamaño de la fuente es mayor que el tamaño de fuente promedio y no es el más grande en el documento, entonces la segunda característica será 1, de lo contrario, 0. Si el tamaño de la fuente es inferior al tamaño promedio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario, 0. Es necesario realizar la normalización de los tamaños de fuente. Por ejemplo, en un documento el tamaño de fuente más grande podría ser de 12pt, mientras que en otro el más pequeño podría ser de 18pt. Negrita: Esta característica binaria representa si la unidad actual está en negrita o no. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. El siguiente formato de características con respecto al contexto juega un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la siguiente unidad difieren del de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la siguiente difieren de la de la actual. Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad siguiente están en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las <br>características lingüísticas</br> se basan en palabras clave. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, asunto:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, es probable que sean líneas de título. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que positivas. Las <br>características lingüísticas</br> mencionadas anteriormente dependen del idioma. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "comparison between model": {
            "translated_key": "comparación entre modelos",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "generality of model": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) <br>generality of model</br>: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) <br>generality of model</br>: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search."
            ],
            "translated_annotated_samples": [
                "En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, ¿cuál modelo funciona mejor para la extracción de títulos?; (2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda."
            ],
            "translated_text": "Extracción automática de títulos de documentos generales utilizando aprendizaje automático. En este documento, proponemos un enfoque de aprendizaje automático para la extracción de títulos de documentos generales. Por documentos generales nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos, incluyendo presentaciones, capítulos de libros, artículos técnicos, folletos, informes y cartas. Anteriormente, se han propuesto métodos principalmente para la extracción de títulos de artículos de investigación. No ha quedado claro si sería posible realizar la extracción automática de títulos de documentos generales. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que principalmente utilizamos información de formato como el tamaño de fuente como características en los modelos. Resulta que el uso de información de formato puede llevar a una extracción bastante precisa de documentos generales. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente en un experimento con datos de intranet. Otros hallazgos importantes en este trabajo incluyen que podemos entrenar modelos en un dominio y aplicarlos a otro dominio, y más sorprendentemente, incluso podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente los resultados de clasificación de búsqueda en la recuperación de documentos utilizando los títulos extraídos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de Búsqueda; H.4.1 [Aplicaciones de Sistemas de Información]: Automatización de Oficinas - Procesamiento de Texto; D.2.8 [Ingeniería de Software]: Métricas - medidas de complejidad, medidas de rendimiento Términos Generales Algoritmos, Experimentación, Rendimiento. 1. METADATA La información de metadatos de los documentos es útil para muchos tipos de procesamiento de documentos, como la búsqueda, la navegación y el filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen los metadatos de los documentos por sí mismas, incluso cuando tienen herramientas convenientes para la definición de metadatos [26]. Por lo tanto, la extracción automática de metadatos de los cuerpos de los documentos resulta ser un tema de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, el enfoque estaba principalmente en la extracción de los documentos de investigación. Por ejemplo, Han et al. [10] propusieron un método basado en aprendizaje automático para realizar extracciones de artículos de investigación. Formalizaron el problema como el de clasificación y emplearon Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron características lingüísticas en el modelo. En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigación sobre la extracción de información de ellos. Los artículos de investigación suelen tener estilos bien formados y características notables. Por el contrario, los estilos de los documentos generales pueden variar considerablemente. No se ha aclarado si un enfoque basado en aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como estudio de caso, consideramos la extracción de títulos en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizamos la extracción de títulos utilizando uno de los tipos de modelos entrenados. En los modelos, principalmente utilizamos información de formato como el tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con Márgenes Desiguales, Modelo de Entropía Máxima de Markov y Perceptrón Votado. En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, ¿cuál modelo funciona mejor para la extracción de títulos?; (2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a los baselines: uno que siempre utiliza las primeras líneas como títulos y el otro que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente. Resulta que el uso de características de formato es la clave para la exitosa extracción de títulos. (1) Hemos observado que los modelos basados en Perceptrón tienen un mejor rendimiento en términos de precisión de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados en otro, y pueden ser entrenados en un idioma y aplicados en otro. (3) Hemos descubierto que al utilizar los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que podemos realizar de manera fiable la extracción de títulos de documentos generales y utilizar los resultados extraídos para mejorar aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, presentamos el trabajo relacionado, y en la sección 3, explicamos la motivación y el contexto del problema de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando los títulos extraídos. La sección 6 presenta nuestros resultados experimentales. Hacemos observaciones finales en la sección 7.2. TRABAJO RELACIONADO 2.1 Se han propuesto métodos de extracción de metadatos de documentos para realizar extracción automática de metadatos de documentos; sin embargo, el enfoque principal ha sido la extracción de artículos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en reglas y el enfoque basado en aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrollaron un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como que los títulos suelen ubicarse en las partes superiores de las primeras páginas y suelen estar en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel et al. [23] realizaron extracción de metadatos de materiales educativos utilizando tecnologías de procesamiento del lenguaje natural basadas en reglas. Mao et al. [16] también llevaron a cabo la extracción automática de metadatos de artículos de investigación utilizando reglas sobre la información de formato. El enfoque basado en reglas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, realizaron extracción de metadatos con enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron información lingüística como características. Informaron de una alta precisión de extracción de información de artículos de investigación en términos de precisión y recuperación. 2.2 Extracción de Información La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la cual, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Los Modelos Ocultos de Markov [6], el Modelo de Entropía Máxima [1, 4], el Modelo de Entropía Máxima de Markov [17], las Máquinas de Vectores de Soporte [3], el Campo Aleatorio Condicional [12] y el Perceptrón Votado [2] son modelos ampliamente utilizados para la extracción de información. La extracción de información se ha aplicado, por ejemplo, al etiquetado de partes del discurso [20], al reconocimiento de entidades nombradas [25] y a la extracción de tablas [19]. 2.3 Búsqueda utilizando la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. lograron extraer títulos de artículos de investigación y utilizar los títulos extraídos en la búsqueda de metadatos de los artículos [8]. En la búsqueda web, los campos de título (es decir, propiedades de archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al. encontraron que las páginas web con metadatos bien definidos son más fáciles de recuperar que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de Office) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y DEFINICIÓN DEL PROBLEMA Consideramos el problema de extraer automáticamente títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de varios géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, artículos técnicos, folletos, informes, memorandos, especificaciones, cartas, anuncios o currículums. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigar la extracción de títulos de los mismos. La Figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e internet [15]. Office y PDF son los principales formatos de archivo en la intranet. Incluso en internet, los documentos en los formatos siguen siendo significativos, dada su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de Office. Figura 1. Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir títulos como propiedades de archivo utilizando una función proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la función y, por lo tanto, los títulos en las propiedades de los archivos suelen ser muy inexactos. Es decir, los títulos en las propiedades del archivo suelen ser inconsistentes con los verdaderos títulos en los cuerpos de los archivos que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 documentos de Word y 6,000 documentos de PowerPoint de una intranet y de internet, y examinamos cuántos títulos en las propiedades de los archivos son correctos. Descubrimos que sorprendentemente la precisión fue solo de 0.265 (cf., Sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, entonces la propiedad de archivo del nuevo archivo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en las propiedades de archivo de documentos de Office en la búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devueltos por Google. Descubrimos que casi todos los títulos presentados en los resultados de búsqueda eran de las propiedades de archivo de los documentos. Sin embargo, solo 0.272 de ellos fueron correctos. De hecho, los títulos verdaderos suelen encontrarse al principio de los cuerpos de los documentos. Si podemos extraer con precisión los títulos de los cuerpos de los documentos, entonces podemos aprovechar la información de título confiable en el procesamiento de documentos. Este es exactamente el problema que abordamos en este artículo. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, debemos extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Solo consideramos la extracción del título principal. Como líneas base para la extracción de títulos, utilizamos la de siempre usar las primeras líneas como títulos y la de siempre usar las líneas con tamaños de fuente más grandes como títulos. Figura 2. Extracción de título de documento de Word. Figura 3. Extracción de títulos de un documento de PowerPoint. A continuación, definimos una especificación para los juicios humanos en la anotación de datos de títulos. Los datos anotados se utilizarán en el entrenamiento y prueba de los métodos de extracción de títulos. Resumen de la especificación: El título de un documento debe ser identificado en base al sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en los que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación para tales casos. Las reglas incluyen que un título suele estar en líneas consecutivas en el mismo formato, un documento puede no tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como borrador, 147 whitepaper, etc., si es difícil determinar cuál es el título, seleccionar el que tenga el tamaño de fuente más grande y, si aún es difícil determinar cuál es el título, seleccionar el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos). Las figuras 2 y 3 muestran ejemplos de documentos de Office de los cuales realizamos la extracción de títulos. En la Figura 2, Diferencias en las Implementaciones de la API de Win32 entre los Sistemas Operativos de Windows es el título del documento de Word. En la parte superior de esta página hay una imagen de Microsoft Windows que por lo tanto es ignorada. En la Figura 3, \"Construyendo Ventajas Competitivas a través de una Infraestructura Ágil\" es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotadores humanos. La Figura 4 muestra una captura de pantalla de la herramienta. Figura 4. Herramienta de anotación de títulos. 4. MÉTODO DE EXTRACCIÓN DE TÍTULOS 4.1 El método de extracción de títulos basado en aprendizaje automático consta de entrenamiento y extracción. El mismo paso de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el preprocesamiento, se extraen una serie de unidades para el procesamiento de la región superior de la primera página de un documento de Word o de la primera diapositiva de un documento de PowerPoint. Si una línea (las líneas están separadas por símbolos de retorno) solo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no solo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Figura 5. Ejemplo de unidades. En el aprendizaje, la entrada son secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end u otras) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin, title_end u otra. Empleamos cuatro tipos de modelos: Perceptrón, Entropía Máxima (ME), Modelo de Perceptrón de Markov (PMM) y Modelo de Entropía Máxima de Markov (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es título_inicio, título_fin u otro. Luego extraemos las unidades desde la unidad etiquetada con title_begin hasta la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que principalmente utilizamos información de formato para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar esos patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el cual solo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos De hecho, los cuatro modelos pueden considerarse dentro del mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias kxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki ,,2,1 L=). Recuerda que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end, u otro. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )|( 11 kk XXYYP LL donde iX e iY denotan variables aleatorias que toman instancias ix e etiquetas iy como valores, respectivamente ( ki ,,2,1 L= ). Herramienta de extracción de herramientas de aprendizaje 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general para simplificarlo lo suficiente para el entrenamiento. Por ejemplo, podemos asumir que kYY ,,1 L son independientes entre sí dado kXX ,,1 L. De esta manera, descomponemos el modelo en varios clasificadores. Entrenamos los clasificadores localmente utilizando los datos etiquetados. Como clasificador, empleamos el modelo Perceptrón o de Máxima Entropía. También podemos asumir que la propiedad de Markov de primer orden se cumple para kYY ,,1 L dado kXX ,,1 L. Por lo tanto, obtenemos una serie de clasificadores. Sin embargo, los clasificadores están condicionados por la etiqueta previa. Cuando empleamos el modelo Perceptrón o de Entropía Máxima como clasificador, los modelos se convierten en un Modelo Markov Perceptrón o un Modelo Markov de Entropía Máxima, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptrón y ME, asignamos etiquetas localmente y luego combinamos los resultados globalmente utilizando heurísticas. Específicamente, primero identificamos el título más probable. Entonces encontramos el título más probable dentro de tres unidades después del título inicial. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo de Viterbi para encontrar la secuencia de etiquetas globalmente óptima. En este artículo, para el Perceptrón, en realidad empleamos una variante mejorada de él, llamada Perceptrón con Margen Desigual [13]. Esta versión de Perceptrón puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren considerablemente, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del Modelo Perceptrón Markov en la cual el modelo Perceptrón es el llamado Perceptrón Votado [2]. Además, en el entrenamiento, los parámetros del modelo se actualizan de forma global en lugar de localmente. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Principalmente usamos el primero. Las características se utilizan tanto para los clasificadores de inicio de título como para los de fin de título. 4.3.1 Características de formato Tamaño de fuente: Hay cuatro características binarias que representan el tamaño de fuente normalizado de la unidad (recordemos que una unidad tiene solo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario, 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario, 0. Si el tamaño de la fuente es mayor que el tamaño de fuente promedio y no es el más grande en el documento, entonces la segunda característica será 1, de lo contrario, 0. Si el tamaño de la fuente es inferior al tamaño promedio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario, 0. Es necesario realizar la normalización de los tamaños de fuente. Por ejemplo, en un documento el tamaño de fuente más grande podría ser de 12pt, mientras que en otro el más pequeño podría ser de 18pt. Negrita: Esta característica binaria representa si la unidad actual está en negrita o no. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. El siguiente formato de características con respecto al contexto juega un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la siguiente unidad difieren del de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la siguiente difieren de la de la actual. Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad siguiente están en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, asunto:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, es probable que sean líneas de título. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Recuento de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, ∞) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario, 0. Carácter de finalización: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título generalmente no termina con un carácter como ese. 5. MÉTODO DE RECUPERACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Normalmente, en la recuperación de información, un documento se divide en varios campos que incluyen cuerpo, título y texto de anclaje. Una función de clasificación en la búsqueda puede utilizar diferentes pesos para diferentes campos del documento. Además, los títulos suelen recibir un peso alto, lo que indica que son importantes para la recuperación de documentos. Como se explicó anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite la ponderación de campos [21]. Como campos, hacemos uso del cuerpo, título, título extraído y ancla. Primero, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; luego, cada frecuencia de campo se pondera según el parámetro de peso correspondiente: ∑= f tfft tfwwtf De manera similar, calculamos la longitud del documento como una suma ponderada de las longitudes de cada campo. La longitud promedio del documento en el corpus se convierte en el promedio de todas las longitudes de documentos ponderadas. ∑= f ff dlwwdl En nuestros experimentos usamos 75.0,8.11 == bk. El peso para el contenido fue de 1.0, para el título fue de 10.0, para el enlace fue de 10.0 y para el título extraído fue de 5.0. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación. Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos aleatoriamente 5,000 documentos de Word y 5,000 documentos de PowerPoint de una intranet de Microsoft. Lo llamamos MS de ahora en adelante. Segundo, descargamos y seleccionamos aleatoriamente 500 documentos de Word y 500 documentos de PowerPoint de los dominios DotGov y DotCom en internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, de hecho, documentos generales tal como los definimos. Figura 7. Distribuciones de géneros de documentos. Tercero, también se descargó un conjunto de datos en chino de internet. Incluye 500 documentos de Word y 500 documentos de PowerPoint en chino. Etiquetamos manualmente los títulos de todos los documentos, basándonos en nuestra especificación. No todos los documentos en los dos conjuntos de datos tienen títulos. La Tabla 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría deberse a que los documentos de PowerPoint publicados en internet son más formales que los de la intranet. Tabla 1. En nuestros experimentos, realizamos evaluaciones sobre la extracción de títulos en términos de precisión, recuperación y medida F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / (A + B) Recall: R = A / (A + C) F-measure: F1 = 2PR / (P + R) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Tabla 2. Tabla de contingencia con respecto a la extracción de títulos. ¿Es título? ¿No es título? Extraído A B No extraído C D 6.2 Baselines Probamos las precisiones de los dos baselines descritos en la sección 4.2. Se denominan como el tamaño de fuente más grande y la primera línea respectivamente. 6.3 Precisión de los títulos en las propiedades del archivo Investigamos cuántos títulos en las propiedades del archivo de los documentos son confiables. Consideramos los títulos anotados por humanos como títulos verdaderos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos la Distancia de Edición para realizar la coincidencia aproximada. (La coincidencia aproximada solo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados por humanos pueden ser ligeramente diferentes de los títulos en las propiedades del archivo en la superficie, por ejemplo, contener espacios adicionales. Dadas las cadenas A y B: si ((D == 0) o (D / (La + Lb) < θ)) entonces la cadena A = cadena B D: Distancia de edición entre la cadena A y la cadena B La: longitud de la cadena A Lb: longitud de la cadena B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Tabla 3. Precisión de los títulos en las propiedades del archivo Tipo de archivo Dominio Precisión Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186 PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparación con Baselines Realizamos la extracción de títulos del primer conjunto de datos (Word y PowerPoint en MS). Como modelo, utilizamos el Perceptrón. Realizamos validación cruzada de 4 pliegues. Por lo tanto, todos los resultados reportados aquí son aquellos promediados en 4 ensayos. Las tablas 4 y 5 muestran los resultados. Vemos que el Perceptrón supera significativamente a los baselines. En la evaluación, utilizamos coincidencia exacta entre los títulos reales anotados por humanos y los títulos extraídos. Tabla 4. Precisión de la extracción de títulos con el Modelo Perceptrón de Palabra Precisión Recall F1 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Líneas base Primera línea 0.707 0.767 0.736 Tabla 5. Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como la exhaustividad del enfoque son un 8 por ciento más altas que las de las líneas de base. Para PowerPoint, tanto la precisión como la exhaustividad del enfoque son un 2 por ciento más altas que las de las líneas de base. Realizamos pruebas de significancia. Los resultados se muestran en la Tabla 6. Aquí, \"Largest\" denota la línea base de usar el tamaño de fuente más grande, \"First\" denota la línea base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático sobre las líneas de base son estadísticamente significativas (en el sentido de que el valor p < 0.05) Tabla 6. Vemos, a partir de los resultados, que los dos baselines pueden funcionar bien para la extracción de títulos, lo que sugiere que el tamaño de fuente y la información de posición son las características más útiles para la extracción de títulos. Sin embargo, también es evidente que utilizar solo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande solo contienen descripciones generales como Confidencial, Documento blanco, etc. Para esos casos, el método del tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio del título), el Perceptrón puede superar a Largest y First. Investigamos el rendimiento de utilizar únicamente características lingüísticas. Descubrimos que no funciona bien. Parece que las características del formato desempeñan roles importantes y las características lingüísticas son complementos. Figura 8. Un documento de Word de ejemplo. Figura 9. Un documento de PowerPoint de ejemplo. Realizamos un análisis de errores en los resultados del Perceptrón. Encontramos que los errores caían en tres categorías. (1) Aproximadamente un tercio de los errores estaban relacionados con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores provienen de los documentos que no tienen títulos verdaderos, sino que solo contienen viñetas. Dado que realizamos la extracción desde las regiones superiores, es difícil deshacernos de estos errores con el enfoque actual. Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que solo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error no afecta mucho al procesamiento de documentos como la búsqueda. 6.5 Comparación entre Modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Nuevamente, realizamos una validación cruzada de 4 pliegues en el primer conjunto de datos (MS). La tabla 7 y 8 muestran los resultados de los cuatro modelos. Resulta que Perceptrón y PMM tienen el mejor rendimiento, seguidos por MEMM, y ME tiene el peor rendimiento. En general, los modelos markovianos tienen un mejor rendimiento que sus contrapartes clasificadoras o igual de bueno. Esto parece ser porque los modelos markovianos se entrenan de forma global, mientras que los clasificadores se entrenan de forma local. Los modelos basados en el Perceptrón tienen un mejor rendimiento que sus contrapartes basadas en el ME. Esto parece ser porque los modelos basados en Perceptrón están creados para realizar mejores clasificaciones, mientras que los modelos de ME se construyen para una mejor predicción. Tabla 7. Comparación entre diferentes modelos de aprendizaje para la extracción de títulos con Modelo de Palabra Precisión Recall F1 Perceptrón 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de títulos con el Modelo de PowerPoint Precisión Recall F1 Perceptrón 0.875 0.895 0.885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). Las tablas 9-12 muestran los resultados. Tabla 9. Precisión de la extracción de títulos con Word en DotGov Precisión Recall F1 Modelo Perceptrón 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582 Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Precisión de la extracción de títulos con PowerPoint en DotGov Precision Recall F1 Modelo Perceptrón 0.900 0.906 0.903 Tamaño de fuente más grande 0.871 0.888 0.879 Baselines Primera línea 0.554 0.564 0.559 Tabla 11. Precisión de la extracción de títulos con Word en DotCom Precisión Recall F1 Modelo Perceptrón 0.832 0.880 0.855 Tamaño de fuente más grande 0.676 0.753 0.712 Baselines Primera línea 0.577 0.643 0.608 Tabla 12. Rendimiento de la extracción de títulos de documentos de PowerPoint en el modelo de Precisión, Recall y F1 de Perceptrón DotCom 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875 Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos pueden adaptarse bien a diferentes dominios. Casi no hay disminución en la precisión. Los resultados indican que los patrones de formatos de títulos existen en diferentes dominios, y es posible construir un modelo independiente del dominio utilizando principalmente información de formato. 6.7 Adaptación de idioma Aplicamos el modelo entrenado con los datos en inglés (MS) al conjunto de datos en chino. Las tablas 13-14 muestran los resultados. Tabla 13. Precisión de la extracción de títulos con Word en chino: Precisión Recall F1 Modelo Perceptrón 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738 Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Vemos que los modelos se pueden adaptar a un idioma diferente. Solo hay pequeñas caídas en la precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no usarlas es insignificante. Los resultados indican que los patrones de formatos de títulos existen en diferentes idiomas. A partir de los resultados de adaptación de dominio y adaptación de lenguaje, concluimos que el uso de información de formato es clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con Títulos Extraídos Realizamos experimentos utilizando la extracción de títulos para la recuperación de documentos. Como línea base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación fue como se describe en la Sección 5. Los pesos fueron establecidos heurísticamente. No realizamos la optimización de los pesos. La evaluación se llevó a cabo en un corpus de 1.3 millones de documentos extraídos de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de los registros de consultas del motor de búsqueda de esta intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 consultas fueron elegidas al azar. A los usuarios se les pidió que proporcionaran juicios sobre el grado de relevancia del documento en una escala del 1 al 5 (1 significando perjudicial, 2 - malo, 3 - regular, 4 - bueno y 5 - excelente). La figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión al considerar documentos buenos o excelentes como relevantes (tres barras izquierdas con umbral de relevancia 0.5), o al considerar solo documentos excelentes como relevantes (tres barras derechas con umbral de relevancia 1.0). Resultados de clasificación de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo los campos cuerpo, título (propiedad de archivo) y texto de anclaje. • Barra morada - BM25 incluyendo los campos cuerpo, título (propiedad de archivo), texto de anclaje y título extraído. Con el campo adicional de título extraído incluido en BM25, la precisión @10 aumentó de 0.132 a 0.145, o aproximadamente un 10%. Por lo tanto, se puede afirmar que el uso del título extraído puede mejorar efectivamente la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este documento, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. Trabajos anteriores mostraron que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de artículos de investigación. En este artículo, demostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas base en la extracción de títulos de documentos de Office. Trabajos anteriores sobre extracción de metadatos principalmente utilizaron características lingüísticas en los documentos, mientras que nosotros principalmente utilizamos información de formato. Parecía que el uso de información de formato es clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Probamos diferentes modelos de aprendizaje automático incluyendo Perceptrón, Entropía Máxima, Modelo de Markov de Entropía Máxima y Perceptrón Votado. Descubrimos que el rendimiento de los modelos Perceptrón fue el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos entrenados en un idioma a otro idioma. Encontramos que las precisiones no disminuyeron sustancialmente en diferentes dominios y en diferentes idiomas, lo que indica que los modelos eran genéricos. También intentamos utilizar los títulos extraídos en la recuperación de documentos. Observamos una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda al utilizar la información del título extraída. Todas las investigaciones anteriores no se llevaron a cabo, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción de títulos. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu y a los revisores anónimos de JCDL05 por sus valiosos comentarios sobre este artículo. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de entropía máxima para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminativo para modelos ocultos de Markov: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en Procesamiento de Lenguaje Natural, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de vectores de soporte. Aprendizaje automático, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información de texto semiestructurado y libre. En Actas de la Decimoctava Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia newsblaster: resumen multilingüe de noticias en la web. En Actas de la conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos ocultos de Markov factorial. Aprendizaje automático, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de Visualización de Información de 1999, 446-451, 1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., y Pal, N. eBizSearch: un motor de búsqueda de nicho para e-Business. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 413-414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. Extracción de metadatos basada en conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de Bibliotecas Digitales de la ACM, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos utilizando máquinas de vectores de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y etiquetar datos de secuencias. En Actas de la Decimoctava Conferencia Internacional sobre Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo del perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial en nuevos formatos de contenido. En Actas de la Séptima Conferencia de Motores de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., y Thoma, G. R. Un sistema de generación de características dinámicas para la extracción automatizada de metadatos en la preservación de materiales digitales. En Actas del Primer Taller Internacional sobre Análisis de Imágenes de Documentos para Bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de máxima entropía de Markov para extracción de información y segmentación. En Actas de la Decimoséptima Conferencia Internacional sobre Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésimo-Primera Conferencia Internacional Anual de Ciencias de Sistemas de Hawái, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas utilizando campos aleatorios condicionales. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para la unión de frases preposicionales. En Actas de la Decimoséptima Conferencia Internacional de Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Extensión simple de BM25 a múltiples campos ponderados, En Actas de la Decimotercera Conferencia de ACM sobre Información y Gestión del Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Minería web basada en metadatos para relevancia, En Actas del Simposio Internacional de Ingeniería y Aplicaciones de Bases de Datos 2000, 113-121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema de procesamiento de lenguaje natural para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE sobre Bibliotecas Digitales de 2004, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda de Internet a la implementación de metadatos Dublin Core. Revista de Ciencia de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y., y Zhang, T. Reconocimiento y uso de entidades nombradas: reconocimiento de entidades nombradas enfocado utilizando aprendizaje automático. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "model generality": {
            "translated_key": "generalidad del modelo",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "usefulness of extracted title": {
            "translated_key": "utilidad del título extraído",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "extracted title usefulness": {
            "translated_key": "utilidad del título extraído",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "genre": {
            "translated_key": "géneros",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific <br>genre</br>s, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific <br>genre</br>s.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific <br>genre</br>s.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the <br>genre</br>s of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document <br>genre</br>s.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [
                "By general documents, we mean documents that can belong to any one of a number of specific <br>genre</br>s, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "By general documents, we mean documents that may belong to any one of a number of specific <br>genre</br>s.",
                "By general documents, we mean documents that belong to one of any number of specific <br>genre</br>s.",
                "Figure 7 shows the distributions of the <br>genre</br>s of the documents.",
                "Distributions of document <br>genre</br>s."
            ],
            "translated_annotated_samples": [
                "Por documentos generales nos referimos a documentos que pueden pertenecer a cualquiera de varios <br>géneros</br> específicos, incluyendo presentaciones, capítulos de libros, artículos técnicos, folletos, informes y cartas.",
                "Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de varios <br>géneros</br> específicos.",
                "Por documentos generales, nos referimos a documentos que pertenecen a uno de varios <br>géneros</br> específicos.",
                "La Figura 7 muestra las distribuciones de los <br>géneros</br> de los documentos.",
                "Distribuciones de <br>géneros</br> de documentos."
            ],
            "translated_text": "Extracción automática de títulos de documentos generales utilizando aprendizaje automático. En este documento, proponemos un enfoque de aprendizaje automático para la extracción de títulos de documentos generales. Por documentos generales nos referimos a documentos que pueden pertenecer a cualquiera de varios <br>géneros</br> específicos, incluyendo presentaciones, capítulos de libros, artículos técnicos, folletos, informes y cartas. Anteriormente, se han propuesto métodos principalmente para la extracción de títulos de artículos de investigación. No ha quedado claro si sería posible realizar la extracción automática de títulos de documentos generales. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que principalmente utilizamos información de formato como el tamaño de fuente como características en los modelos. Resulta que el uso de información de formato puede llevar a una extracción bastante precisa de documentos generales. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente en un experimento con datos de intranet. Otros hallazgos importantes en este trabajo incluyen que podemos entrenar modelos en un dominio y aplicarlos a otro dominio, y más sorprendentemente, incluso podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente los resultados de clasificación de búsqueda en la recuperación de documentos utilizando los títulos extraídos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de Búsqueda; H.4.1 [Aplicaciones de Sistemas de Información]: Automatización de Oficinas - Procesamiento de Texto; D.2.8 [Ingeniería de Software]: Métricas - medidas de complejidad, medidas de rendimiento Términos Generales Algoritmos, Experimentación, Rendimiento. 1. METADATA La información de metadatos de los documentos es útil para muchos tipos de procesamiento de documentos, como la búsqueda, la navegación y el filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen los metadatos de los documentos por sí mismas, incluso cuando tienen herramientas convenientes para la definición de metadatos [26]. Por lo tanto, la extracción automática de metadatos de los cuerpos de los documentos resulta ser un tema de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, el enfoque estaba principalmente en la extracción de los documentos de investigación. Por ejemplo, Han et al. [10] propusieron un método basado en aprendizaje automático para realizar extracciones de artículos de investigación. Formalizaron el problema como el de clasificación y emplearon Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron características lingüísticas en el modelo. En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de varios <br>géneros</br> específicos. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigación sobre la extracción de información de ellos. Los artículos de investigación suelen tener estilos bien formados y características notables. Por el contrario, los estilos de los documentos generales pueden variar considerablemente. No se ha aclarado si un enfoque basado en aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como estudio de caso, consideramos la extracción de títulos en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizamos la extracción de títulos utilizando uno de los tipos de modelos entrenados. En los modelos, principalmente utilizamos información de formato como el tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con Márgenes Desiguales, Modelo de Entropía Máxima de Markov y Perceptrón Votado. En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, ¿cuál modelo funciona mejor para la extracción de títulos?; (2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a los baselines: uno que siempre utiliza las primeras líneas como títulos y el otro que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente. Resulta que el uso de características de formato es la clave para la exitosa extracción de títulos. (1) Hemos observado que los modelos basados en Perceptrón tienen un mejor rendimiento en términos de precisión de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados en otro, y pueden ser entrenados en un idioma y aplicados en otro. (3) Hemos descubierto que al utilizar los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que podemos realizar de manera fiable la extracción de títulos de documentos generales y utilizar los resultados extraídos para mejorar aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, presentamos el trabajo relacionado, y en la sección 3, explicamos la motivación y el contexto del problema de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando los títulos extraídos. La sección 6 presenta nuestros resultados experimentales. Hacemos observaciones finales en la sección 7.2. TRABAJO RELACIONADO 2.1 Se han propuesto métodos de extracción de metadatos de documentos para realizar extracción automática de metadatos de documentos; sin embargo, el enfoque principal ha sido la extracción de artículos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en reglas y el enfoque basado en aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrollaron un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como que los títulos suelen ubicarse en las partes superiores de las primeras páginas y suelen estar en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel et al. [23] realizaron extracción de metadatos de materiales educativos utilizando tecnologías de procesamiento del lenguaje natural basadas en reglas. Mao et al. [16] también llevaron a cabo la extracción automática de metadatos de artículos de investigación utilizando reglas sobre la información de formato. El enfoque basado en reglas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, realizaron extracción de metadatos con enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron información lingüística como características. Informaron de una alta precisión de extracción de información de artículos de investigación en términos de precisión y recuperación. 2.2 Extracción de Información La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la cual, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Los Modelos Ocultos de Markov [6], el Modelo de Entropía Máxima [1, 4], el Modelo de Entropía Máxima de Markov [17], las Máquinas de Vectores de Soporte [3], el Campo Aleatorio Condicional [12] y el Perceptrón Votado [2] son modelos ampliamente utilizados para la extracción de información. La extracción de información se ha aplicado, por ejemplo, al etiquetado de partes del discurso [20], al reconocimiento de entidades nombradas [25] y a la extracción de tablas [19]. 2.3 Búsqueda utilizando la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. lograron extraer títulos de artículos de investigación y utilizar los títulos extraídos en la búsqueda de metadatos de los artículos [8]. En la búsqueda web, los campos de título (es decir, propiedades de archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al. encontraron que las páginas web con metadatos bien definidos son más fáciles de recuperar que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de Office) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y DEFINICIÓN DEL PROBLEMA Consideramos el problema de extraer automáticamente títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de varios <br>géneros</br> específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, artículos técnicos, folletos, informes, memorandos, especificaciones, cartas, anuncios o currículums. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigar la extracción de títulos de los mismos. La Figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e internet [15]. Office y PDF son los principales formatos de archivo en la intranet. Incluso en internet, los documentos en los formatos siguen siendo significativos, dada su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de Office. Figura 1. Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir títulos como propiedades de archivo utilizando una función proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la función y, por lo tanto, los títulos en las propiedades de los archivos suelen ser muy inexactos. Es decir, los títulos en las propiedades del archivo suelen ser inconsistentes con los verdaderos títulos en los cuerpos de los archivos que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 documentos de Word y 6,000 documentos de PowerPoint de una intranet y de internet, y examinamos cuántos títulos en las propiedades de los archivos son correctos. Descubrimos que sorprendentemente la precisión fue solo de 0.265 (cf., Sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, entonces la propiedad de archivo del nuevo archivo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en las propiedades de archivo de documentos de Office en la búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devueltos por Google. Descubrimos que casi todos los títulos presentados en los resultados de búsqueda eran de las propiedades de archivo de los documentos. Sin embargo, solo 0.272 de ellos fueron correctos. De hecho, los títulos verdaderos suelen encontrarse al principio de los cuerpos de los documentos. Si podemos extraer con precisión los títulos de los cuerpos de los documentos, entonces podemos aprovechar la información de título confiable en el procesamiento de documentos. Este es exactamente el problema que abordamos en este artículo. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, debemos extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Solo consideramos la extracción del título principal. Como líneas base para la extracción de títulos, utilizamos la de siempre usar las primeras líneas como títulos y la de siempre usar las líneas con tamaños de fuente más grandes como títulos. Figura 2. Extracción de título de documento de Word. Figura 3. Extracción de títulos de un documento de PowerPoint. A continuación, definimos una especificación para los juicios humanos en la anotación de datos de títulos. Los datos anotados se utilizarán en el entrenamiento y prueba de los métodos de extracción de títulos. Resumen de la especificación: El título de un documento debe ser identificado en base al sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en los que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación para tales casos. Las reglas incluyen que un título suele estar en líneas consecutivas en el mismo formato, un documento puede no tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como borrador, 147 whitepaper, etc., si es difícil determinar cuál es el título, seleccionar el que tenga el tamaño de fuente más grande y, si aún es difícil determinar cuál es el título, seleccionar el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos). Las figuras 2 y 3 muestran ejemplos de documentos de Office de los cuales realizamos la extracción de títulos. En la Figura 2, Diferencias en las Implementaciones de la API de Win32 entre los Sistemas Operativos de Windows es el título del documento de Word. En la parte superior de esta página hay una imagen de Microsoft Windows que por lo tanto es ignorada. En la Figura 3, \"Construyendo Ventajas Competitivas a través de una Infraestructura Ágil\" es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotadores humanos. La Figura 4 muestra una captura de pantalla de la herramienta. Figura 4. Herramienta de anotación de títulos. 4. MÉTODO DE EXTRACCIÓN DE TÍTULOS 4.1 El método de extracción de títulos basado en aprendizaje automático consta de entrenamiento y extracción. El mismo paso de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el preprocesamiento, se extraen una serie de unidades para el procesamiento de la región superior de la primera página de un documento de Word o de la primera diapositiva de un documento de PowerPoint. Si una línea (las líneas están separadas por símbolos de retorno) solo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no solo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Figura 5. Ejemplo de unidades. En el aprendizaje, la entrada son secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end u otras) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin, title_end u otra. Empleamos cuatro tipos de modelos: Perceptrón, Entropía Máxima (ME), Modelo de Perceptrón de Markov (PMM) y Modelo de Entropía Máxima de Markov (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es título_inicio, título_fin u otro. Luego extraemos las unidades desde la unidad etiquetada con title_begin hasta la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que principalmente utilizamos información de formato para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar esos patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el cual solo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos De hecho, los cuatro modelos pueden considerarse dentro del mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias kxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki ,,2,1 L=). Recuerda que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end, u otro. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )|( 11 kk XXYYP LL donde iX e iY denotan variables aleatorias que toman instancias ix e etiquetas iy como valores, respectivamente ( ki ,,2,1 L= ). Herramienta de extracción de herramientas de aprendizaje 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general para simplificarlo lo suficiente para el entrenamiento. Por ejemplo, podemos asumir que kYY ,,1 L son independientes entre sí dado kXX ,,1 L. De esta manera, descomponemos el modelo en varios clasificadores. Entrenamos los clasificadores localmente utilizando los datos etiquetados. Como clasificador, empleamos el modelo Perceptrón o de Máxima Entropía. También podemos asumir que la propiedad de Markov de primer orden se cumple para kYY ,,1 L dado kXX ,,1 L. Por lo tanto, obtenemos una serie de clasificadores. Sin embargo, los clasificadores están condicionados por la etiqueta previa. Cuando empleamos el modelo Perceptrón o de Entropía Máxima como clasificador, los modelos se convierten en un Modelo Markov Perceptrón o un Modelo Markov de Entropía Máxima, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptrón y ME, asignamos etiquetas localmente y luego combinamos los resultados globalmente utilizando heurísticas. Específicamente, primero identificamos el título más probable. Entonces encontramos el título más probable dentro de tres unidades después del título inicial. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo de Viterbi para encontrar la secuencia de etiquetas globalmente óptima. En este artículo, para el Perceptrón, en realidad empleamos una variante mejorada de él, llamada Perceptrón con Margen Desigual [13]. Esta versión de Perceptrón puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren considerablemente, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del Modelo Perceptrón Markov en la cual el modelo Perceptrón es el llamado Perceptrón Votado [2]. Además, en el entrenamiento, los parámetros del modelo se actualizan de forma global en lugar de localmente. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Principalmente usamos el primero. Las características se utilizan tanto para los clasificadores de inicio de título como para los de fin de título. 4.3.1 Características de formato Tamaño de fuente: Hay cuatro características binarias que representan el tamaño de fuente normalizado de la unidad (recordemos que una unidad tiene solo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario, 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario, 0. Si el tamaño de la fuente es mayor que el tamaño de fuente promedio y no es el más grande en el documento, entonces la segunda característica será 1, de lo contrario, 0. Si el tamaño de la fuente es inferior al tamaño promedio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario, 0. Es necesario realizar la normalización de los tamaños de fuente. Por ejemplo, en un documento el tamaño de fuente más grande podría ser de 12pt, mientras que en otro el más pequeño podría ser de 18pt. Negrita: Esta característica binaria representa si la unidad actual está en negrita o no. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. El siguiente formato de características con respecto al contexto juega un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la siguiente unidad difieren del de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la siguiente difieren de la de la actual. Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad siguiente están en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, asunto:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, es probable que sean líneas de título. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Recuento de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, ∞) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario, 0. Carácter de finalización: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título generalmente no termina con un carácter como ese. 5. MÉTODO DE RECUPERACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Normalmente, en la recuperación de información, un documento se divide en varios campos que incluyen cuerpo, título y texto de anclaje. Una función de clasificación en la búsqueda puede utilizar diferentes pesos para diferentes campos del documento. Además, los títulos suelen recibir un peso alto, lo que indica que son importantes para la recuperación de documentos. Como se explicó anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite la ponderación de campos [21]. Como campos, hacemos uso del cuerpo, título, título extraído y ancla. Primero, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; luego, cada frecuencia de campo se pondera según el parámetro de peso correspondiente: ∑= f tfft tfwwtf De manera similar, calculamos la longitud del documento como una suma ponderada de las longitudes de cada campo. La longitud promedio del documento en el corpus se convierte en el promedio de todas las longitudes de documentos ponderadas. ∑= f ff dlwwdl En nuestros experimentos usamos 75.0,8.11 == bk. El peso para el contenido fue de 1.0, para el título fue de 10.0, para el enlace fue de 10.0 y para el título extraído fue de 5.0. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación. Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos aleatoriamente 5,000 documentos de Word y 5,000 documentos de PowerPoint de una intranet de Microsoft. Lo llamamos MS de ahora en adelante. Segundo, descargamos y seleccionamos aleatoriamente 500 documentos de Word y 500 documentos de PowerPoint de los dominios DotGov y DotCom en internet, respectivamente. La Figura 7 muestra las distribuciones de los <br>géneros</br> de los documentos. Vemos que los documentos son, de hecho, documentos generales tal como los definimos. Figura 7. Distribuciones de <br>géneros</br> de documentos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "classifier": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the <br>classifier</br>.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the <br>classifier</br>.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the <br>classifier</br>, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a <br>classifier</br>, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their <br>classifier</br> counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [
                "They formalized the problem as that of classification and employed Support Vector Machines as the <br>classifier</br>.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the <br>classifier</br>.",
                "As the <br>classifier</br>, we employ the Perceptron or Maximum Entropy model.",
                "When we employ the Percepton or Maximum Entropy model as a <br>classifier</br>, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "In general, the Markovian models perform better than or as well as their <br>classifier</br> counterparts."
            ],
            "translated_annotated_samples": [
                "Formalizaron el problema como el de clasificación y emplearon Máquinas de Vectores de Soporte como <br>clasificador</br>.",
                "Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar Máquinas de Vectores de Soporte como <br>clasificador</br>.",
                "Como <br>clasificador</br>, empleamos el modelo Perceptrón o de Máxima Entropía.",
                "Cuando empleamos el modelo Perceptrón o de Entropía Máxima como <br>clasificador</br>, los modelos se convierten en un Modelo Markov Perceptrón o un Modelo Markov de Entropía Máxima, respectivamente.",
                "En general, los modelos markovianos tienen un mejor rendimiento que sus contrapartes <br>clasificadoras</br> o igual de bueno."
            ],
            "translated_text": "Extracción automática de títulos de documentos generales utilizando aprendizaje automático. En este documento, proponemos un enfoque de aprendizaje automático para la extracción de títulos de documentos generales. Por documentos generales nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos, incluyendo presentaciones, capítulos de libros, artículos técnicos, folletos, informes y cartas. Anteriormente, se han propuesto métodos principalmente para la extracción de títulos de artículos de investigación. No ha quedado claro si sería posible realizar la extracción automática de títulos de documentos generales. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que principalmente utilizamos información de formato como el tamaño de fuente como características en los modelos. Resulta que el uso de información de formato puede llevar a una extracción bastante precisa de documentos generales. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente en un experimento con datos de intranet. Otros hallazgos importantes en este trabajo incluyen que podemos entrenar modelos en un dominio y aplicarlos a otro dominio, y más sorprendentemente, incluso podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente los resultados de clasificación de búsqueda en la recuperación de documentos utilizando los títulos extraídos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de Búsqueda; H.4.1 [Aplicaciones de Sistemas de Información]: Automatización de Oficinas - Procesamiento de Texto; D.2.8 [Ingeniería de Software]: Métricas - medidas de complejidad, medidas de rendimiento Términos Generales Algoritmos, Experimentación, Rendimiento. 1. METADATA La información de metadatos de los documentos es útil para muchos tipos de procesamiento de documentos, como la búsqueda, la navegación y el filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen los metadatos de los documentos por sí mismas, incluso cuando tienen herramientas convenientes para la definición de metadatos [26]. Por lo tanto, la extracción automática de metadatos de los cuerpos de los documentos resulta ser un tema de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, el enfoque estaba principalmente en la extracción de los documentos de investigación. Por ejemplo, Han et al. [10] propusieron un método basado en aprendizaje automático para realizar extracciones de artículos de investigación. Formalizaron el problema como el de clasificación y emplearon Máquinas de Vectores de Soporte como <br>clasificador</br>. Principalmente utilizaron características lingüísticas en el modelo. En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigación sobre la extracción de información de ellos. Los artículos de investigación suelen tener estilos bien formados y características notables. Por el contrario, los estilos de los documentos generales pueden variar considerablemente. No se ha aclarado si un enfoque basado en aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como estudio de caso, consideramos la extracción de títulos en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizamos la extracción de títulos utilizando uno de los tipos de modelos entrenados. En los modelos, principalmente utilizamos información de formato como el tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con Márgenes Desiguales, Modelo de Entropía Máxima de Markov y Perceptrón Votado. En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, ¿cuál modelo funciona mejor para la extracción de títulos?; (2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a los baselines: uno que siempre utiliza las primeras líneas como títulos y el otro que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente. Resulta que el uso de características de formato es la clave para la exitosa extracción de títulos. (1) Hemos observado que los modelos basados en Perceptrón tienen un mejor rendimiento en términos de precisión de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados en otro, y pueden ser entrenados en un idioma y aplicados en otro. (3) Hemos descubierto que al utilizar los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que podemos realizar de manera fiable la extracción de títulos de documentos generales y utilizar los resultados extraídos para mejorar aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, presentamos el trabajo relacionado, y en la sección 3, explicamos la motivación y el contexto del problema de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando los títulos extraídos. La sección 6 presenta nuestros resultados experimentales. Hacemos observaciones finales en la sección 7.2. TRABAJO RELACIONADO 2.1 Se han propuesto métodos de extracción de metadatos de documentos para realizar extracción automática de metadatos de documentos; sin embargo, el enfoque principal ha sido la extracción de artículos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en reglas y el enfoque basado en aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrollaron un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como que los títulos suelen ubicarse en las partes superiores de las primeras páginas y suelen estar en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel et al. [23] realizaron extracción de metadatos de materiales educativos utilizando tecnologías de procesamiento del lenguaje natural basadas en reglas. Mao et al. [16] también llevaron a cabo la extracción automática de metadatos de artículos de investigación utilizando reglas sobre la información de formato. El enfoque basado en reglas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, realizaron extracción de metadatos con enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar Máquinas de Vectores de Soporte como <br>clasificador</br>. Principalmente utilizaron información lingüística como características. Informaron de una alta precisión de extracción de información de artículos de investigación en términos de precisión y recuperación. 2.2 Extracción de Información La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la cual, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Los Modelos Ocultos de Markov [6], el Modelo de Entropía Máxima [1, 4], el Modelo de Entropía Máxima de Markov [17], las Máquinas de Vectores de Soporte [3], el Campo Aleatorio Condicional [12] y el Perceptrón Votado [2] son modelos ampliamente utilizados para la extracción de información. La extracción de información se ha aplicado, por ejemplo, al etiquetado de partes del discurso [20], al reconocimiento de entidades nombradas [25] y a la extracción de tablas [19]. 2.3 Búsqueda utilizando la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. lograron extraer títulos de artículos de investigación y utilizar los títulos extraídos en la búsqueda de metadatos de los artículos [8]. En la búsqueda web, los campos de título (es decir, propiedades de archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al. encontraron que las páginas web con metadatos bien definidos son más fáciles de recuperar que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de Office) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y DEFINICIÓN DEL PROBLEMA Consideramos el problema de extraer automáticamente títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de varios géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, artículos técnicos, folletos, informes, memorandos, especificaciones, cartas, anuncios o currículums. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigar la extracción de títulos de los mismos. La Figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e internet [15]. Office y PDF son los principales formatos de archivo en la intranet. Incluso en internet, los documentos en los formatos siguen siendo significativos, dada su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de Office. Figura 1. Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir títulos como propiedades de archivo utilizando una función proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la función y, por lo tanto, los títulos en las propiedades de los archivos suelen ser muy inexactos. Es decir, los títulos en las propiedades del archivo suelen ser inconsistentes con los verdaderos títulos en los cuerpos de los archivos que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 documentos de Word y 6,000 documentos de PowerPoint de una intranet y de internet, y examinamos cuántos títulos en las propiedades de los archivos son correctos. Descubrimos que sorprendentemente la precisión fue solo de 0.265 (cf., Sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, entonces la propiedad de archivo del nuevo archivo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en las propiedades de archivo de documentos de Office en la búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devueltos por Google. Descubrimos que casi todos los títulos presentados en los resultados de búsqueda eran de las propiedades de archivo de los documentos. Sin embargo, solo 0.272 de ellos fueron correctos. De hecho, los títulos verdaderos suelen encontrarse al principio de los cuerpos de los documentos. Si podemos extraer con precisión los títulos de los cuerpos de los documentos, entonces podemos aprovechar la información de título confiable en el procesamiento de documentos. Este es exactamente el problema que abordamos en este artículo. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, debemos extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Solo consideramos la extracción del título principal. Como líneas base para la extracción de títulos, utilizamos la de siempre usar las primeras líneas como títulos y la de siempre usar las líneas con tamaños de fuente más grandes como títulos. Figura 2. Extracción de título de documento de Word. Figura 3. Extracción de títulos de un documento de PowerPoint. A continuación, definimos una especificación para los juicios humanos en la anotación de datos de títulos. Los datos anotados se utilizarán en el entrenamiento y prueba de los métodos de extracción de títulos. Resumen de la especificación: El título de un documento debe ser identificado en base al sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en los que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación para tales casos. Las reglas incluyen que un título suele estar en líneas consecutivas en el mismo formato, un documento puede no tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como borrador, 147 whitepaper, etc., si es difícil determinar cuál es el título, seleccionar el que tenga el tamaño de fuente más grande y, si aún es difícil determinar cuál es el título, seleccionar el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos). Las figuras 2 y 3 muestran ejemplos de documentos de Office de los cuales realizamos la extracción de títulos. En la Figura 2, Diferencias en las Implementaciones de la API de Win32 entre los Sistemas Operativos de Windows es el título del documento de Word. En la parte superior de esta página hay una imagen de Microsoft Windows que por lo tanto es ignorada. En la Figura 3, \"Construyendo Ventajas Competitivas a través de una Infraestructura Ágil\" es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotadores humanos. La Figura 4 muestra una captura de pantalla de la herramienta. Figura 4. Herramienta de anotación de títulos. 4. MÉTODO DE EXTRACCIÓN DE TÍTULOS 4.1 El método de extracción de títulos basado en aprendizaje automático consta de entrenamiento y extracción. El mismo paso de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el preprocesamiento, se extraen una serie de unidades para el procesamiento de la región superior de la primera página de un documento de Word o de la primera diapositiva de un documento de PowerPoint. Si una línea (las líneas están separadas por símbolos de retorno) solo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no solo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Figura 5. Ejemplo de unidades. En el aprendizaje, la entrada son secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end u otras) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin, title_end u otra. Empleamos cuatro tipos de modelos: Perceptrón, Entropía Máxima (ME), Modelo de Perceptrón de Markov (PMM) y Modelo de Entropía Máxima de Markov (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es título_inicio, título_fin u otro. Luego extraemos las unidades desde la unidad etiquetada con title_begin hasta la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que principalmente utilizamos información de formato para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar esos patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el cual solo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos De hecho, los cuatro modelos pueden considerarse dentro del mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias kxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki ,,2,1 L=). Recuerda que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end, u otro. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )|( 11 kk XXYYP LL donde iX e iY denotan variables aleatorias que toman instancias ix e etiquetas iy como valores, respectivamente ( ki ,,2,1 L= ). Herramienta de extracción de herramientas de aprendizaje 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general para simplificarlo lo suficiente para el entrenamiento. Por ejemplo, podemos asumir que kYY ,,1 L son independientes entre sí dado kXX ,,1 L. De esta manera, descomponemos el modelo en varios clasificadores. Entrenamos los clasificadores localmente utilizando los datos etiquetados. Como <br>clasificador</br>, empleamos el modelo Perceptrón o de Máxima Entropía. También podemos asumir que la propiedad de Markov de primer orden se cumple para kYY ,,1 L dado kXX ,,1 L. Por lo tanto, obtenemos una serie de clasificadores. Sin embargo, los clasificadores están condicionados por la etiqueta previa. Cuando empleamos el modelo Perceptrón o de Entropía Máxima como <br>clasificador</br>, los modelos se convierten en un Modelo Markov Perceptrón o un Modelo Markov de Entropía Máxima, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptrón y ME, asignamos etiquetas localmente y luego combinamos los resultados globalmente utilizando heurísticas. Específicamente, primero identificamos el título más probable. Entonces encontramos el título más probable dentro de tres unidades después del título inicial. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo de Viterbi para encontrar la secuencia de etiquetas globalmente óptima. En este artículo, para el Perceptrón, en realidad empleamos una variante mejorada de él, llamada Perceptrón con Margen Desigual [13]. Esta versión de Perceptrón puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren considerablemente, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del Modelo Perceptrón Markov en la cual el modelo Perceptrón es el llamado Perceptrón Votado [2]. Además, en el entrenamiento, los parámetros del modelo se actualizan de forma global en lugar de localmente. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Principalmente usamos el primero. Las características se utilizan tanto para los clasificadores de inicio de título como para los de fin de título. 4.3.1 Características de formato Tamaño de fuente: Hay cuatro características binarias que representan el tamaño de fuente normalizado de la unidad (recordemos que una unidad tiene solo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario, 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario, 0. Si el tamaño de la fuente es mayor que el tamaño de fuente promedio y no es el más grande en el documento, entonces la segunda característica será 1, de lo contrario, 0. Si el tamaño de la fuente es inferior al tamaño promedio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario, 0. Es necesario realizar la normalización de los tamaños de fuente. Por ejemplo, en un documento el tamaño de fuente más grande podría ser de 12pt, mientras que en otro el más pequeño podría ser de 18pt. Negrita: Esta característica binaria representa si la unidad actual está en negrita o no. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. El siguiente formato de características con respecto al contexto juega un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la siguiente unidad difieren del de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la siguiente difieren de la de la actual. Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad siguiente están en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, asunto:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, es probable que sean líneas de título. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Recuento de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, ∞) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario, 0. Carácter de finalización: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título generalmente no termina con un carácter como ese. 5. MÉTODO DE RECUPERACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Normalmente, en la recuperación de información, un documento se divide en varios campos que incluyen cuerpo, título y texto de anclaje. Una función de clasificación en la búsqueda puede utilizar diferentes pesos para diferentes campos del documento. Además, los títulos suelen recibir un peso alto, lo que indica que son importantes para la recuperación de documentos. Como se explicó anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite la ponderación de campos [21]. Como campos, hacemos uso del cuerpo, título, título extraído y ancla. Primero, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; luego, cada frecuencia de campo se pondera según el parámetro de peso correspondiente: ∑= f tfft tfwwtf De manera similar, calculamos la longitud del documento como una suma ponderada de las longitudes de cada campo. La longitud promedio del documento en el corpus se convierte en el promedio de todas las longitudes de documentos ponderadas. ∑= f ff dlwwdl En nuestros experimentos usamos 75.0,8.11 == bk. El peso para el contenido fue de 1.0, para el título fue de 10.0, para el enlace fue de 10.0 y para el título extraído fue de 5.0. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación. Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos aleatoriamente 5,000 documentos de Word y 5,000 documentos de PowerPoint de una intranet de Microsoft. Lo llamamos MS de ahora en adelante. Segundo, descargamos y seleccionamos aleatoriamente 500 documentos de Word y 500 documentos de PowerPoint de los dominios DotGov y DotCom en internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, de hecho, documentos generales tal como los definimos. Figura 7. Distribuciones de géneros de documentos. Tercero, también se descargó un conjunto de datos en chino de internet. Incluye 500 documentos de Word y 500 documentos de PowerPoint en chino. Etiquetamos manualmente los títulos de todos los documentos, basándonos en nuestra especificación. No todos los documentos en los dos conjuntos de datos tienen títulos. La Tabla 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría deberse a que los documentos de PowerPoint publicados en internet son más formales que los de la intranet. Tabla 1. En nuestros experimentos, realizamos evaluaciones sobre la extracción de títulos en términos de precisión, recuperación y medida F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / (A + B) Recall: R = A / (A + C) F-measure: F1 = 2PR / (P + R) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Tabla 2. Tabla de contingencia con respecto a la extracción de títulos. ¿Es título? ¿No es título? Extraído A B No extraído C D 6.2 Baselines Probamos las precisiones de los dos baselines descritos en la sección 4.2. Se denominan como el tamaño de fuente más grande y la primera línea respectivamente. 6.3 Precisión de los títulos en las propiedades del archivo Investigamos cuántos títulos en las propiedades del archivo de los documentos son confiables. Consideramos los títulos anotados por humanos como títulos verdaderos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos la Distancia de Edición para realizar la coincidencia aproximada. (La coincidencia aproximada solo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados por humanos pueden ser ligeramente diferentes de los títulos en las propiedades del archivo en la superficie, por ejemplo, contener espacios adicionales. Dadas las cadenas A y B: si ((D == 0) o (D / (La + Lb) < θ)) entonces la cadena A = cadena B D: Distancia de edición entre la cadena A y la cadena B La: longitud de la cadena A Lb: longitud de la cadena B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Tabla 3. Precisión de los títulos en las propiedades del archivo Tipo de archivo Dominio Precisión Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186 PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparación con Baselines Realizamos la extracción de títulos del primer conjunto de datos (Word y PowerPoint en MS). Como modelo, utilizamos el Perceptrón. Realizamos validación cruzada de 4 pliegues. Por lo tanto, todos los resultados reportados aquí son aquellos promediados en 4 ensayos. Las tablas 4 y 5 muestran los resultados. Vemos que el Perceptrón supera significativamente a los baselines. En la evaluación, utilizamos coincidencia exacta entre los títulos reales anotados por humanos y los títulos extraídos. Tabla 4. Precisión de la extracción de títulos con el Modelo Perceptrón de Palabra Precisión Recall F1 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Líneas base Primera línea 0.707 0.767 0.736 Tabla 5. Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como la exhaustividad del enfoque son un 8 por ciento más altas que las de las líneas de base. Para PowerPoint, tanto la precisión como la exhaustividad del enfoque son un 2 por ciento más altas que las de las líneas de base. Realizamos pruebas de significancia. Los resultados se muestran en la Tabla 6. Aquí, \"Largest\" denota la línea base de usar el tamaño de fuente más grande, \"First\" denota la línea base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático sobre las líneas de base son estadísticamente significativas (en el sentido de que el valor p < 0.05) Tabla 6. Vemos, a partir de los resultados, que los dos baselines pueden funcionar bien para la extracción de títulos, lo que sugiere que el tamaño de fuente y la información de posición son las características más útiles para la extracción de títulos. Sin embargo, también es evidente que utilizar solo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande solo contienen descripciones generales como Confidencial, Documento blanco, etc. Para esos casos, el método del tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio del título), el Perceptrón puede superar a Largest y First. Investigamos el rendimiento de utilizar únicamente características lingüísticas. Descubrimos que no funciona bien. Parece que las características del formato desempeñan roles importantes y las características lingüísticas son complementos. Figura 8. Un documento de Word de ejemplo. Figura 9. Un documento de PowerPoint de ejemplo. Realizamos un análisis de errores en los resultados del Perceptrón. Encontramos que los errores caían en tres categorías. (1) Aproximadamente un tercio de los errores estaban relacionados con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores provienen de los documentos que no tienen títulos verdaderos, sino que solo contienen viñetas. Dado que realizamos la extracción desde las regiones superiores, es difícil deshacernos de estos errores con el enfoque actual. Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que solo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error no afecta mucho al procesamiento de documentos como la búsqueda. 6.5 Comparación entre Modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Nuevamente, realizamos una validación cruzada de 4 pliegues en el primer conjunto de datos (MS). La tabla 7 y 8 muestran los resultados de los cuatro modelos. Resulta que Perceptrón y PMM tienen el mejor rendimiento, seguidos por MEMM, y ME tiene el peor rendimiento. En general, los modelos markovianos tienen un mejor rendimiento que sus contrapartes <br>clasificadoras</br> o igual de bueno. ",
            "candidates": [],
            "error": [
                [
                    "clasificador",
                    "clasificador",
                    "clasificador",
                    "clasificador",
                    "clasificadoras"
                ]
            ]
        },
        "document retrieval": {
            "translated_key": "recuperación de documentos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in <br>document retrieval</br> by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of <br>document retrieval</br> (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of <br>document retrieval</br> using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for <br>document retrieval</br>.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "<br>document retrieval</br> METHOD We describe our method of <br>document retrieval</br> using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for <br>document retrieval</br>.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for <br>document retrieval</br>.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different <br>document retrieval</br> results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of <br>document retrieval</br>. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in <br>document retrieval</br>.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [
                "Moreover, we can significantly improve search ranking results in <br>document retrieval</br> by using the extracted titles.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of <br>document retrieval</br> (by 10%).",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of <br>document retrieval</br> using extracted titles.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for <br>document retrieval</br>.",
                "<br>document retrieval</br> METHOD We describe our method of <br>document retrieval</br> using extracted titles."
            ],
            "translated_annotated_samples": [
                "Además, podemos mejorar significativamente los resultados de clasificación de búsqueda en la <br>recuperación de documentos</br> utilizando los títulos extraídos.",
                "Resulta que el uso de características de formato es la clave para la exitosa extracción de títulos. (1) Hemos observado que los modelos basados en Perceptrón tienen un mejor rendimiento en términos de precisión de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados en otro, y pueden ser entrenados en un idioma y aplicados en otro. (3) Hemos descubierto que al utilizar los títulos extraídos podemos mejorar significativamente la precisión de la <br>recuperación de documentos</br> (en un 10%).",
                "En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de <br>recuperación de documentos</br> utilizando los títulos extraídos.",
                "La extracción de información se ha aplicado, por ejemplo, al etiquetado de partes del discurso [20], al reconocimiento de entidades nombradas [25] y a la extracción de tablas [19]. 2.3 Búsqueda utilizando la información del título La información del título es útil para la <br>recuperación de documentos</br>.",
                "MÉTODO DE RECUPERACIÓN DE DOCUMENTOS Describimos nuestro método de <br>recuperación de documentos</br> utilizando títulos extraídos."
            ],
            "translated_text": "Extracción automática de títulos de documentos generales utilizando aprendizaje automático. En este documento, proponemos un enfoque de aprendizaje automático para la extracción de títulos de documentos generales. Por documentos generales nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos, incluyendo presentaciones, capítulos de libros, artículos técnicos, folletos, informes y cartas. Anteriormente, se han propuesto métodos principalmente para la extracción de títulos de artículos de investigación. No ha quedado claro si sería posible realizar la extracción automática de títulos de documentos generales. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que principalmente utilizamos información de formato como el tamaño de fuente como características en los modelos. Resulta que el uso de información de formato puede llevar a una extracción bastante precisa de documentos generales. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente en un experimento con datos de intranet. Otros hallazgos importantes en este trabajo incluyen que podemos entrenar modelos en un dominio y aplicarlos a otro dominio, y más sorprendentemente, incluso podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente los resultados de clasificación de búsqueda en la <br>recuperación de documentos</br> utilizando los títulos extraídos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de Búsqueda; H.4.1 [Aplicaciones de Sistemas de Información]: Automatización de Oficinas - Procesamiento de Texto; D.2.8 [Ingeniería de Software]: Métricas - medidas de complejidad, medidas de rendimiento Términos Generales Algoritmos, Experimentación, Rendimiento. 1. METADATA La información de metadatos de los documentos es útil para muchos tipos de procesamiento de documentos, como la búsqueda, la navegación y el filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen los metadatos de los documentos por sí mismas, incluso cuando tienen herramientas convenientes para la definición de metadatos [26]. Por lo tanto, la extracción automática de metadatos de los cuerpos de los documentos resulta ser un tema de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, el enfoque estaba principalmente en la extracción de los documentos de investigación. Por ejemplo, Han et al. [10] propusieron un método basado en aprendizaje automático para realizar extracciones de artículos de investigación. Formalizaron el problema como el de clasificación y emplearon Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron características lingüísticas en el modelo. En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigación sobre la extracción de información de ellos. Los artículos de investigación suelen tener estilos bien formados y características notables. Por el contrario, los estilos de los documentos generales pueden variar considerablemente. No se ha aclarado si un enfoque basado en aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como estudio de caso, consideramos la extracción de títulos en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizamos la extracción de títulos utilizando uno de los tipos de modelos entrenados. En los modelos, principalmente utilizamos información de formato como el tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con Márgenes Desiguales, Modelo de Entropía Máxima de Markov y Perceptrón Votado. En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, ¿cuál modelo funciona mejor para la extracción de títulos?; (2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a los baselines: uno que siempre utiliza las primeras líneas como títulos y el otro que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente. Resulta que el uso de características de formato es la clave para la exitosa extracción de títulos. (1) Hemos observado que los modelos basados en Perceptrón tienen un mejor rendimiento en términos de precisión de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados en otro, y pueden ser entrenados en un idioma y aplicados en otro. (3) Hemos descubierto que al utilizar los títulos extraídos podemos mejorar significativamente la precisión de la <br>recuperación de documentos</br> (en un 10%). Concluimos que podemos realizar de manera fiable la extracción de títulos de documentos generales y utilizar los resultados extraídos para mejorar aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, presentamos el trabajo relacionado, y en la sección 3, explicamos la motivación y el contexto del problema de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de <br>recuperación de documentos</br> utilizando los títulos extraídos. La sección 6 presenta nuestros resultados experimentales. Hacemos observaciones finales en la sección 7.2. TRABAJO RELACIONADO 2.1 Se han propuesto métodos de extracción de metadatos de documentos para realizar extracción automática de metadatos de documentos; sin embargo, el enfoque principal ha sido la extracción de artículos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en reglas y el enfoque basado en aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrollaron un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como que los títulos suelen ubicarse en las partes superiores de las primeras páginas y suelen estar en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel et al. [23] realizaron extracción de metadatos de materiales educativos utilizando tecnologías de procesamiento del lenguaje natural basadas en reglas. Mao et al. [16] también llevaron a cabo la extracción automática de metadatos de artículos de investigación utilizando reglas sobre la información de formato. El enfoque basado en reglas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, realizaron extracción de metadatos con enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron información lingüística como características. Informaron de una alta precisión de extracción de información de artículos de investigación en términos de precisión y recuperación. 2.2 Extracción de Información La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la cual, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Los Modelos Ocultos de Markov [6], el Modelo de Entropía Máxima [1, 4], el Modelo de Entropía Máxima de Markov [17], las Máquinas de Vectores de Soporte [3], el Campo Aleatorio Condicional [12] y el Perceptrón Votado [2] son modelos ampliamente utilizados para la extracción de información. La extracción de información se ha aplicado, por ejemplo, al etiquetado de partes del discurso [20], al reconocimiento de entidades nombradas [25] y a la extracción de tablas [19]. 2.3 Búsqueda utilizando la información del título La información del título es útil para la <br>recuperación de documentos</br>. En el sistema Citeseer, por ejemplo, Giles et al. lograron extraer títulos de artículos de investigación y utilizar los títulos extraídos en la búsqueda de metadatos de los artículos [8]. En la búsqueda web, los campos de título (es decir, propiedades de archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al. encontraron que las páginas web con metadatos bien definidos son más fáciles de recuperar que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de Office) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y DEFINICIÓN DEL PROBLEMA Consideramos el problema de extraer automáticamente títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de varios géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, artículos técnicos, folletos, informes, memorandos, especificaciones, cartas, anuncios o currículums. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigar la extracción de títulos de los mismos. La Figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e internet [15]. Office y PDF son los principales formatos de archivo en la intranet. Incluso en internet, los documentos en los formatos siguen siendo significativos, dada su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de Office. Figura 1. Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir títulos como propiedades de archivo utilizando una función proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la función y, por lo tanto, los títulos en las propiedades de los archivos suelen ser muy inexactos. Es decir, los títulos en las propiedades del archivo suelen ser inconsistentes con los verdaderos títulos en los cuerpos de los archivos que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 documentos de Word y 6,000 documentos de PowerPoint de una intranet y de internet, y examinamos cuántos títulos en las propiedades de los archivos son correctos. Descubrimos que sorprendentemente la precisión fue solo de 0.265 (cf., Sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, entonces la propiedad de archivo del nuevo archivo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en las propiedades de archivo de documentos de Office en la búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devueltos por Google. Descubrimos que casi todos los títulos presentados en los resultados de búsqueda eran de las propiedades de archivo de los documentos. Sin embargo, solo 0.272 de ellos fueron correctos. De hecho, los títulos verdaderos suelen encontrarse al principio de los cuerpos de los documentos. Si podemos extraer con precisión los títulos de los cuerpos de los documentos, entonces podemos aprovechar la información de título confiable en el procesamiento de documentos. Este es exactamente el problema que abordamos en este artículo. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, debemos extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Solo consideramos la extracción del título principal. Como líneas base para la extracción de títulos, utilizamos la de siempre usar las primeras líneas como títulos y la de siempre usar las líneas con tamaños de fuente más grandes como títulos. Figura 2. Extracción de título de documento de Word. Figura 3. Extracción de títulos de un documento de PowerPoint. A continuación, definimos una especificación para los juicios humanos en la anotación de datos de títulos. Los datos anotados se utilizarán en el entrenamiento y prueba de los métodos de extracción de títulos. Resumen de la especificación: El título de un documento debe ser identificado en base al sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en los que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación para tales casos. Las reglas incluyen que un título suele estar en líneas consecutivas en el mismo formato, un documento puede no tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como borrador, 147 whitepaper, etc., si es difícil determinar cuál es el título, seleccionar el que tenga el tamaño de fuente más grande y, si aún es difícil determinar cuál es el título, seleccionar el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos). Las figuras 2 y 3 muestran ejemplos de documentos de Office de los cuales realizamos la extracción de títulos. En la Figura 2, Diferencias en las Implementaciones de la API de Win32 entre los Sistemas Operativos de Windows es el título del documento de Word. En la parte superior de esta página hay una imagen de Microsoft Windows que por lo tanto es ignorada. En la Figura 3, \"Construyendo Ventajas Competitivas a través de una Infraestructura Ágil\" es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotadores humanos. La Figura 4 muestra una captura de pantalla de la herramienta. Figura 4. Herramienta de anotación de títulos. 4. MÉTODO DE EXTRACCIÓN DE TÍTULOS 4.1 El método de extracción de títulos basado en aprendizaje automático consta de entrenamiento y extracción. El mismo paso de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el preprocesamiento, se extraen una serie de unidades para el procesamiento de la región superior de la primera página de un documento de Word o de la primera diapositiva de un documento de PowerPoint. Si una línea (las líneas están separadas por símbolos de retorno) solo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no solo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Figura 5. Ejemplo de unidades. En el aprendizaje, la entrada son secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end u otras) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin, title_end u otra. Empleamos cuatro tipos de modelos: Perceptrón, Entropía Máxima (ME), Modelo de Perceptrón de Markov (PMM) y Modelo de Entropía Máxima de Markov (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es título_inicio, título_fin u otro. Luego extraemos las unidades desde la unidad etiquetada con title_begin hasta la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que principalmente utilizamos información de formato para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar esos patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el cual solo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos De hecho, los cuatro modelos pueden considerarse dentro del mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias kxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki ,,2,1 L=). Recuerda que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end, u otro. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )|( 11 kk XXYYP LL donde iX e iY denotan variables aleatorias que toman instancias ix e etiquetas iy como valores, respectivamente ( ki ,,2,1 L= ). Herramienta de extracción de herramientas de aprendizaje 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general para simplificarlo lo suficiente para el entrenamiento. Por ejemplo, podemos asumir que kYY ,,1 L son independientes entre sí dado kXX ,,1 L. De esta manera, descomponemos el modelo en varios clasificadores. Entrenamos los clasificadores localmente utilizando los datos etiquetados. Como clasificador, empleamos el modelo Perceptrón o de Máxima Entropía. También podemos asumir que la propiedad de Markov de primer orden se cumple para kYY ,,1 L dado kXX ,,1 L. Por lo tanto, obtenemos una serie de clasificadores. Sin embargo, los clasificadores están condicionados por la etiqueta previa. Cuando empleamos el modelo Perceptrón o de Entropía Máxima como clasificador, los modelos se convierten en un Modelo Markov Perceptrón o un Modelo Markov de Entropía Máxima, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptrón y ME, asignamos etiquetas localmente y luego combinamos los resultados globalmente utilizando heurísticas. Específicamente, primero identificamos el título más probable. Entonces encontramos el título más probable dentro de tres unidades después del título inicial. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo de Viterbi para encontrar la secuencia de etiquetas globalmente óptima. En este artículo, para el Perceptrón, en realidad empleamos una variante mejorada de él, llamada Perceptrón con Margen Desigual [13]. Esta versión de Perceptrón puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren considerablemente, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del Modelo Perceptrón Markov en la cual el modelo Perceptrón es el llamado Perceptrón Votado [2]. Además, en el entrenamiento, los parámetros del modelo se actualizan de forma global en lugar de localmente. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Principalmente usamos el primero. Las características se utilizan tanto para los clasificadores de inicio de título como para los de fin de título. 4.3.1 Características de formato Tamaño de fuente: Hay cuatro características binarias que representan el tamaño de fuente normalizado de la unidad (recordemos que una unidad tiene solo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario, 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario, 0. Si el tamaño de la fuente es mayor que el tamaño de fuente promedio y no es el más grande en el documento, entonces la segunda característica será 1, de lo contrario, 0. Si el tamaño de la fuente es inferior al tamaño promedio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario, 0. Es necesario realizar la normalización de los tamaños de fuente. Por ejemplo, en un documento el tamaño de fuente más grande podría ser de 12pt, mientras que en otro el más pequeño podría ser de 18pt. Negrita: Esta característica binaria representa si la unidad actual está en negrita o no. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. El siguiente formato de características con respecto al contexto juega un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la siguiente unidad difieren del de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la siguiente difieren de la de la actual. Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad siguiente están en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, asunto:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, es probable que sean líneas de título. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Recuento de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, ∞) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario, 0. Carácter de finalización: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título generalmente no termina con un carácter como ese. 5. MÉTODO DE RECUPERACIÓN DE DOCUMENTOS Describimos nuestro método de <br>recuperación de documentos</br> utilizando títulos extraídos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "automatic title extraction": {
            "translated_key": "extracción automática de títulos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct <br>automatic title extraction</br> from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [
                "It has not been clear whether it could be possible to conduct <br>automatic title extraction</br> from general documents."
            ],
            "translated_annotated_samples": [
                "No ha quedado claro si sería posible realizar la <br>extracción automática de títulos</br> de documentos generales."
            ],
            "translated_text": "Extracción automática de títulos de documentos generales utilizando aprendizaje automático. En este documento, proponemos un enfoque de aprendizaje automático para la extracción de títulos de documentos generales. Por documentos generales nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos, incluyendo presentaciones, capítulos de libros, artículos técnicos, folletos, informes y cartas. Anteriormente, se han propuesto métodos principalmente para la extracción de títulos de artículos de investigación. No ha quedado claro si sería posible realizar la <br>extracción automática de títulos</br> de documentos generales. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que principalmente utilizamos información de formato como el tamaño de fuente como características en los modelos. Resulta que el uso de información de formato puede llevar a una extracción bastante precisa de documentos generales. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente en un experimento con datos de intranet. Otros hallazgos importantes en este trabajo incluyen que podemos entrenar modelos en un dominio y aplicarlos a otro dominio, y más sorprendentemente, incluso podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente los resultados de clasificación de búsqueda en la recuperación de documentos utilizando los títulos extraídos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de Búsqueda; H.4.1 [Aplicaciones de Sistemas de Información]: Automatización de Oficinas - Procesamiento de Texto; D.2.8 [Ingeniería de Software]: Métricas - medidas de complejidad, medidas de rendimiento Términos Generales Algoritmos, Experimentación, Rendimiento. 1. METADATA La información de metadatos de los documentos es útil para muchos tipos de procesamiento de documentos, como la búsqueda, la navegación y el filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen los metadatos de los documentos por sí mismas, incluso cuando tienen herramientas convenientes para la definición de metadatos [26]. Por lo tanto, la extracción automática de metadatos de los cuerpos de los documentos resulta ser un tema de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, el enfoque estaba principalmente en la extracción de los documentos de investigación. Por ejemplo, Han et al. [10] propusieron un método basado en aprendizaje automático para realizar extracciones de artículos de investigación. Formalizaron el problema como el de clasificación y emplearon Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron características lingüísticas en el modelo. En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigación sobre la extracción de información de ellos. Los artículos de investigación suelen tener estilos bien formados y características notables. Por el contrario, los estilos de los documentos generales pueden variar considerablemente. No se ha aclarado si un enfoque basado en aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como estudio de caso, consideramos la extracción de títulos en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizamos la extracción de títulos utilizando uno de los tipos de modelos entrenados. En los modelos, principalmente utilizamos información de formato como el tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con Márgenes Desiguales, Modelo de Entropía Máxima de Markov y Perceptrón Votado. En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, ¿cuál modelo funciona mejor para la extracción de títulos?; (2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a los baselines: uno que siempre utiliza las primeras líneas como títulos y el otro que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente. Resulta que el uso de características de formato es la clave para la exitosa extracción de títulos. (1) Hemos observado que los modelos basados en Perceptrón tienen un mejor rendimiento en términos de precisión de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados en otro, y pueden ser entrenados en un idioma y aplicados en otro. (3) Hemos descubierto que al utilizar los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que podemos realizar de manera fiable la extracción de títulos de documentos generales y utilizar los resultados extraídos para mejorar aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, presentamos el trabajo relacionado, y en la sección 3, explicamos la motivación y el contexto del problema de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando los títulos extraídos. La sección 6 presenta nuestros resultados experimentales. Hacemos observaciones finales en la sección 7.2. TRABAJO RELACIONADO 2.1 Se han propuesto métodos de extracción de metadatos de documentos para realizar extracción automática de metadatos de documentos; sin embargo, el enfoque principal ha sido la extracción de artículos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en reglas y el enfoque basado en aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrollaron un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como que los títulos suelen ubicarse en las partes superiores de las primeras páginas y suelen estar en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel et al. [23] realizaron extracción de metadatos de materiales educativos utilizando tecnologías de procesamiento del lenguaje natural basadas en reglas. Mao et al. [16] también llevaron a cabo la extracción automática de metadatos de artículos de investigación utilizando reglas sobre la información de formato. El enfoque basado en reglas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, realizaron extracción de metadatos con enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron información lingüística como características. Informaron de una alta precisión de extracción de información de artículos de investigación en términos de precisión y recuperación. 2.2 Extracción de Información La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la cual, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Los Modelos Ocultos de Markov [6], el Modelo de Entropía Máxima [1, 4], el Modelo de Entropía Máxima de Markov [17], las Máquinas de Vectores de Soporte [3], el Campo Aleatorio Condicional [12] y el Perceptrón Votado [2] son modelos ampliamente utilizados para la extracción de información. La extracción de información se ha aplicado, por ejemplo, al etiquetado de partes del discurso [20], al reconocimiento de entidades nombradas [25] y a la extracción de tablas [19]. 2.3 Búsqueda utilizando la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. lograron extraer títulos de artículos de investigación y utilizar los títulos extraídos en la búsqueda de metadatos de los artículos [8]. En la búsqueda web, los campos de título (es decir, propiedades de archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al. encontraron que las páginas web con metadatos bien definidos son más fáciles de recuperar que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de Office) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y DEFINICIÓN DEL PROBLEMA Consideramos el problema de extraer automáticamente títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de varios géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, artículos técnicos, folletos, informes, memorandos, especificaciones, cartas, anuncios o currículums. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigar la extracción de títulos de los mismos. La Figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e internet [15]. Office y PDF son los principales formatos de archivo en la intranet. Incluso en internet, los documentos en los formatos siguen siendo significativos, dada su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de Office. Figura 1. Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir títulos como propiedades de archivo utilizando una función proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la función y, por lo tanto, los títulos en las propiedades de los archivos suelen ser muy inexactos. Es decir, los títulos en las propiedades del archivo suelen ser inconsistentes con los verdaderos títulos en los cuerpos de los archivos que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 documentos de Word y 6,000 documentos de PowerPoint de una intranet y de internet, y examinamos cuántos títulos en las propiedades de los archivos son correctos. Descubrimos que sorprendentemente la precisión fue solo de 0.265 (cf., Sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, entonces la propiedad de archivo del nuevo archivo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en las propiedades de archivo de documentos de Office en la búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devueltos por Google. Descubrimos que casi todos los títulos presentados en los resultados de búsqueda eran de las propiedades de archivo de los documentos. Sin embargo, solo 0.272 de ellos fueron correctos. De hecho, los títulos verdaderos suelen encontrarse al principio de los cuerpos de los documentos. Si podemos extraer con precisión los títulos de los cuerpos de los documentos, entonces podemos aprovechar la información de título confiable en el procesamiento de documentos. Este es exactamente el problema que abordamos en este artículo. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, debemos extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Solo consideramos la extracción del título principal. Como líneas base para la extracción de títulos, utilizamos la de siempre usar las primeras líneas como títulos y la de siempre usar las líneas con tamaños de fuente más grandes como títulos. Figura 2. Extracción de título de documento de Word. Figura 3. Extracción de títulos de un documento de PowerPoint. A continuación, definimos una especificación para los juicios humanos en la anotación de datos de títulos. Los datos anotados se utilizarán en el entrenamiento y prueba de los métodos de extracción de títulos. Resumen de la especificación: El título de un documento debe ser identificado en base al sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en los que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación para tales casos. Las reglas incluyen que un título suele estar en líneas consecutivas en el mismo formato, un documento puede no tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como borrador, 147 whitepaper, etc., si es difícil determinar cuál es el título, seleccionar el que tenga el tamaño de fuente más grande y, si aún es difícil determinar cuál es el título, seleccionar el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos). Las figuras 2 y 3 muestran ejemplos de documentos de Office de los cuales realizamos la extracción de títulos. En la Figura 2, Diferencias en las Implementaciones de la API de Win32 entre los Sistemas Operativos de Windows es el título del documento de Word. En la parte superior de esta página hay una imagen de Microsoft Windows que por lo tanto es ignorada. En la Figura 3, \"Construyendo Ventajas Competitivas a través de una Infraestructura Ágil\" es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotadores humanos. La Figura 4 muestra una captura de pantalla de la herramienta. Figura 4. Herramienta de anotación de títulos. 4. MÉTODO DE EXTRACCIÓN DE TÍTULOS 4.1 El método de extracción de títulos basado en aprendizaje automático consta de entrenamiento y extracción. El mismo paso de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el preprocesamiento, se extraen una serie de unidades para el procesamiento de la región superior de la primera página de un documento de Word o de la primera diapositiva de un documento de PowerPoint. Si una línea (las líneas están separadas por símbolos de retorno) solo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no solo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Figura 5. Ejemplo de unidades. En el aprendizaje, la entrada son secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end u otras) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin, title_end u otra. Empleamos cuatro tipos de modelos: Perceptrón, Entropía Máxima (ME), Modelo de Perceptrón de Markov (PMM) y Modelo de Entropía Máxima de Markov (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es título_inicio, título_fin u otro. Luego extraemos las unidades desde la unidad etiquetada con title_begin hasta la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que principalmente utilizamos información de formato para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar esos patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el cual solo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos De hecho, los cuatro modelos pueden considerarse dentro del mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias kxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki ,,2,1 L=). Recuerda que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end, u otro. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )|( 11 kk XXYYP LL donde iX e iY denotan variables aleatorias que toman instancias ix e etiquetas iy como valores, respectivamente ( ki ,,2,1 L= ). Herramienta de extracción de herramientas de aprendizaje 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general para simplificarlo lo suficiente para el entrenamiento. Por ejemplo, podemos asumir que kYY ,,1 L son independientes entre sí dado kXX ,,1 L. De esta manera, descomponemos el modelo en varios clasificadores. Entrenamos los clasificadores localmente utilizando los datos etiquetados. Como clasificador, empleamos el modelo Perceptrón o de Máxima Entropía. También podemos asumir que la propiedad de Markov de primer orden se cumple para kYY ,,1 L dado kXX ,,1 L. Por lo tanto, obtenemos una serie de clasificadores. Sin embargo, los clasificadores están condicionados por la etiqueta previa. Cuando empleamos el modelo Perceptrón o de Entropía Máxima como clasificador, los modelos se convierten en un Modelo Markov Perceptrón o un Modelo Markov de Entropía Máxima, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptrón y ME, asignamos etiquetas localmente y luego combinamos los resultados globalmente utilizando heurísticas. Específicamente, primero identificamos el título más probable. Entonces encontramos el título más probable dentro de tres unidades después del título inicial. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo de Viterbi para encontrar la secuencia de etiquetas globalmente óptima. En este artículo, para el Perceptrón, en realidad empleamos una variante mejorada de él, llamada Perceptrón con Margen Desigual [13]. Esta versión de Perceptrón puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren considerablemente, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del Modelo Perceptrón Markov en la cual el modelo Perceptrón es el llamado Perceptrón Votado [2]. Además, en el entrenamiento, los parámetros del modelo se actualizan de forma global en lugar de localmente. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Principalmente usamos el primero. Las características se utilizan tanto para los clasificadores de inicio de título como para los de fin de título. 4.3.1 Características de formato Tamaño de fuente: Hay cuatro características binarias que representan el tamaño de fuente normalizado de la unidad (recordemos que una unidad tiene solo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario, 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario, 0. Si el tamaño de la fuente es mayor que el tamaño de fuente promedio y no es el más grande en el documento, entonces la segunda característica será 1, de lo contrario, 0. Si el tamaño de la fuente es inferior al tamaño promedio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario, 0. Es necesario realizar la normalización de los tamaños de fuente. Por ejemplo, en un documento el tamaño de fuente más grande podría ser de 12pt, mientras que en otro el más pequeño podría ser de 18pt. Negrita: Esta característica binaria representa si la unidad actual está en negrita o no. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. El siguiente formato de características con respecto al contexto juega un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la siguiente unidad difieren del de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la siguiente difieren de la de la actual. Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad siguiente están en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, asunto:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, es probable que sean líneas de título. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Recuento de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, ∞) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario, 0. Carácter de finalización: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título generalmente no termina con un carácter como ese. 5. MÉTODO DE RECUPERACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Normalmente, en la recuperación de información, un documento se divide en varios campos que incluyen cuerpo, título y texto de anclaje. Una función de clasificación en la búsqueda puede utilizar diferentes pesos para diferentes campos del documento. Además, los títulos suelen recibir un peso alto, lo que indica que son importantes para la recuperación de documentos. Como se explicó anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite la ponderación de campos [21]. Como campos, hacemos uso del cuerpo, título, título extraído y ancla. Primero, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; luego, cada frecuencia de campo se pondera según el parámetro de peso correspondiente: ∑= f tfft tfwwtf De manera similar, calculamos la longitud del documento como una suma ponderada de las longitudes de cada campo. La longitud promedio del documento en el corpus se convierte en el promedio de todas las longitudes de documentos ponderadas. ∑= f ff dlwwdl En nuestros experimentos usamos 75.0,8.11 == bk. El peso para el contenido fue de 1.0, para el título fue de 10.0, para el enlace fue de 10.0 y para el título extraído fue de 5.0. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación. Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos aleatoriamente 5,000 documentos de Word y 5,000 documentos de PowerPoint de una intranet de Microsoft. Lo llamamos MS de ahora en adelante. Segundo, descargamos y seleccionamos aleatoriamente 500 documentos de Word y 500 documentos de PowerPoint de los dominios DotGov y DotCom en internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, de hecho, documentos generales tal como los definimos. Figura 7. Distribuciones de géneros de documentos. Tercero, también se descargó un conjunto de datos en chino de internet. Incluye 500 documentos de Word y 500 documentos de PowerPoint en chino. Etiquetamos manualmente los títulos de todos los documentos, basándonos en nuestra especificación. No todos los documentos en los dos conjuntos de datos tienen títulos. La Tabla 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría deberse a que los documentos de PowerPoint publicados en internet son más formales que los de la intranet. Tabla 1. En nuestros experimentos, realizamos evaluaciones sobre la extracción de títulos en términos de precisión, recuperación y medida F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / (A + B) Recall: R = A / (A + C) F-measure: F1 = 2PR / (P + R) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Tabla 2. Tabla de contingencia con respecto a la extracción de títulos. ¿Es título? ¿No es título? Extraído A B No extraído C D 6.2 Baselines Probamos las precisiones de los dos baselines descritos en la sección 4.2. Se denominan como el tamaño de fuente más grande y la primera línea respectivamente. 6.3 Precisión de los títulos en las propiedades del archivo Investigamos cuántos títulos en las propiedades del archivo de los documentos son confiables. Consideramos los títulos anotados por humanos como títulos verdaderos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos la Distancia de Edición para realizar la coincidencia aproximada. (La coincidencia aproximada solo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados por humanos pueden ser ligeramente diferentes de los títulos en las propiedades del archivo en la superficie, por ejemplo, contener espacios adicionales. Dadas las cadenas A y B: si ((D == 0) o (D / (La + Lb) < θ)) entonces la cadena A = cadena B D: Distancia de edición entre la cadena A y la cadena B La: longitud de la cadena A Lb: longitud de la cadena B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Tabla 3. Precisión de los títulos en las propiedades del archivo Tipo de archivo Dominio Precisión Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186 PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparación con Baselines Realizamos la extracción de títulos del primer conjunto de datos (Word y PowerPoint en MS). Como modelo, utilizamos el Perceptrón. Realizamos validación cruzada de 4 pliegues. Por lo tanto, todos los resultados reportados aquí son aquellos promediados en 4 ensayos. Las tablas 4 y 5 muestran los resultados. Vemos que el Perceptrón supera significativamente a los baselines. En la evaluación, utilizamos coincidencia exacta entre los títulos reales anotados por humanos y los títulos extraídos. Tabla 4. Precisión de la extracción de títulos con el Modelo Perceptrón de Palabra Precisión Recall F1 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Líneas base Primera línea 0.707 0.767 0.736 Tabla 5. Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como la exhaustividad del enfoque son un 8 por ciento más altas que las de las líneas de base. Para PowerPoint, tanto la precisión como la exhaustividad del enfoque son un 2 por ciento más altas que las de las líneas de base. Realizamos pruebas de significancia. Los resultados se muestran en la Tabla 6. Aquí, \"Largest\" denota la línea base de usar el tamaño de fuente más grande, \"First\" denota la línea base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático sobre las líneas de base son estadísticamente significativas (en el sentido de que el valor p < 0.05) Tabla 6. Vemos, a partir de los resultados, que los dos baselines pueden funcionar bien para la extracción de títulos, lo que sugiere que el tamaño de fuente y la información de posición son las características más útiles para la extracción de títulos. Sin embargo, también es evidente que utilizar solo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande solo contienen descripciones generales como Confidencial, Documento blanco, etc. Para esos casos, el método del tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio del título), el Perceptrón puede superar a Largest y First. Investigamos el rendimiento de utilizar únicamente características lingüísticas. Descubrimos que no funciona bien. Parece que las características del formato desempeñan roles importantes y las características lingüísticas son complementos. Figura 8. Un documento de Word de ejemplo. Figura 9. Un documento de PowerPoint de ejemplo. Realizamos un análisis de errores en los resultados del Perceptrón. Encontramos que los errores caían en tres categorías. (1) Aproximadamente un tercio de los errores estaban relacionados con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores provienen de los documentos que no tienen títulos verdaderos, sino que solo contienen viñetas. Dado que realizamos la extracción desde las regiones superiores, es difícil deshacernos de estos errores con el enfoque actual. Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que solo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error no afecta mucho al procesamiento de documentos como la búsqueda. 6.5 Comparación entre Modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Nuevamente, realizamos una validación cruzada de 4 pliegues en el primer conjunto de datos (MS). La tabla 7 y 8 muestran los resultados de los cuatro modelos. Resulta que Perceptrón y PMM tienen el mejor rendimiento, seguidos por MEMM, y ME tiene el peor rendimiento. En general, los modelos markovianos tienen un mejor rendimiento que sus contrapartes clasificadoras o igual de bueno. Esto parece ser porque los modelos markovianos se entrenan de forma global, mientras que los clasificadores se entrenan de forma local. Los modelos basados en el Perceptrón tienen un mejor rendimiento que sus contrapartes basadas en el ME. Esto parece ser porque los modelos basados en Perceptrón están creados para realizar mejores clasificaciones, mientras que los modelos de ME se construyen para una mejor predicción. Tabla 7. Comparación entre diferentes modelos de aprendizaje para la extracción de títulos con Modelo de Palabra Precisión Recall F1 Perceptrón 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de títulos con el Modelo de PowerPoint Precisión Recall F1 Perceptrón 0.875 0.895 0.885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). Las tablas 9-12 muestran los resultados. Tabla 9. Precisión de la extracción de títulos con Word en DotGov Precisión Recall F1 Modelo Perceptrón 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582 Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Precisión de la extracción de títulos con PowerPoint en DotGov Precision Recall F1 Modelo Perceptrón 0.900 0.906 0.903 Tamaño de fuente más grande 0.871 0.888 0.879 Baselines Primera línea 0.554 0.564 0.559 Tabla 11. Precisión de la extracción de títulos con Word en DotCom Precisión Recall F1 Modelo Perceptrón 0.832 0.880 0.855 Tamaño de fuente más grande 0.676 0.753 0.712 Baselines Primera línea 0.577 0.643 0.608 Tabla 12. Rendimiento de la extracción de títulos de documentos de PowerPoint en el modelo de Precisión, Recall y F1 de Perceptrón DotCom 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875 Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos pueden adaptarse bien a diferentes dominios. Casi no hay disminución en la precisión. Los resultados indican que los patrones de formatos de títulos existen en diferentes dominios, y es posible construir un modelo independiente del dominio utilizando principalmente información de formato. 6.7 Adaptación de idioma Aplicamos el modelo entrenado con los datos en inglés (MS) al conjunto de datos en chino. Las tablas 13-14 muestran los resultados. Tabla 13. Precisión de la extracción de títulos con Word en chino: Precisión Recall F1 Modelo Perceptrón 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738 Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Vemos que los modelos se pueden adaptar a un idioma diferente. Solo hay pequeñas caídas en la precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no usarlas es insignificante. Los resultados indican que los patrones de formatos de títulos existen en diferentes idiomas. A partir de los resultados de adaptación de dominio y adaptación de lenguaje, concluimos que el uso de información de formato es clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con Títulos Extraídos Realizamos experimentos utilizando la extracción de títulos para la recuperación de documentos. Como línea base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación fue como se describe en la Sección 5. Los pesos fueron establecidos heurísticamente. No realizamos la optimización de los pesos. La evaluación se llevó a cabo en un corpus de 1.3 millones de documentos extraídos de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de los registros de consultas del motor de búsqueda de esta intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 consultas fueron elegidas al azar. A los usuarios se les pidió que proporcionaran juicios sobre el grado de relevancia del documento en una escala del 1 al 5 (1 significando perjudicial, 2 - malo, 3 - regular, 4 - bueno y 5 - excelente). La figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión al considerar documentos buenos o excelentes como relevantes (tres barras izquierdas con umbral de relevancia 0.5), o al considerar solo documentos excelentes como relevantes (tres barras derechas con umbral de relevancia 1.0). Resultados de clasificación de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo los campos cuerpo, título (propiedad de archivo) y texto de anclaje. • Barra morada - BM25 incluyendo los campos cuerpo, título (propiedad de archivo), texto de anclaje y título extraído. Con el campo adicional de título extraído incluido en BM25, la precisión @10 aumentó de 0.132 a 0.145, o aproximadamente un 10%. Por lo tanto, se puede afirmar que el uso del título extraído puede mejorar efectivamente la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este documento, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. Trabajos anteriores mostraron que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de artículos de investigación. En este artículo, demostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas base en la extracción de títulos de documentos de Office. Trabajos anteriores sobre extracción de metadatos principalmente utilizaron características lingüísticas en los documentos, mientras que nosotros principalmente utilizamos información de formato. Parecía que el uso de información de formato es clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Probamos diferentes modelos de aprendizaje automático incluyendo Perceptrón, Entropía Máxima, Modelo de Markov de Entropía Máxima y Perceptrón Votado. Descubrimos que el rendimiento de los modelos Perceptrón fue el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos entrenados en un idioma a otro idioma. Encontramos que las precisiones no disminuyeron sustancialmente en diferentes dominios y en diferentes idiomas, lo que indica que los modelos eran genéricos. También intentamos utilizar los títulos extraídos en la recuperación de documentos. Observamos una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda al utilizar la información del título extraída. Todas las investigaciones anteriores no se llevaron a cabo, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción de títulos. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu y a los revisores anónimos de JCDL05 por sus valiosos comentarios sobre este artículo. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de entropía máxima para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminativo para modelos ocultos de Markov: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en Procesamiento de Lenguaje Natural, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de vectores de soporte. Aprendizaje automático, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información de texto semiestructurado y libre. En Actas de la Decimoctava Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia newsblaster: resumen multilingüe de noticias en la web. En Actas de la conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos ocultos de Markov factorial. Aprendizaje automático, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de Visualización de Información de 1999, 446-451, 1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., y Pal, N. eBizSearch: un motor de búsqueda de nicho para e-Business. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 413-414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. Extracción de metadatos basada en conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de Bibliotecas Digitales de la ACM, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos utilizando máquinas de vectores de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y etiquetar datos de secuencias. En Actas de la Decimoctava Conferencia Internacional sobre Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo del perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial en nuevos formatos de contenido. En Actas de la Séptima Conferencia de Motores de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., y Thoma, G. R. Un sistema de generación de características dinámicas para la extracción automatizada de metadatos en la preservación de materiales digitales. En Actas del Primer Taller Internacional sobre Análisis de Imágenes de Documentos para Bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de máxima entropía de Markov para extracción de información y segmentación. En Actas de la Decimoséptima Conferencia Internacional sobre Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésimo-Primera Conferencia Internacional Anual de Ciencias de Sistemas de Hawái, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas utilizando campos aleatorios condicionales. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para la unión de frases preposicionales. En Actas de la Decimoséptima Conferencia Internacional de Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Extensión simple de BM25 a múltiples campos ponderados, En Actas de la Decimotercera Conferencia de ACM sobre Información y Gestión del Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Minería web basada en metadatos para relevancia, En Actas del Simposio Internacional de Ingeniería y Aplicaciones de Bases de Datos 2000, 113-121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema de procesamiento de lenguaje natural para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE sobre Bibliotecas Digitales de 2004, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda de Internet a la implementación de metadatos Dublin Core. Revista de Ciencia de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y., y Zhang, T. Reconocimiento y uso de entidades nombradas: reconocimiento de entidades nombradas enfocado utilizando aprendizaje automático. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "information extraction": {
            "translated_key": "extracción de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 <br>information extraction</br> Metadata extraction can be viewed as an application of <br>information extraction</br>, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used <br>information extraction</br> models.",
                "<br>information extraction</br> has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to <br>information extraction</br> from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for <br>information extraction</br> and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 <br>information extraction</br> Metadata extraction can be viewed as an application of <br>information extraction</br>, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used <br>information extraction</br> models.",
                "<br>information extraction</br> has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to <br>information extraction</br> from semi-structured and free text.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for <br>information extraction</br> and segmentation."
            ],
            "translated_annotated_samples": [
                "Informaron de una alta precisión de <br>extracción de información</br> de artículos de investigación en términos de precisión y recuperación. 2.2 Extracción de Información La extracción de metadatos puede ser vista como una aplicación de <br>extracción de información</br>, en la cual, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados.",
                "Los Modelos Ocultos de Markov [6], el Modelo de Entropía Máxima [1, 4], el Modelo de Entropía Máxima de Markov [17], las Máquinas de Vectores de Soporte [3], el Campo Aleatorio Condicional [12] y el Perceptrón Votado [2] son modelos ampliamente utilizados para la <br>extracción de información</br>.",
                "La <br>extracción de información</br> se ha aplicado, por ejemplo, al etiquetado de partes del discurso [20], al reconocimiento de entidades nombradas [25] y a la extracción de tablas [19]. 2.3 Búsqueda utilizando la información del título La información del título es útil para la recuperación de documentos.",
                "Aprendizaje automático, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la <br>extracción de información</br> de texto semiestructurado y libre.",
                "En Actas del Primer Taller Internacional sobre Análisis de Imágenes de Documentos para Bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de máxima entropía de Markov para <br>extracción de información</br> y segmentación."
            ],
            "translated_text": "Extracción automática de títulos de documentos generales utilizando aprendizaje automático. En este documento, proponemos un enfoque de aprendizaje automático para la extracción de títulos de documentos generales. Por documentos generales nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos, incluyendo presentaciones, capítulos de libros, artículos técnicos, folletos, informes y cartas. Anteriormente, se han propuesto métodos principalmente para la extracción de títulos de artículos de investigación. No ha quedado claro si sería posible realizar la extracción automática de títulos de documentos generales. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que principalmente utilizamos información de formato como el tamaño de fuente como características en los modelos. Resulta que el uso de información de formato puede llevar a una extracción bastante precisa de documentos generales. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente en un experimento con datos de intranet. Otros hallazgos importantes en este trabajo incluyen que podemos entrenar modelos en un dominio y aplicarlos a otro dominio, y más sorprendentemente, incluso podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente los resultados de clasificación de búsqueda en la recuperación de documentos utilizando los títulos extraídos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de Búsqueda; H.4.1 [Aplicaciones de Sistemas de Información]: Automatización de Oficinas - Procesamiento de Texto; D.2.8 [Ingeniería de Software]: Métricas - medidas de complejidad, medidas de rendimiento Términos Generales Algoritmos, Experimentación, Rendimiento. 1. METADATA La información de metadatos de los documentos es útil para muchos tipos de procesamiento de documentos, como la búsqueda, la navegación y el filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen los metadatos de los documentos por sí mismas, incluso cuando tienen herramientas convenientes para la definición de metadatos [26]. Por lo tanto, la extracción automática de metadatos de los cuerpos de los documentos resulta ser un tema de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, el enfoque estaba principalmente en la extracción de los documentos de investigación. Por ejemplo, Han et al. [10] propusieron un método basado en aprendizaje automático para realizar extracciones de artículos de investigación. Formalizaron el problema como el de clasificación y emplearon Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron características lingüísticas en el modelo. En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigación sobre la extracción de información de ellos. Los artículos de investigación suelen tener estilos bien formados y características notables. Por el contrario, los estilos de los documentos generales pueden variar considerablemente. No se ha aclarado si un enfoque basado en aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como estudio de caso, consideramos la extracción de títulos en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizamos la extracción de títulos utilizando uno de los tipos de modelos entrenados. En los modelos, principalmente utilizamos información de formato como el tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con Márgenes Desiguales, Modelo de Entropía Máxima de Markov y Perceptrón Votado. En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, ¿cuál modelo funciona mejor para la extracción de títulos?; (2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a los baselines: uno que siempre utiliza las primeras líneas como títulos y el otro que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente. Resulta que el uso de características de formato es la clave para la exitosa extracción de títulos. (1) Hemos observado que los modelos basados en Perceptrón tienen un mejor rendimiento en términos de precisión de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados en otro, y pueden ser entrenados en un idioma y aplicados en otro. (3) Hemos descubierto que al utilizar los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que podemos realizar de manera fiable la extracción de títulos de documentos generales y utilizar los resultados extraídos para mejorar aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, presentamos el trabajo relacionado, y en la sección 3, explicamos la motivación y el contexto del problema de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando los títulos extraídos. La sección 6 presenta nuestros resultados experimentales. Hacemos observaciones finales en la sección 7.2. TRABAJO RELACIONADO 2.1 Se han propuesto métodos de extracción de metadatos de documentos para realizar extracción automática de metadatos de documentos; sin embargo, el enfoque principal ha sido la extracción de artículos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en reglas y el enfoque basado en aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrollaron un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como que los títulos suelen ubicarse en las partes superiores de las primeras páginas y suelen estar en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel et al. [23] realizaron extracción de metadatos de materiales educativos utilizando tecnologías de procesamiento del lenguaje natural basadas en reglas. Mao et al. [16] también llevaron a cabo la extracción automática de metadatos de artículos de investigación utilizando reglas sobre la información de formato. El enfoque basado en reglas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, realizaron extracción de metadatos con enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron información lingüística como características. Informaron de una alta precisión de <br>extracción de información</br> de artículos de investigación en términos de precisión y recuperación. 2.2 Extracción de Información La extracción de metadatos puede ser vista como una aplicación de <br>extracción de información</br>, en la cual, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Los Modelos Ocultos de Markov [6], el Modelo de Entropía Máxima [1, 4], el Modelo de Entropía Máxima de Markov [17], las Máquinas de Vectores de Soporte [3], el Campo Aleatorio Condicional [12] y el Perceptrón Votado [2] son modelos ampliamente utilizados para la <br>extracción de información</br>. La <br>extracción de información</br> se ha aplicado, por ejemplo, al etiquetado de partes del discurso [20], al reconocimiento de entidades nombradas [25] y a la extracción de tablas [19]. 2.3 Búsqueda utilizando la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. lograron extraer títulos de artículos de investigación y utilizar los títulos extraídos en la búsqueda de metadatos de los artículos [8]. En la búsqueda web, los campos de título (es decir, propiedades de archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al. encontraron que las páginas web con metadatos bien definidos son más fáciles de recuperar que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de Office) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y DEFINICIÓN DEL PROBLEMA Consideramos el problema de extraer automáticamente títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de varios géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, artículos técnicos, folletos, informes, memorandos, especificaciones, cartas, anuncios o currículums. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigar la extracción de títulos de los mismos. La Figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e internet [15]. Office y PDF son los principales formatos de archivo en la intranet. Incluso en internet, los documentos en los formatos siguen siendo significativos, dada su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de Office. Figura 1. Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir títulos como propiedades de archivo utilizando una función proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la función y, por lo tanto, los títulos en las propiedades de los archivos suelen ser muy inexactos. Es decir, los títulos en las propiedades del archivo suelen ser inconsistentes con los verdaderos títulos en los cuerpos de los archivos que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 documentos de Word y 6,000 documentos de PowerPoint de una intranet y de internet, y examinamos cuántos títulos en las propiedades de los archivos son correctos. Descubrimos que sorprendentemente la precisión fue solo de 0.265 (cf., Sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, entonces la propiedad de archivo del nuevo archivo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en las propiedades de archivo de documentos de Office en la búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devueltos por Google. Descubrimos que casi todos los títulos presentados en los resultados de búsqueda eran de las propiedades de archivo de los documentos. Sin embargo, solo 0.272 de ellos fueron correctos. De hecho, los títulos verdaderos suelen encontrarse al principio de los cuerpos de los documentos. Si podemos extraer con precisión los títulos de los cuerpos de los documentos, entonces podemos aprovechar la información de título confiable en el procesamiento de documentos. Este es exactamente el problema que abordamos en este artículo. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, debemos extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Solo consideramos la extracción del título principal. Como líneas base para la extracción de títulos, utilizamos la de siempre usar las primeras líneas como títulos y la de siempre usar las líneas con tamaños de fuente más grandes como títulos. Figura 2. Extracción de título de documento de Word. Figura 3. Extracción de títulos de un documento de PowerPoint. A continuación, definimos una especificación para los juicios humanos en la anotación de datos de títulos. Los datos anotados se utilizarán en el entrenamiento y prueba de los métodos de extracción de títulos. Resumen de la especificación: El título de un documento debe ser identificado en base al sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en los que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación para tales casos. Las reglas incluyen que un título suele estar en líneas consecutivas en el mismo formato, un documento puede no tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como borrador, 147 whitepaper, etc., si es difícil determinar cuál es el título, seleccionar el que tenga el tamaño de fuente más grande y, si aún es difícil determinar cuál es el título, seleccionar el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos). Las figuras 2 y 3 muestran ejemplos de documentos de Office de los cuales realizamos la extracción de títulos. En la Figura 2, Diferencias en las Implementaciones de la API de Win32 entre los Sistemas Operativos de Windows es el título del documento de Word. En la parte superior de esta página hay una imagen de Microsoft Windows que por lo tanto es ignorada. En la Figura 3, \"Construyendo Ventajas Competitivas a través de una Infraestructura Ágil\" es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotadores humanos. La Figura 4 muestra una captura de pantalla de la herramienta. Figura 4. Herramienta de anotación de títulos. 4. MÉTODO DE EXTRACCIÓN DE TÍTULOS 4.1 El método de extracción de títulos basado en aprendizaje automático consta de entrenamiento y extracción. El mismo paso de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el preprocesamiento, se extraen una serie de unidades para el procesamiento de la región superior de la primera página de un documento de Word o de la primera diapositiva de un documento de PowerPoint. Si una línea (las líneas están separadas por símbolos de retorno) solo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no solo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Figura 5. Ejemplo de unidades. En el aprendizaje, la entrada son secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end u otras) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin, title_end u otra. Empleamos cuatro tipos de modelos: Perceptrón, Entropía Máxima (ME), Modelo de Perceptrón de Markov (PMM) y Modelo de Entropía Máxima de Markov (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es título_inicio, título_fin u otro. Luego extraemos las unidades desde la unidad etiquetada con title_begin hasta la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que principalmente utilizamos información de formato para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar esos patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el cual solo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos De hecho, los cuatro modelos pueden considerarse dentro del mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias kxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki ,,2,1 L=). Recuerda que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end, u otro. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )|( 11 kk XXYYP LL donde iX e iY denotan variables aleatorias que toman instancias ix e etiquetas iy como valores, respectivamente ( ki ,,2,1 L= ). Herramienta de extracción de herramientas de aprendizaje 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general para simplificarlo lo suficiente para el entrenamiento. Por ejemplo, podemos asumir que kYY ,,1 L son independientes entre sí dado kXX ,,1 L. De esta manera, descomponemos el modelo en varios clasificadores. Entrenamos los clasificadores localmente utilizando los datos etiquetados. Como clasificador, empleamos el modelo Perceptrón o de Máxima Entropía. También podemos asumir que la propiedad de Markov de primer orden se cumple para kYY ,,1 L dado kXX ,,1 L. Por lo tanto, obtenemos una serie de clasificadores. Sin embargo, los clasificadores están condicionados por la etiqueta previa. Cuando empleamos el modelo Perceptrón o de Entropía Máxima como clasificador, los modelos se convierten en un Modelo Markov Perceptrón o un Modelo Markov de Entropía Máxima, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptrón y ME, asignamos etiquetas localmente y luego combinamos los resultados globalmente utilizando heurísticas. Específicamente, primero identificamos el título más probable. Entonces encontramos el título más probable dentro de tres unidades después del título inicial. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo de Viterbi para encontrar la secuencia de etiquetas globalmente óptima. En este artículo, para el Perceptrón, en realidad empleamos una variante mejorada de él, llamada Perceptrón con Margen Desigual [13]. Esta versión de Perceptrón puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren considerablemente, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del Modelo Perceptrón Markov en la cual el modelo Perceptrón es el llamado Perceptrón Votado [2]. Además, en el entrenamiento, los parámetros del modelo se actualizan de forma global en lugar de localmente. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Principalmente usamos el primero. Las características se utilizan tanto para los clasificadores de inicio de título como para los de fin de título. 4.3.1 Características de formato Tamaño de fuente: Hay cuatro características binarias que representan el tamaño de fuente normalizado de la unidad (recordemos que una unidad tiene solo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario, 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario, 0. Si el tamaño de la fuente es mayor que el tamaño de fuente promedio y no es el más grande en el documento, entonces la segunda característica será 1, de lo contrario, 0. Si el tamaño de la fuente es inferior al tamaño promedio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario, 0. Es necesario realizar la normalización de los tamaños de fuente. Por ejemplo, en un documento el tamaño de fuente más grande podría ser de 12pt, mientras que en otro el más pequeño podría ser de 18pt. Negrita: Esta característica binaria representa si la unidad actual está en negrita o no. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. El siguiente formato de características con respecto al contexto juega un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la siguiente unidad difieren del de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la siguiente difieren de la de la actual. Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad siguiente están en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, asunto:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, es probable que sean líneas de título. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Recuento de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, ∞) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario, 0. Carácter de finalización: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título generalmente no termina con un carácter como ese. 5. MÉTODO DE RECUPERACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Normalmente, en la recuperación de información, un documento se divide en varios campos que incluyen cuerpo, título y texto de anclaje. Una función de clasificación en la búsqueda puede utilizar diferentes pesos para diferentes campos del documento. Además, los títulos suelen recibir un peso alto, lo que indica que son importantes para la recuperación de documentos. Como se explicó anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite la ponderación de campos [21]. Como campos, hacemos uso del cuerpo, título, título extraído y ancla. Primero, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; luego, cada frecuencia de campo se pondera según el parámetro de peso correspondiente: ∑= f tfft tfwwtf De manera similar, calculamos la longitud del documento como una suma ponderada de las longitudes de cada campo. La longitud promedio del documento en el corpus se convierte en el promedio de todas las longitudes de documentos ponderadas. ∑= f ff dlwwdl En nuestros experimentos usamos 75.0,8.11 == bk. El peso para el contenido fue de 1.0, para el título fue de 10.0, para el enlace fue de 10.0 y para el título extraído fue de 5.0. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación. Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos aleatoriamente 5,000 documentos de Word y 5,000 documentos de PowerPoint de una intranet de Microsoft. Lo llamamos MS de ahora en adelante. Segundo, descargamos y seleccionamos aleatoriamente 500 documentos de Word y 500 documentos de PowerPoint de los dominios DotGov y DotCom en internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, de hecho, documentos generales tal como los definimos. Figura 7. Distribuciones de géneros de documentos. Tercero, también se descargó un conjunto de datos en chino de internet. Incluye 500 documentos de Word y 500 documentos de PowerPoint en chino. Etiquetamos manualmente los títulos de todos los documentos, basándonos en nuestra especificación. No todos los documentos en los dos conjuntos de datos tienen títulos. La Tabla 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría deberse a que los documentos de PowerPoint publicados en internet son más formales que los de la intranet. Tabla 1. En nuestros experimentos, realizamos evaluaciones sobre la extracción de títulos en términos de precisión, recuperación y medida F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / (A + B) Recall: R = A / (A + C) F-measure: F1 = 2PR / (P + R) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Tabla 2. Tabla de contingencia con respecto a la extracción de títulos. ¿Es título? ¿No es título? Extraído A B No extraído C D 6.2 Baselines Probamos las precisiones de los dos baselines descritos en la sección 4.2. Se denominan como el tamaño de fuente más grande y la primera línea respectivamente. 6.3 Precisión de los títulos en las propiedades del archivo Investigamos cuántos títulos en las propiedades del archivo de los documentos son confiables. Consideramos los títulos anotados por humanos como títulos verdaderos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos la Distancia de Edición para realizar la coincidencia aproximada. (La coincidencia aproximada solo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados por humanos pueden ser ligeramente diferentes de los títulos en las propiedades del archivo en la superficie, por ejemplo, contener espacios adicionales. Dadas las cadenas A y B: si ((D == 0) o (D / (La + Lb) < θ)) entonces la cadena A = cadena B D: Distancia de edición entre la cadena A y la cadena B La: longitud de la cadena A Lb: longitud de la cadena B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Tabla 3. Precisión de los títulos en las propiedades del archivo Tipo de archivo Dominio Precisión Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186 PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparación con Baselines Realizamos la extracción de títulos del primer conjunto de datos (Word y PowerPoint en MS). Como modelo, utilizamos el Perceptrón. Realizamos validación cruzada de 4 pliegues. Por lo tanto, todos los resultados reportados aquí son aquellos promediados en 4 ensayos. Las tablas 4 y 5 muestran los resultados. Vemos que el Perceptrón supera significativamente a los baselines. En la evaluación, utilizamos coincidencia exacta entre los títulos reales anotados por humanos y los títulos extraídos. Tabla 4. Precisión de la extracción de títulos con el Modelo Perceptrón de Palabra Precisión Recall F1 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Líneas base Primera línea 0.707 0.767 0.736 Tabla 5. Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como la exhaustividad del enfoque son un 8 por ciento más altas que las de las líneas de base. Para PowerPoint, tanto la precisión como la exhaustividad del enfoque son un 2 por ciento más altas que las de las líneas de base. Realizamos pruebas de significancia. Los resultados se muestran en la Tabla 6. Aquí, \"Largest\" denota la línea base de usar el tamaño de fuente más grande, \"First\" denota la línea base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático sobre las líneas de base son estadísticamente significativas (en el sentido de que el valor p < 0.05) Tabla 6. Vemos, a partir de los resultados, que los dos baselines pueden funcionar bien para la extracción de títulos, lo que sugiere que el tamaño de fuente y la información de posición son las características más útiles para la extracción de títulos. Sin embargo, también es evidente que utilizar solo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande solo contienen descripciones generales como Confidencial, Documento blanco, etc. Para esos casos, el método del tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio del título), el Perceptrón puede superar a Largest y First. Investigamos el rendimiento de utilizar únicamente características lingüísticas. Descubrimos que no funciona bien. Parece que las características del formato desempeñan roles importantes y las características lingüísticas son complementos. Figura 8. Un documento de Word de ejemplo. Figura 9. Un documento de PowerPoint de ejemplo. Realizamos un análisis de errores en los resultados del Perceptrón. Encontramos que los errores caían en tres categorías. (1) Aproximadamente un tercio de los errores estaban relacionados con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores provienen de los documentos que no tienen títulos verdaderos, sino que solo contienen viñetas. Dado que realizamos la extracción desde las regiones superiores, es difícil deshacernos de estos errores con el enfoque actual. Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que solo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error no afecta mucho al procesamiento de documentos como la búsqueda. 6.5 Comparación entre Modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Nuevamente, realizamos una validación cruzada de 4 pliegues en el primer conjunto de datos (MS). La tabla 7 y 8 muestran los resultados de los cuatro modelos. Resulta que Perceptrón y PMM tienen el mejor rendimiento, seguidos por MEMM, y ME tiene el peor rendimiento. En general, los modelos markovianos tienen un mejor rendimiento que sus contrapartes clasificadoras o igual de bueno. Esto parece ser porque los modelos markovianos se entrenan de forma global, mientras que los clasificadores se entrenan de forma local. Los modelos basados en el Perceptrón tienen un mejor rendimiento que sus contrapartes basadas en el ME. Esto parece ser porque los modelos basados en Perceptrón están creados para realizar mejores clasificaciones, mientras que los modelos de ME se construyen para una mejor predicción. Tabla 7. Comparación entre diferentes modelos de aprendizaje para la extracción de títulos con Modelo de Palabra Precisión Recall F1 Perceptrón 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de títulos con el Modelo de PowerPoint Precisión Recall F1 Perceptrón 0.875 0.895 0.885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). Las tablas 9-12 muestran los resultados. Tabla 9. Precisión de la extracción de títulos con Word en DotGov Precisión Recall F1 Modelo Perceptrón 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582 Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Precisión de la extracción de títulos con PowerPoint en DotGov Precision Recall F1 Modelo Perceptrón 0.900 0.906 0.903 Tamaño de fuente más grande 0.871 0.888 0.879 Baselines Primera línea 0.554 0.564 0.559 Tabla 11. Precisión de la extracción de títulos con Word en DotCom Precisión Recall F1 Modelo Perceptrón 0.832 0.880 0.855 Tamaño de fuente más grande 0.676 0.753 0.712 Baselines Primera línea 0.577 0.643 0.608 Tabla 12. Rendimiento de la extracción de títulos de documentos de PowerPoint en el modelo de Precisión, Recall y F1 de Perceptrón DotCom 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875 Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos pueden adaptarse bien a diferentes dominios. Casi no hay disminución en la precisión. Los resultados indican que los patrones de formatos de títulos existen en diferentes dominios, y es posible construir un modelo independiente del dominio utilizando principalmente información de formato. 6.7 Adaptación de idioma Aplicamos el modelo entrenado con los datos en inglés (MS) al conjunto de datos en chino. Las tablas 13-14 muestran los resultados. Tabla 13. Precisión de la extracción de títulos con Word en chino: Precisión Recall F1 Modelo Perceptrón 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738 Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Vemos que los modelos se pueden adaptar a un idioma diferente. Solo hay pequeñas caídas en la precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no usarlas es insignificante. Los resultados indican que los patrones de formatos de títulos existen en diferentes idiomas. A partir de los resultados de adaptación de dominio y adaptación de lenguaje, concluimos que el uso de información de formato es clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con Títulos Extraídos Realizamos experimentos utilizando la extracción de títulos para la recuperación de documentos. Como línea base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación fue como se describe en la Sección 5. Los pesos fueron establecidos heurísticamente. No realizamos la optimización de los pesos. La evaluación se llevó a cabo en un corpus de 1.3 millones de documentos extraídos de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de los registros de consultas del motor de búsqueda de esta intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 consultas fueron elegidas al azar. A los usuarios se les pidió que proporcionaran juicios sobre el grado de relevancia del documento en una escala del 1 al 5 (1 significando perjudicial, 2 - malo, 3 - regular, 4 - bueno y 5 - excelente). La figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión al considerar documentos buenos o excelentes como relevantes (tres barras izquierdas con umbral de relevancia 0.5), o al considerar solo documentos excelentes como relevantes (tres barras derechas con umbral de relevancia 1.0). Resultados de clasificación de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo los campos cuerpo, título (propiedad de archivo) y texto de anclaje. • Barra morada - BM25 incluyendo los campos cuerpo, título (propiedad de archivo), texto de anclaje y título extraído. Con el campo adicional de título extraído incluido en BM25, la precisión @10 aumentó de 0.132 a 0.145, o aproximadamente un 10%. Por lo tanto, se puede afirmar que el uso del título extraído puede mejorar efectivamente la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este documento, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. Trabajos anteriores mostraron que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de artículos de investigación. En este artículo, demostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas base en la extracción de títulos de documentos de Office. Trabajos anteriores sobre extracción de metadatos principalmente utilizaron características lingüísticas en los documentos, mientras que nosotros principalmente utilizamos información de formato. Parecía que el uso de información de formato es clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Probamos diferentes modelos de aprendizaje automático incluyendo Perceptrón, Entropía Máxima, Modelo de Markov de Entropía Máxima y Perceptrón Votado. Descubrimos que el rendimiento de los modelos Perceptrón fue el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos entrenados en un idioma a otro idioma. Encontramos que las precisiones no disminuyeron sustancialmente en diferentes dominios y en diferentes idiomas, lo que indica que los modelos eran genéricos. También intentamos utilizar los títulos extraídos en la recuperación de documentos. Observamos una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda al utilizar la información del título extraída. Todas las investigaciones anteriores no se llevaron a cabo, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción de títulos. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu y a los revisores anónimos de JCDL05 por sus valiosos comentarios sobre este artículo. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de entropía máxima para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminativo para modelos ocultos de Markov: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en Procesamiento de Lenguaje Natural, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de vectores de soporte. Aprendizaje automático, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la <br>extracción de información</br> de texto semiestructurado y libre. En Actas de la Decimoctava Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia newsblaster: resumen multilingüe de noticias en la web. En Actas de la conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos ocultos de Markov factorial. Aprendizaje automático, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de Visualización de Información de 1999, 446-451, 1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., y Pal, N. eBizSearch: un motor de búsqueda de nicho para e-Business. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 413-414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. Extracción de metadatos basada en conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de Bibliotecas Digitales de la ACM, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos utilizando máquinas de vectores de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y etiquetar datos de secuencias. En Actas de la Decimoctava Conferencia Internacional sobre Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo del perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial en nuevos formatos de contenido. En Actas de la Séptima Conferencia de Motores de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., y Thoma, G. R. Un sistema de generación de características dinámicas para la extracción automatizada de metadatos en la preservación de materiales digitales. En Actas del Primer Taller Internacional sobre Análisis de Imágenes de Documentos para Bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de máxima entropía de Markov para <br>extracción de información</br> y segmentación. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "metada extraction": {
            "translated_key": "extracción de metadatos",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "machine learn": {
            "translated_key": "aprendizaje automático",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a <br>machine learn</br>ing approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train <br>machine learn</br>ing models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a <br>machine learn</br>ing based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a <br>machine learn</br>ing based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a <br>machine learn</br>ing approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the <br>machine learn</br>ing based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the <br>machine learn</br>ing approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the <br>machine learn</br>ing approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on <br>machine learn</br>ing consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the <br>machine learn</br>ing approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of <br>machine learn</br>ing over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different <br>machine learn</br>ing models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a <br>machine learn</br>ing approach to address the problem.",
                "Previous work showed that the <br>machine learn</br>ing approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the <br>machine learn</br>ing approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different <br>machine learn</br>ing models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using <br>machine learn</br>ing.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a <br>machine learn</br>ing approach to title extraction from general documents.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train <br>machine learn</br>ing models, and perform title extraction using the trained models.",
                "For instance, Han et al. [10] proposed a <br>machine learn</br>ing based method to conduct extraction from research papers.",
                "It has not been clarified whether a <br>machine learn</br>ing based approach can work well for this task.",
                "We take a <br>machine learn</br>ing approach."
            ],
            "translated_annotated_samples": [
                "En este documento, proponemos un enfoque de <br>aprendizaje automático</br> para la extracción de títulos de documentos generales.",
                "En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenamos modelos de <br>aprendizaje automático</br> y realizamos la extracción de títulos utilizando los modelos entrenados.",
                "Por ejemplo, Han et al. [10] propusieron un método basado en <br>aprendizaje automático</br> para realizar extracciones de artículos de investigación.",
                "No se ha aclarado si un enfoque basado en <br>aprendizaje automático</br> puede funcionar bien para esta tarea.",
                "Tomamos un enfoque de <br>aprendizaje automático</br>."
            ],
            "translated_text": "Extracción automática de títulos de documentos generales utilizando aprendizaje automático. En este documento, proponemos un enfoque de <br>aprendizaje automático</br> para la extracción de títulos de documentos generales. Por documentos generales nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos, incluyendo presentaciones, capítulos de libros, artículos técnicos, folletos, informes y cartas. Anteriormente, se han propuesto métodos principalmente para la extracción de títulos de artículos de investigación. No ha quedado claro si sería posible realizar la extracción automática de títulos de documentos generales. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenamos modelos de <br>aprendizaje automático</br> y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que principalmente utilizamos información de formato como el tamaño de fuente como características en los modelos. Resulta que el uso de información de formato puede llevar a una extracción bastante precisa de documentos generales. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente en un experimento con datos de intranet. Otros hallazgos importantes en este trabajo incluyen que podemos entrenar modelos en un dominio y aplicarlos a otro dominio, y más sorprendentemente, incluso podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente los resultados de clasificación de búsqueda en la recuperación de documentos utilizando los títulos extraídos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de Búsqueda; H.4.1 [Aplicaciones de Sistemas de Información]: Automatización de Oficinas - Procesamiento de Texto; D.2.8 [Ingeniería de Software]: Métricas - medidas de complejidad, medidas de rendimiento Términos Generales Algoritmos, Experimentación, Rendimiento. 1. METADATA La información de metadatos de los documentos es útil para muchos tipos de procesamiento de documentos, como la búsqueda, la navegación y el filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen los metadatos de los documentos por sí mismas, incluso cuando tienen herramientas convenientes para la definición de metadatos [26]. Por lo tanto, la extracción automática de metadatos de los cuerpos de los documentos resulta ser un tema de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, el enfoque estaba principalmente en la extracción de los documentos de investigación. Por ejemplo, Han et al. [10] propusieron un método basado en <br>aprendizaje automático</br> para realizar extracciones de artículos de investigación. Formalizaron el problema como el de clasificación y emplearon Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron características lingüísticas en el modelo. En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigación sobre la extracción de información de ellos. Los artículos de investigación suelen tener estilos bien formados y características notables. Por el contrario, los estilos de los documentos generales pueden variar considerablemente. No se ha aclarado si un enfoque basado en <br>aprendizaje automático</br> puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como estudio de caso, consideramos la extracción de títulos en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. Tomamos un enfoque de <br>aprendizaje automático</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "search": {
            "translated_key": "búsqueda",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve <br>search</br> ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information <br>search</br> and Retrieval - <br>search</br> Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as <br>search</br>, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as <br>search</br>.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 <br>search</br> Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata <br>search</br> of papers [8].",
                "In web <br>search</br>, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many <br>search</br> engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for <br>search</br> of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in <br>search</br> and browsing, but the titles are not very accurate.",
                "We created 50 queries to <br>search</br> Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the <br>search</br> results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in <br>search</br> can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like <br>search</br>, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 <br>search</br> with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets <br>search</br> engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "<br>search</br> ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for <br>search</br> when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche <br>search</br> engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh <br>search</br> Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet <br>search</br> engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [
                "Moreover, we can significantly improve <br>search</br> ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information <br>search</br> and Retrieval - <br>search</br> Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as <br>search</br>, browsing, and filtering.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as <br>search</br>.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 <br>search</br> Using Title Information Title information is useful for document retrieval."
            ],
            "translated_annotated_samples": [
                "Además, podemos mejorar significativamente los resultados de clasificación de <br>búsqueda</br> en la recuperación de documentos utilizando los títulos extraídos.",
                "Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de Búsqueda; H.4.1 [Aplicaciones de Sistemas de Información]: Automatización de Oficinas - Procesamiento de Texto; D.2.8 [Ingeniería de Software]: Métricas - medidas de complejidad, medidas de rendimiento Términos Generales Algoritmos, Experimentación, Rendimiento. 1.",
                "METADATA La información de metadatos de los documentos es útil para muchos tipos de procesamiento de documentos, como la <br>búsqueda</br>, la navegación y el filtrado.",
                "En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, ¿cuál modelo funciona mejor para la extracción de títulos?; (2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la <br>búsqueda</br>.",
                "La extracción de información se ha aplicado, por ejemplo, al etiquetado de partes del discurso [20], al reconocimiento de entidades nombradas [25] y a la extracción de tablas [19]. 2.3 Búsqueda utilizando la información del título La información del título es útil para la recuperación de documentos."
            ],
            "translated_text": "Extracción automática de títulos de documentos generales utilizando aprendizaje automático. En este documento, proponemos un enfoque de aprendizaje automático para la extracción de títulos de documentos generales. Por documentos generales nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos, incluyendo presentaciones, capítulos de libros, artículos técnicos, folletos, informes y cartas. Anteriormente, se han propuesto métodos principalmente para la extracción de títulos de artículos de investigación. No ha quedado claro si sería posible realizar la extracción automática de títulos de documentos generales. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que principalmente utilizamos información de formato como el tamaño de fuente como características en los modelos. Resulta que el uso de información de formato puede llevar a una extracción bastante precisa de documentos generales. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente en un experimento con datos de intranet. Otros hallazgos importantes en este trabajo incluyen que podemos entrenar modelos en un dominio y aplicarlos a otro dominio, y más sorprendentemente, incluso podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente los resultados de clasificación de <br>búsqueda</br> en la recuperación de documentos utilizando los títulos extraídos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de Búsqueda; H.4.1 [Aplicaciones de Sistemas de Información]: Automatización de Oficinas - Procesamiento de Texto; D.2.8 [Ingeniería de Software]: Métricas - medidas de complejidad, medidas de rendimiento Términos Generales Algoritmos, Experimentación, Rendimiento. 1. METADATA La información de metadatos de los documentos es útil para muchos tipos de procesamiento de documentos, como la <br>búsqueda</br>, la navegación y el filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen los metadatos de los documentos por sí mismas, incluso cuando tienen herramientas convenientes para la definición de metadatos [26]. Por lo tanto, la extracción automática de metadatos de los cuerpos de los documentos resulta ser un tema de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, el enfoque estaba principalmente en la extracción de los documentos de investigación. Por ejemplo, Han et al. [10] propusieron un método basado en aprendizaje automático para realizar extracciones de artículos de investigación. Formalizaron el problema como el de clasificación y emplearon Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron características lingüísticas en el modelo. En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigación sobre la extracción de información de ellos. Los artículos de investigación suelen tener estilos bien formados y características notables. Por el contrario, los estilos de los documentos generales pueden variar considerablemente. No se ha aclarado si un enfoque basado en aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como estudio de caso, consideramos la extracción de títulos en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizamos la extracción de títulos utilizando uno de los tipos de modelos entrenados. En los modelos, principalmente utilizamos información de formato como el tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con Márgenes Desiguales, Modelo de Entropía Máxima de Markov y Perceptrón Votado. En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, ¿cuál modelo funciona mejor para la extracción de títulos?; (2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la <br>búsqueda</br>. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a los baselines: uno que siempre utiliza las primeras líneas como títulos y el otro que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente. Resulta que el uso de características de formato es la clave para la exitosa extracción de títulos. (1) Hemos observado que los modelos basados en Perceptrón tienen un mejor rendimiento en términos de precisión de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados en otro, y pueden ser entrenados en un idioma y aplicados en otro. (3) Hemos descubierto que al utilizar los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que podemos realizar de manera fiable la extracción de títulos de documentos generales y utilizar los resultados extraídos para mejorar aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, presentamos el trabajo relacionado, y en la sección 3, explicamos la motivación y el contexto del problema de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando los títulos extraídos. La sección 6 presenta nuestros resultados experimentales. Hacemos observaciones finales en la sección 7.2. TRABAJO RELACIONADO 2.1 Se han propuesto métodos de extracción de metadatos de documentos para realizar extracción automática de metadatos de documentos; sin embargo, el enfoque principal ha sido la extracción de artículos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en reglas y el enfoque basado en aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrollaron un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como que los títulos suelen ubicarse en las partes superiores de las primeras páginas y suelen estar en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel et al. [23] realizaron extracción de metadatos de materiales educativos utilizando tecnologías de procesamiento del lenguaje natural basadas en reglas. Mao et al. [16] también llevaron a cabo la extracción automática de metadatos de artículos de investigación utilizando reglas sobre la información de formato. El enfoque basado en reglas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, realizaron extracción de metadatos con enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron información lingüística como características. Informaron de una alta precisión de extracción de información de artículos de investigación en términos de precisión y recuperación. 2.2 Extracción de Información La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la cual, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Los Modelos Ocultos de Markov [6], el Modelo de Entropía Máxima [1, 4], el Modelo de Entropía Máxima de Markov [17], las Máquinas de Vectores de Soporte [3], el Campo Aleatorio Condicional [12] y el Perceptrón Votado [2] son modelos ampliamente utilizados para la extracción de información. La extracción de información se ha aplicado, por ejemplo, al etiquetado de partes del discurso [20], al reconocimiento de entidades nombradas [25] y a la extracción de tablas [19]. 2.3 Búsqueda utilizando la información del título La información del título es útil para la recuperación de documentos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}